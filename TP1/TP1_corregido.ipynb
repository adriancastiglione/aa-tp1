{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1 \n",
    "### Clasificación sobre datos simulados. \n",
    "\n",
    "## Introducción\n",
    "Para este trabajo, hemos creado una función generadora de minions. Sobre cada minion, hemos medido 200 características que representan habilidades que poseen en distintas tareas (relacionadas al Mal).  \n",
    "\n",
    "El doctor Nefario ha ideado una fórmula para determinar si un minion es o no apto para concretar su plan para conquistar el mundo. De esta manera ha etiquetado más de 500 minions. Lamentablemente, ha perdido dicha fórmula y necesita seguir decidiendo si nuevos minions son o no aptos para su macabro plan.\n",
    "\n",
    "Es por esto que nuestro objetivo será construir clasificadores que estimen lo mejor posible la probabilidad de que nuevos minions sean o no aptos para concretar el plan de conquista y así facilitarle las cosas al doctor Nefario.\n",
    "\n",
    "Por otra parte, ya que el doctor Nefario tuvo problemas con equipos que sobreestiman sus resultados, decidió guardarse varias etiquetas extra que no compartirá con nadie, y que luego utilizará para elegir al mejor equipo, al cual contratará para (de una vez por todas) conquistar el mundo. \n",
    "\n",
    "\n",
    "En concreto:\n",
    "\n",
    "Tendrán disponible una matriz de datos $X$ de $500$ filas en donde cada fila $x^{(i)}$ representa un vector de $200$ características de cada instancia. Es decir, $\\textbf{x}^{(i)} = x_1^{(i)}, \\dots, x_{200}^{(i)}$ con $i$ entre $1$ y $500$. Además, tendrán y, un vector de $500$ posiciones con dos posibles valores: $True$ y $False$. \n",
    "\n",
    "Por otra parte, tendrán disponibles más instancias de evaluación $X_{competencia}$ sin las respectivas etiquetas que utilizaremos para evaluar sus resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREAMBULOS\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import display, HTML\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.svm\n",
    "\n",
    "import sklearn.model_selection\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.1644</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>1.2429</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1983</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>1.5375</td>\n",
       "      <td>-0.7727</td>\n",
       "      <td>-0.1401</td>\n",
       "      <td>2.0871</td>\n",
       "      <td>-0.8312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2749</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>-1.3108</td>\n",
       "      <td>0.6801</td>\n",
       "      <td>-0.5503</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>-0.4478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2190</td>\n",
       "      <td>-0.3190</td>\n",
       "      <td>-0.6446</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-1.2374</td>\n",
       "      <td>-1.3291</td>\n",
       "      <td>-1.3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2243</td>\n",
       "      <td>-0.5710</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>-0.1328</td>\n",
       "      <td>-1.0045</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>-1.4507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>-0.1989</td>\n",
       "      <td>-0.0393</td>\n",
       "      <td>-0.5866</td>\n",
       "      <td>2.2507</td>\n",
       "      <td>1.4925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5853</td>\n",
       "      <td>-0.8532</td>\n",
       "      <td>-0.2723</td>\n",
       "      <td>-0.5493</td>\n",
       "      <td>-2.9824</td>\n",
       "      <td>-0.1697</td>\n",
       "      <td>-0.0430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6488</td>\n",
       "      <td>-0.7363</td>\n",
       "      <td>-0.8866</td>\n",
       "      <td>-1.2717</td>\n",
       "      <td>-0.1493</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>-1.4820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4155</td>\n",
       "      <td>1.4187</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>-0.7993</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>-0.1796</td>\n",
       "      <td>-0.7140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1314</td>\n",
       "      <td>-0.4230</td>\n",
       "      <td>-0.2685</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>-1.2245</td>\n",
       "      <td>-1.9421</td>\n",
       "      <td>1.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-1.1980</td>\n",
       "      <td>0.4577</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>-0.5494</td>\n",
       "      <td>0.4607</td>\n",
       "      <td>1.2182</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>-0.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.6246</td>\n",
       "      <td>-1.0590</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-1.6657</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>-0.4229</td>\n",
       "      <td>0.3977</td>\n",
       "      <td>-0.0808</td>\n",
       "      <td>-1.7054</td>\n",
       "      <td>-0.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>-0.9023</td>\n",
       "      <td>-1.7792</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>0.3317</td>\n",
       "      <td>-1.7513</td>\n",
       "      <td>-0.5397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>1.7056</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>-0.8350</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0130</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-1.6642</td>\n",
       "      <td>2.5117</td>\n",
       "      <td>-0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>-0.6085</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3587</td>\n",
       "      <td>-0.3121</td>\n",
       "      <td>-0.7630</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>-1.0215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "0      1.4914  0.1644  1.2315  1.2429  1.5576  0.0455  0.1302   ...   -1.1983   \n",
       "1     -0.2749  0.2780 -1.3108  0.6801 -0.5503  0.6359 -0.4478   ...    1.2190   \n",
       "2     -0.2243 -0.5710 -0.2712 -0.1328 -1.0045  0.9315 -1.4507   ...    0.9459   \n",
       "3      0.5853 -0.8532 -0.2723 -0.5493 -2.9824 -0.1697 -0.0430   ...    1.6488   \n",
       "4     -1.4155  1.4187  0.6027 -0.7993  0.2939 -0.1796 -0.7140   ...    1.1314   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "495    0.2516  0.9375 -1.1980  0.4577  0.9287  0.5373  0.2476   ...    0.5829   \n",
       "496    0.6246 -1.0590  0.9491  0.2687  0.6610 -1.6657  0.3982   ...   -0.1075   \n",
       "497    0.2677  0.1802  0.7154  0.3542 -0.9023 -1.7792 -0.0121   ...    0.8491   \n",
       "498    0.1926  0.7834  1.7056  0.3418 -0.8350  0.4068  0.0495   ...   -0.0130   \n",
       "499    0.0427  0.4028 -0.6085  1.0845  0.1033  0.2698 -0.8598   ...   -0.3587   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "0     -0.0118  1.5375 -0.7727 -0.1401  2.0871 -0.8312  \n",
       "1     -0.3190 -0.6446 -0.0061 -1.2374 -1.3291 -1.3265  \n",
       "2      0.1430 -0.1989 -0.0393 -0.5866  2.2507  1.4925  \n",
       "3     -0.7363 -0.8866 -1.2717 -0.1493  0.2007 -1.4820  \n",
       "4     -0.4230 -0.2685  0.3045 -1.2245 -1.9421  1.5186  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "495   -0.5494  0.4607  1.2182  0.1025  3.0034 -0.0344  \n",
       "496    0.8993 -0.4229  0.3977 -0.0808 -1.7054 -0.4786  \n",
       "497    0.7469  0.2071 -1.0090  0.3317 -1.7513 -0.5397  \n",
       "498    0.1483  0.5019 -0.0020 -1.6642  2.5117 -0.0118  \n",
       "499   -0.3121 -0.7630  0.6525  0.6161 -0.0902 -1.0215  \n",
       "\n",
       "[500 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "index        \n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "...       ...\n",
       "495         1\n",
       "496         0\n",
       "497         1\n",
       "498         0\n",
       "499         0\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "X = pd.read_csv(\"X.csv\", index_col=\"index\")\n",
    "y = pd.read_csv(\"y.csv\", index_col=\"index\", dtype=int)  # Cargamos los valores booleanos (True y False)\n",
    "                                                        # como números (1 y 0) para facilitar el manejo luego. \n",
    "    \n",
    "X_competencia = pd.read_csv(\"X_competencia1.csv\", index_col=\"index\")\n",
    "y_competencia_ejemplo = pd.read_csv(\"y_competencia_ejemplo.csv\", index_col=\"index\")\n",
    "display(X)\n",
    "display(y)\n",
    "\n",
    "# Descomentar si quieren ver los datos para la competencia:\n",
    "# display(X_competencia) \n",
    "# display(y_competencia_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "### Separación de datos\n",
    "\n",
    "Contarán con una cantidad limitada de datos, por lo cual es importante tomar una buena decisión en el momento de empezar a utilizarlos. En este punto pedimos que evalúen cómo separar sus datos para desarrollo y para evaluación tomando en cuenta la competencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dev: (250, 200), y_dev: (250, 1) para desarrollo\n",
      "X_eval: (250, 200), y_eval: (250, 1) para evaluación\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "# EJERCICIO 1. \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO\n",
    "#X_dev, X_eval, y_dev, y_eval = X, X, y, y  # cambiar esta línea si lo consideran necesario\n",
    "\n",
    "X_dev, X_eval, y_dev, y_eval = sklearn.model_selection.train_test_split(\n",
    "    X, y, random_state=1234, test_size=0.5)\n",
    "\n",
    "# Objetivo: variables X_dev, X_eval, y_dev e y_eval asignadas\n",
    "#########################################################\n",
    "\n",
    "\n",
    "print(\"X_dev: {}, y_dev: {} para desarrollo\".format(X_dev.shape, y_dev.shape))\n",
    "print(\"X_eval: {}, y_eval: {} para evaluación\".format(X_eval.shape, y_eval.shape))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(np.array(y))  # muestra un histograma para la distribución de y.\n",
    "plt.title('Histograma de clases para todas las instancias')\n",
    "plt.show()\n",
    "print(y['output'].sum()) #229/500 = 46% de todas son true\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(np.array(y_dev))  # muestra un histograma para la distribución de y.\n",
    "plt.title('Histograma de clases para las instancias de desarrollo')\n",
    "plt.show()\n",
    "print(y_dev['output'].sum()) #118/250 = 47% de las de dev son true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "\n",
    "En principio, la división de las instancias para desarrollo y validación debe ser aleatoria, puesto que para este conjunto de datos las instancias no parecen estar ordenadas bajo algún criterio. Tampoco tenemos información sobre la importancia de los atributos. \n",
    "\n",
    "Tenemos 500 instancias disponibles para hacer esta separación. Teniendo en cuenta la competencia, debemos seleccionar un modelo y estimar su performance sobre las instancias _para la competencia_. Estas son 5000 en total, una cantidad bastante mayor al máximo que podríamos destinar para entrenamiento. Teniendo en cuenta este desbalance, consideramos que para poder obtener un estimado realista al utilizar las instancias _held out_, la cantidad de instancias destinadas para este fin debe ser igual o mayor a la cantidad utilizadas para entrenamiento. De esta manera, se intentará reflejar este desbalance entre cantidades.\n",
    "\n",
    "No obstante, reservar pocas instancias para entrenamiento acarrea también el riesgo de desarrollar modelos poco predictivos. Por esa razón, decidimos separar las instancias en *dos mitades iguales* para entrenamiento y validación, de forma aleatoria. \n",
    "\n",
    "Observamos también que, sobre el total de las 500 instancias, 229 pertenecen a la clase $1$, un poco menos que la mitad (el $46 \\% $ ). De manera tal de evitar un sesgo en la selección, nos aseguramos que este porcentaje de instancias se refleje también en las reservadas para entrenamiento (resulta del $ 47 \\%$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "### Construcción de modelos\n",
    "\n",
    "Para este punto, la tarea consiste en construir y evaluar modelos de tipo árbol de decisión, de manera de obtener una estimación realista de la performance de los mismos. \n",
    "\n",
    "1. Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default. \n",
    "2. Estimar la performance del modelo utilizando K-fold cross validation con K = 5, con las métricas “Accuracy” y “ROC AUC”. Para ello, se pide medir la performance en cada partición tanto sobre el fold de validación como sobre los folds de entrenamiento. Luego, completar la primera tabla.\n",
    "3. Entrenar árboles de decisión para cada una de las siguientes combinaciones y completar la segunda tabla.\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA: Usar la implementación de árboles de decisión que realizaron para la guía de ejercicios de la materia. Adaptarla para que cumpla con la interfaz requerida por sklearn, asegurarse de que funcione con variables continuas y reproducir las tablas anteriores.   **\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9, 0.9, 0.845, 0.875, 0.875]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3> TABLA 1 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (training)</th>\n",
       "      <th>Accuracy (validación)</th>\n",
       "      <th>AUC ROC (training)</th>\n",
       "      <th>AUC ROC (validación)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permutación</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.9463</td>\n",
       "      <td>0.7011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.6526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.845</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.9134</td>\n",
       "      <td>0.6177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>0.8194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.8584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy (training)  Accuracy (validación)  AUC ROC (training)  \\\n",
       "Permutación                                                                   \n",
       "1                          0.900                   0.68              0.9463   \n",
       "2                          0.900                   0.68              0.9460   \n",
       "3                          0.845                   0.56              0.9134   \n",
       "4                          0.875                   0.76              0.9323   \n",
       "5                          0.875                   0.80              0.9518   \n",
       "\n",
       "             AUC ROC (validación)  \n",
       "Permutación                        \n",
       "1                          0.7011  \n",
       "2                          0.6526  \n",
       "3                          0.6177  \n",
       "4                          0.8194  \n",
       "5                          0.8584  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##CORREGIDO\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "accuracies_training = []\n",
    "accuracies_validation = []\n",
    "aucs_training = []\n",
    "aucs_validation = []\n",
    "\n",
    "# Puede serles de utilidad tener a X_dev e y_dev como matrices de numpy directamente:\n",
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: accuracies_training, accuracies_validation, aucs_training y aucs_validation asignados\n",
    "\n",
    "\n",
    "#kf = KFold(n_splits=5, random_state=1234, shuffle=True)\n",
    "\n",
    "# ya que las instancias no tienen orden aparente (y ya habían sido seleccionadas aleatoriamente al reservar las \n",
    "# de desarollo), no hace falta mezclarlas en el K-fold cross validation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kf.split(X_dev):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "    y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "    \n",
    "    arbol = DecisionTreeClassifier(max_depth=3)\n",
    "    arbol.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_test = arbol.predict(X_test)\n",
    "    y_pred_train = arbol.predict(X_train)\n",
    "    \n",
    "    y_pred_test_proba = arbol.predict_proba(X_test)\n",
    "    y_pred_train_proba = arbol.predict_proba(X_train)\n",
    "    \n",
    "    y_pred_test_proba=y_pred_test_proba[:,1]\n",
    "    y_pred_train_proba=y_pred_train_proba[:,1]\n",
    "    \n",
    "    ac_test=sklearn.metrics.accuracy_score(y_test, y_pred_test, normalize=True, sample_weight=None)\n",
    "    ac_train=sklearn.metrics.accuracy_score(y_train, y_pred_train, normalize=True, sample_weight=None)\n",
    "    accuracies_validation.append(ac_test)\n",
    "    accuracies_training.append(ac_train)\n",
    "    \n",
    "    roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test_proba, sample_weight=None)\n",
    "    roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train_proba, sample_weight=None)\n",
    "    aucs_training.append(roc_auc_train)\n",
    "    aucs_validation.append(roc_auc_test)\n",
    "#########################################################\n",
    "\n",
    "print(accuracies_training)\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Permutación\"\n",
    "                  \n",
    "df[\"Accuracy (training)\"] = accuracies_training     # cambiar por accuracies_training\n",
    "df[\"Accuracy (validación)\"] = accuracies_validation   # cambiar por accuracies_validation\n",
    "df[\"AUC ROC (training)\"] = aucs_training     # cambiar por aucs_training\n",
    "df[\"AUC ROC (validación)\"] = aucs_validation    # [\"-\"] * 5 cambiar por aucs_validation\n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "# Descomentar las siguientes líneas para graficar el resultado\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 0 1\n",
      " 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0\n",
      " 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0\n",
      " 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.1818 0.8182]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.8125 0.1875]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [0.1818 0.8182]\n",
      " [0.5    0.5   ]\n",
      " [0.9091 0.0909]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.8125 0.1875]\n",
      " [0.1818 0.8182]\n",
      " [0.1818 0.8182]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.8125 0.1875]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.1818 0.8182]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.     1.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.     1.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.8125 0.1875]\n",
      " [0.9091 0.0909]\n",
      " [0.8125 0.1875]\n",
      " [0.5    0.5   ]\n",
      " [0.9091 0.0909]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.8125 0.1875]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.     1.    ]\n",
      " [1.     0.    ]\n",
      " [0.     1.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.3333 0.6667]\n",
      " [0.8125 0.1875]\n",
      " [0.5    0.5   ]\n",
      " [0.5    0.5   ]\n",
      " [0.5    0.5   ]\n",
      " [0.8125 0.1875]\n",
      " [0.05   0.95  ]\n",
      " [0.1818 0.8182]\n",
      " [0.8125 0.1875]\n",
      " [0.5    0.5   ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.1818 0.8182]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.9091 0.0909]\n",
      " [0.9091 0.0909]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.9091 0.0909]\n",
      " [0.1818 0.8182]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.9091 0.0909]\n",
      " [0.9091 0.0909]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [0.     1.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.8125 0.1875]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.8125 0.1875]\n",
      " [0.05   0.95  ]\n",
      " [0.1818 0.8182]\n",
      " [0.8125 0.1875]\n",
      " [1.     0.    ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.1818 0.8182]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.8125 0.1875]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.8125 0.1875]\n",
      " [0.05   0.95  ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.5    0.5   ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [1.     0.    ]\n",
      " [0.8125 0.1875]\n",
      " [0.5    0.5   ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [0.1818 0.8182]\n",
      " [0.3333 0.6667]\n",
      " [0.5    0.5   ]\n",
      " [0.8125 0.1875]\n",
      " [0.     1.    ]\n",
      " [0.05   0.95  ]\n",
      " [0.05   0.95  ]\n",
      " [0.9091 0.0909]\n",
      " [0.9091 0.0909]\n",
      " [0.3333 0.6667]\n",
      " [0.05   0.95  ]]\n",
      "[0.8182 0.95   0.95   0.95   0.     0.5    0.1875 0.     0.0909 0.5\n",
      " 0.     0.8182 0.5    0.0909 0.95   0.95   0.     0.     0.     0.5\n",
      " 0.     0.0909 0.95   0.5    0.5    0.     0.5    0.95   0.95   0.1875\n",
      " 0.8182 0.8182 0.     0.95   0.1875 0.     0.95   0.95   0.8182 0.95\n",
      " 0.95   0.0909 0.     0.95   0.95   0.5    0.95   0.95   1.     0.\n",
      " 0.     0.5    0.95   0.0909 0.     0.5    1.     0.     0.95   0.95\n",
      " 0.     0.1875 0.0909 0.1875 0.5    0.0909 0.95   0.95   0.95   0.\n",
      " 0.     0.     0.1875 0.     0.0909 1.     0.     1.     0.5    0.6667\n",
      " 0.1875 0.5    0.5    0.5    0.1875 0.95   0.8182 0.1875 0.5    0.5\n",
      " 0.     0.0909 0.8182 0.     0.0909 0.0909 0.0909 0.     0.5    0.\n",
      " 0.     0.95   0.5    0.0909 0.8182 0.95   0.     0.     0.95   0.\n",
      " 0.     0.0909 0.0909 0.95   0.95   0.95   0.     0.95   0.0909 1.\n",
      " 0.     0.95   0.5    0.95   0.1875 0.95   0.0909 0.     0.95   0.\n",
      " 0.     0.     0.95   0.     0.1875 0.95   0.8182 0.1875 0.     0.5\n",
      " 0.95   0.8182 0.     0.     0.95   0.     0.95   0.5    0.1875 0.95\n",
      " 0.95   0.5    0.5    0.     0.95   0.95   0.95   0.95   0.0909 0.95\n",
      " 0.95   0.95   0.     0.     0.     0.95   0.95   0.95   0.1875 0.95\n",
      " 0.     0.95   0.0909 0.     0.     0.     0.95   0.5    0.95   0.5\n",
      " 0.     0.     0.     0.     0.     0.1875 0.5    0.95   0.0909 0.8182\n",
      " 0.6667 0.5    0.1875 1.     0.95   0.95   0.0909 0.0909 0.6667 0.95  ]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "arbol = DecisionTreeClassifier(max_depth=3)\n",
    "arbol.fit(X_train, y_train)\n",
    "  \n",
    "y_pred_test = arbol.predict(X_test)\n",
    "y_pred_train = arbol.predict(X_train)    \n",
    "print(y_pred_train)\n",
    "print(type(y_pred_train))\n",
    "\n",
    "\n",
    "y_pred_test = arbol.predict_proba(X_test)\n",
    "y_pred_train = arbol.predict_proba(X_train)\n",
    "print(y_pred_train)\n",
    "print(y_pred_train[:,1])\n",
    "print(type(y_pred_train))\n",
    "print(type(y_pred_train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 2 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura máxima</th>\n",
       "      <th>Criterio de evaluación de corte</th>\n",
       "      <th>AUC ROC promedio (training)</th>\n",
       "      <th>AUC ROC promedio (validación)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>0.7406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>0.6643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Gini</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9453</td>\n",
       "      <td>0.7698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.7357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Altura máxima Criterio de evaluación de corte  AUC ROC promedio (training)  \\\n",
       "0             3                            Gini                       0.9401   \n",
       "1             5                            Gini                       0.9957   \n",
       "2     Inifinito                            Gini                       1.0000   \n",
       "3             3         Ganancia de Información                       0.9453   \n",
       "4             5         Ganancia de Información                       0.9975   \n",
       "5     Inifinito         Ganancia de Información                       1.0000   \n",
       "\n",
       "   AUC ROC promedio (validación)  \n",
       "0                         0.7406  \n",
       "1                         0.6643  \n",
       "2                         0.6849  \n",
       "3                         0.7698  \n",
       "4                         0.7357  \n",
       "5                         0.7163  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##CORREGIDO\n",
    "\n",
    "resultados_training = []\n",
    "resultados_validation = []\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: resultados_training y resultados_validation asignadas\n",
    "#\n",
    "## Recomendamos seguir el siguiente esquema:\n",
    "np.random.seed(SEED)\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "     for altura in [3, 5, None]:\n",
    "        \n",
    "        lista_aucs_train = []\n",
    "        lista_aucs_test = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_dev):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "            y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "\n",
    "            arbol = DecisionTreeClassifier(max_depth=altura, criterion=criterio)\n",
    "            arbol.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_test = arbol.predict(X_test)\n",
    "            y_pred_train = arbol.predict(X_train)\n",
    "            \n",
    "            y_pred_test_proba = arbol.predict_proba(X_test)\n",
    "            y_pred_train_proba = arbol.predict_proba(X_train)\n",
    "    \n",
    "            y_pred_test_proba=y_pred_test_proba[:,1]\n",
    "            y_pred_train_proba=y_pred_train_proba[:,1]\n",
    "    \n",
    "\n",
    "            #ac_test=sklearn.metrics.accuracy_score(y_test, y_pred_test, normalize=True, sample_weight=None)\n",
    "            #ac_train=sklearn.metrics.accuracy_score(y_train, y_pred_train, normalize=True, sample_weight=None)\n",
    "            #accuracies_validation.append(ac_test)\n",
    "            #accuracies_training.append(ac_train)  \n",
    "            \n",
    "            #EN EL PDF DICE ACURACY, ACA FIGURA AUC ROC\n",
    "            \n",
    "            roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test_proba, sample_weight=None)\n",
    "            roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train_proba, sample_weight=None)\n",
    "            lista_aucs_train.append(roc_auc_train)\n",
    "            lista_aucs_test.append(roc_auc_test)\n",
    "                \n",
    "\n",
    "        resultados_training.append( np.mean(lista_aucs_train) )\n",
    "        resultados_validation.append( np.mean(lista_aucs_test) )\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = resultados_training # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = resultados_validation # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "def construir_arbol(instancias, etiquetas, criterion, max_depth):\n",
    "    # ALGORITMO RECURSIVO para construcción de un árbol de decisión binario. \n",
    "    \n",
    "    if max_depth == 0 or len(instancias) <= 2:\n",
    "        return Hoja(etiquetas)\n",
    "    \n",
    "    # Suponemos que estamos parados en la raiz del árbol y tenemos que decidir cómo construirlo. \n",
    "    ganancia, pregunta = encontrar_mejor_atributo_y_corte(instancias, etiquetas, criterion)\n",
    "    \n",
    "    # Criterio de corte: ¿Hay ganancia?\n",
    "    if ganancia == 0:\n",
    "        #  Si no hay ganancia en separar, no separamos. \n",
    "        return Hoja(etiquetas)\n",
    "    else: \n",
    "        # Si hay ganancia en partir el conjunto en 2\n",
    "        instancias_cumplen, etiquetas_cumplen, instancias_no_cumplen, etiquetas_no_cumplen = partir_segun(pregunta, instancias, etiquetas, criterion)\n",
    "        # partir devuelve instancias y etiquetas que caen en cada rama (izquierda y derecha)\n",
    "\n",
    "        new_max_depth = None\n",
    "        if max_depth != None:\n",
    "            new_max_depth = max_depth - 1\n",
    "            \n",
    "        # Paso recursivo (consultar con el computador más cercano)\n",
    "        sub_arbol_izquierdo = construir_arbol(instancias_cumplen, etiquetas_cumplen, criterion, new_max_depth)\n",
    "        sub_arbol_derecho   = construir_arbol(instancias_no_cumplen, etiquetas_no_cumplen, criterion, new_max_depth)\n",
    "        # los pasos anteriores crean todo lo que necesitemos de sub-árbol izquierdo y sub-árbol derecho\n",
    "        \n",
    "        # sólo falta conectarlos con un nodo de decisión:\n",
    "        return Nodo_De_Decision(pregunta, sub_arbol_izquierdo, sub_arbol_derecho)\n",
    "    \n",
    "    \n",
    "# Definición de la estructura del árbol. \n",
    "\n",
    "class Hoja:\n",
    "    #  Contiene las cuentas para cada clase (en forma de diccionario)\n",
    "    #  Por ejemplo, {'Si': 2, 'No': 2}\n",
    "    def __init__(self, etiquetas):\n",
    "        self.cuentas = dict(Counter(etiquetas))\n",
    "\n",
    "\n",
    "class Nodo_De_Decision:\n",
    "    # Un Nodo de Decisión contiene preguntas y una referencia al sub-árbol izquierdo y al sub-árbol derecho\n",
    "     \n",
    "    def __init__(self, pregunta, sub_arbol_izquierdo, sub_arbol_derecho):\n",
    "        self.pregunta = pregunta\n",
    "        self.sub_arbol_izquierdo = sub_arbol_izquierdo\n",
    "        self.sub_arbol_derecho = sub_arbol_derecho\n",
    "        \n",
    "        \n",
    "# Definición de la clase \"Pregunta\"\n",
    "class Pregunta:\n",
    "    def __init__(self, atributo, valor):\n",
    "        self.atributo = atributo\n",
    "        self.valor = valor\n",
    "    \n",
    "    def cumple(self, instancia):\n",
    "        # Devuelve verdadero si la instancia cumple con la pregunta\n",
    "        return instancia[self.atributo] == self.valor\n",
    "    \n",
    "    def __repr__(self):\n",
    "        try:\n",
    "            float(self.valor)\n",
    "            return \"¿Es el valor para {} <= a {}?\".format(self.atributo, self.valor)\n",
    "        except:\n",
    "            return \"¿Es el valor para {} igual a {}?\".format(self.atributo, self.valor)\n",
    "    \n",
    "def gini(etiquetas):\n",
    "    d = dict(Counter(etiquetas))\n",
    "    impureza = 1\n",
    "    for key, value in d.items():\n",
    "        impureza -= (value/len(etiquetas))**2\n",
    "    return impureza\n",
    "\n",
    "def ganancia_gini(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha):\n",
    "    # COMPLETAR\n",
    "    ganancia_previa = gini(etiquetas_rama_izquierda + etiquetas_rama_derecha)\n",
    "    gini_izq = gini(etiquetas_rama_izquierda)\n",
    "    \n",
    "    proporcion_izq = (len(etiquetas_rama_izquierda)/len(instancias))\n",
    "    proporcion_der = (len(etiquetas_rama_derecha)/len(instancias))\n",
    "    \n",
    "    gini_der = gini(etiquetas_rama_derecha)\n",
    "    ganancia_gini = ganancia_previa - ((gini_izq*proporcion_izq + gini_der*proporcion_der)/2)\n",
    "    return ganancia_gini\n",
    "\n",
    "def entropy(etiquetas):\n",
    "    d = dict(Counter(etiquetas))\n",
    "    entropy = 0\n",
    "    for key, value in d.items():\n",
    "        proportion = value/len(etiquetas)\n",
    "        entropy -= (proportion)*log(proportion, 2)\n",
    "    return entropy\n",
    "\n",
    "def ganancia_entropy(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha):\n",
    "    \n",
    "    ganancia_previa = entropy(etiquetas_rama_izquierda + etiquetas_rama_derecha)\n",
    "    entropy_izq = gini(etiquetas_rama_izquierda)\n",
    "    \n",
    "    proporcion_izq = (len(etiquetas_rama_izquierda)/len(instancias))\n",
    "    proporcion_der = (len(etiquetas_rama_derecha)/len(instancias))\n",
    "    \n",
    "    entropy_der = gini(etiquetas_rama_derecha)\n",
    "    ganancia_entropia = ganancia_previa - ((entropy_izq*proporcion_izq + entropy_der*proporcion_der)/2)\n",
    "    return ganancia_entropia\n",
    "\n",
    "    \n",
    "def get_ganancia(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha, criterion):\n",
    "    \n",
    "    if criterion == \"gini\":\n",
    "        return ganancia_gini(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha)\n",
    "    else:\n",
    "        return ganancia_entropy(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha)\n",
    "    \n",
    "\n",
    "def partir_segun(pregunta, instancias, etiquetas, criterion):\n",
    "\n",
    "    try:\n",
    "        float(pregunta.valor)\n",
    "        return partir_segun_attr_numerico(pregunta, instancias, etiquetas, criterion)\n",
    "    except:\n",
    "        return partir_segun_attr_discreto(pregunta, instancias, etiquetas, criterion)\n",
    "    \n",
    "def partir_segun_attr_discreto(pregunta, instancias, etiquetas, criterion):\n",
    "    \n",
    "    # Esta función debe separar instancias y etiquetas según si cada instancia cumple o no con la pregunta (ver método 'cumple')\n",
    "    # COMPLETAR (recomendamos utilizar máscaras para este punto)\n",
    "    \n",
    "    instancias_cumplen, etiquetas_cumplen, instancias_no_cumplen, etiquetas_no_cumplen = list(), list(), list(), list()\n",
    "\n",
    "    index = 0\n",
    "    for x, instance in instancias.iterrows():\n",
    "\n",
    "        if instance[pregunta.atributo] == pregunta.valor:\n",
    "            instancias_cumplen.append(instance)\n",
    "            etiquetas_cumplen.append(etiquetas[index])\n",
    "        else:\n",
    "            instancias_no_cumplen.append(instance)\n",
    "            etiquetas_no_cumplen.append(etiquetas[index])\n",
    "        index += 1\n",
    "    return pd.DataFrame(instancias_cumplen), etiquetas_cumplen, pd.DataFrame(instancias_no_cumplen), etiquetas_no_cumplen\n",
    "\n",
    "def partir_segun_attr_numerico(pregunta, instancias, etiquetas, criterion):\n",
    "    \n",
    "    instancias_copy = instancias.copy()\n",
    "    instancias_copy[\"___tmp___\"] = etiquetas\n",
    "    \n",
    "    cut = pregunta.valor\n",
    "    cumplen = instancias_copy[instancias_copy[pregunta.atributo] < cut]\n",
    "    no_cumplen = instancias_copy[instancias_copy[pregunta.atributo] >= cut]\n",
    "    etiquetas_cumplen = list(cumplen[\"___tmp___\"])\n",
    "    etiquetas_no_cumplen = list(no_cumplen[\"___tmp___\"])\n",
    "    del cumplen[\"___tmp___\"]\n",
    "    del no_cumplen[\"___tmp___\"]\n",
    "    \n",
    "    return cumplen, etiquetas_cumplen, no_cumplen, etiquetas_no_cumplen \n",
    "\n",
    "def encontrar_mejor_atributo_y_corte(instancias, etiquetas, criterion):\n",
    "    \n",
    "    max_ganancia = 0\n",
    "    mejor_pregunta = None\n",
    "    for columna in instancias.columns:\n",
    "        \n",
    "        try:\n",
    "            for valor in set(instancias[columna]): #asegurarse que son valores numericos\n",
    "                float(valor)\n",
    "            \n",
    "            instancias_tmp = instancias.copy()\n",
    "            instancias_tmp[\"___tmp___\"] = etiquetas\n",
    "            instancias_tmp.sort_values(columna)\n",
    "            etiquetas_tmp = instancias_tmp[\"___tmp___\"]\n",
    "            del instancias_tmp[\"___tmp___\"]\n",
    "            \n",
    "            if len(instancias_tmp) == 1:\n",
    "                pregunta = Pregunta(columna, 0) #da igual\n",
    "                _, etiquetas_rama_izquierda, _, etiquetas_rama_derecha = partir_segun_attr_numerico(pregunta, instancias_tmp, etiquetas_tmp, criterion)\n",
    "                ganancia = get_ganancia(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha, criterion)\n",
    "                if ganancia > max_ganancia:\n",
    "                    max_ganancia = ganancia\n",
    "                    mejor_pregunta = pregunta\n",
    "            else:\n",
    "                \n",
    "                instancia_anterior = None\n",
    "                mejor_ganancia_hasta_ahora = 0\n",
    "                for x, una_instancia in instancias_tmp.iterrows():\n",
    "                    if instancia_anterior is None:\n",
    "                        instancia_anterior = una_instancia\n",
    "                    else:\n",
    "                        cut = (instancia_anterior[columna] + una_instancia[columna])/2\n",
    "                        #num_pregunta += 1\n",
    "                        #print(num_pregunta)\n",
    "                        pregunta = Pregunta(columna, cut)\n",
    "                        _, etiquetas_rama_izquierda, _, etiquetas_rama_derecha = partir_segun_attr_numerico(pregunta, instancias_tmp, etiquetas_tmp, criterion)\n",
    "                        ganancia = get_ganancia(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha, criterion)\n",
    "                        if ganancia > max_ganancia:\n",
    "                            max_ganancia = ganancia\n",
    "                            mejor_pregunta = pregunta\n",
    "                        instancia_anterior = una_instancia\n",
    "        \n",
    "        except:\n",
    "            for valor in set(instancias[columna]):\n",
    "                # Probando corte para atributo y valor\n",
    "                pregunta = Pregunta(columna, valor)\n",
    "                _, etiquetas_rama_izquierda, _, etiquetas_rama_derecha = partir_segun_attr_discreto(pregunta, instancias, etiquetas, criterion)\n",
    "\n",
    "                ganancia = get_ganancia(instancias, etiquetas_rama_izquierda, etiquetas_rama_derecha, criterion)\n",
    "\n",
    "                if ganancia > max_ganancia:\n",
    "                    max_ganancia = ganancia\n",
    "                    mejor_pregunta = pregunta\n",
    "      \n",
    "    return max_ganancia, mejor_pregunta\n",
    "\n",
    "def predecir(arbol, x_t):\n",
    "    \n",
    "    if isinstance(arbol, Hoja):\n",
    "        max = 0\n",
    "        return_value = None\n",
    "        for key,value in arbol.cuentas.items():\n",
    "            if value > max:\n",
    "                return_value = key\n",
    "                max = value\n",
    "        return return_value\n",
    "    \n",
    "    else:\n",
    "        attr = arbol.pregunta.atributo\n",
    "        val = arbol.pregunta.valor\n",
    "        try:\n",
    "            float(val)\n",
    "            if x_t[attr] < val:\n",
    "                return predecir(arbol.sub_arbol_izquierdo, x_t)    \n",
    "            else:\n",
    "                return predecir(arbol.sub_arbol_derecho, x_t)\n",
    "        except:\n",
    "            if(x_t.loc[attr]) == val:\n",
    "                return predecir(arbol.sub_arbol_izquierdo, x_t)\n",
    "            else:\n",
    "                return predecir(arbol.sub_arbol_derecho, x_t)\n",
    "\n",
    "        \n",
    "class CustomDecisionTreeClassifier(): \n",
    "    def __init__(self, criterion = \"gini\", max_depth=None):\n",
    "        self.arbol = None\n",
    "        if criterion != \"gini\" and criterion != \"entropy\":\n",
    "            raise Exception(\"Criterio desconocido\")\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.arbol = construir_arbol(pd.DataFrame(X_train), y_train, criterion=self.criterion, max_depth=self.max_depth)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x_t in X_test:\n",
    "            x_t_df = pd.DataFrame([x_t]).iloc[0]\n",
    "            prediction = predecir(self.arbol, x_t_df) \n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        \n",
    "        accuracy = sum(y_i == y_j for (y_i, y_j) in zip(y_pred, y_test)) / len(y_test)\n",
    "        return accuracy\n",
    "        \n",
    "def imprimir_arbol(arbol, spacing=\"\"):\n",
    "    \n",
    "    if isinstance(arbol, MiClasificadorArbol):\n",
    "        imprimir_arbol(arbol.arbol)\n",
    "        return\n",
    "    \n",
    "    if isinstance(arbol, Hoja):\n",
    "        print (spacing + \"Hoja:\", arbol.cuentas)\n",
    "        return\n",
    "\n",
    "    print (spacing + str(arbol.pregunta))\n",
    "\n",
    "    print (spacing + '--> True:')\n",
    "    imprimir_arbol(arbol.sub_arbol_izquierdo, spacing + \"  \")\n",
    "\n",
    "    print (spacing + '--> False:')\n",
    "    imprimir_arbol(arbol.sub_arbol_derecho, spacing + \"  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c60b17ca4879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_dev_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0marbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0marbol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomDecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracies_training = []\n",
    "accuracies_validation = []\n",
    "aucs_training = []\n",
    "aucs_validation = []\n",
    "\n",
    "# Puede serles de utilidad tener a X_dev e y_dev como matrices de numpy directamente:\n",
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: accuracies_training, accuracies_validation, aucs_training y aucs_validation asignados\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "for train_index, test_index in kf.split(X_dev):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "    y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "    \n",
    "    arbol = CustomDecisionTreeClassifier(max_depth=3)\n",
    "    arbol.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_test = arbol.predict(X_test)\n",
    "    y_pred_train = arbol.predict(X_train)\n",
    "    \n",
    "    ac_test=sklearn.metrics.accuracy_score(y_test, y_pred_test, normalize=True, sample_weight=None)\n",
    "    ac_train=sklearn.metrics.accuracy_score(y_train, y_pred_train, normalize=True, sample_weight=None)\n",
    "    accuracies_validation.append(ac_test)\n",
    "    accuracies_training.append(ac_train)\n",
    "    \n",
    "    roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test, sample_weight=None)\n",
    "    roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train, sample_weight=None)\n",
    "    aucs_training.append(roc_auc_train)\n",
    "    aucs_validation.append(roc_auc_test)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Permutación\"\n",
    "                  \n",
    "df[\"Accuracy (training)\"] = accuracies_training     # cambiar por accuracies_training\n",
    "df[\"Accuracy (validación)\"] = accuracies_validation   # cambiar por accuracies_validation\n",
    "df[\"AUC ROC (training)\"] = aucs_training     # cambiar por aucs_training\n",
    "df[\"AUC ROC (validación)\"] = aucs_validation    # [\"-\"] * 5 cambiar por aucs_validation\n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "# Descomentar las siguientes líneas para graficar el resultado\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 2 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura máxima</th>\n",
       "      <th>Criterio de evaluación de corte</th>\n",
       "      <th>AUC ROC promedio (training)</th>\n",
       "      <th>AUC ROC promedio (validación)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.6487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.6368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9789</td>\n",
       "      <td>0.6487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.6368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Altura máxima Criterio de evaluación de corte  AUC ROC promedio (training)  \\\n",
       "0             3                            Gini                       0.8818   \n",
       "1             5                            Gini                       0.9789   \n",
       "2     Inifinito                            Gini                       0.9968   \n",
       "3             3         Ganancia de Información                       0.8818   \n",
       "4             5         Ganancia de Información                       0.9789   \n",
       "5     Inifinito         Ganancia de Información                       0.9968   \n",
       "\n",
       "   AUC ROC promedio (validación)  \n",
       "0                         0.6801  \n",
       "1                         0.6487  \n",
       "2                         0.6368  \n",
       "3                         0.6801  \n",
       "4                         0.6487  \n",
       "5                         0.6368  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultados_training = []\n",
    "resultados_validation = []\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: resultados_training y resultados_validation asignadas\n",
    "#\n",
    "## Recomendamos seguir el siguiente esquema:\n",
    "np.random.seed(SEED)\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "     for altura in [3, 5, None]:\n",
    "        \n",
    "        lista_aucs_train = []\n",
    "        lista_aucs_test = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(X_dev):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "            y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "\n",
    "            arbol = CustomDecisionTreeClassifier(max_depth=altura, criterion=criterio)\n",
    "            arbol.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_test = arbol.predict(X_test)\n",
    "            y_pred_train = arbol.predict(X_train)\n",
    "\n",
    "            #ac_test=sklearn.metrics.accuracy_score(y_test, y_pred_test, normalize=True, sample_weight=None)\n",
    "            #ac_train=sklearn.metrics.accuracy_score(y_train, y_pred_train, normalize=True, sample_weight=None)\n",
    "            #accuracies_validation.append(ac_test)\n",
    "            #accuracies_training.append(ac_train)  \n",
    "            \n",
    "            #EN EL PDF DICE ACURACY, ACA FIGURA AUC ROC\n",
    "            \n",
    "            roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test, sample_weight=None)\n",
    "            roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train, sample_weight=None)\n",
    "            lista_aucs_train.append(roc_auc_train)\n",
    "            lista_aucs_test.append(roc_auc_test)\n",
    "                \n",
    "\n",
    "        resultados_training.append( np.mean(lista_aucs_train) )\n",
    "        resultados_validation.append( np.mean(lista_aucs_test) )\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = resultados_training # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = resultados_validation # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparación de algoritmos\n",
    "\n",
    "\n",
    "Se pide explorar distintas combinaciones de algoritmos de aprendizaje e hiperparámetros, de manera de buscar una performance óptima. Para este ejercicio es necesario que evalúen posibilidades utilizando la técnica de Grid Search. Como métrica de performance, usar siempre el área bajo la curva (AUC ROC) resultante de 5-fold cross-validation. \n",
    "\n",
    "Algoritmos a probar: KNN, árboles de decisión, LDA, Naive Bayes y SVM. Hiperparámetros: Revisar la documentación de cada uno para la búsqueda de combinaciones prometedoras.  \n",
    "\n",
    "Se pide generar un reporte que contenga: \n",
    "\n",
    "1. Una descripción de las distintas combinaciones consideradas y su performance asociada (las que consideren relevantes, con al menos la mejor combinación para cada algoritmo). \n",
    "\n",
    "1. Una breve explicación de los factores que creen que produjeron dicho resultado. \n",
    "\n",
    "En este punto evaluaremos tanto los hiperparámetros elegidos como las conclusiones relacionadas a por qué piensan que ciertos algoritmos funcionan mejor que otros para estos datos. \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Utilizar RandomizedSearchCV con rangos de parámetros que contengan a los utilizados en el GridSearch. Analizar si se encontraron mejores combinaciones de parámetros que no hayan sido tenidas en cuenta con el GridSearch y cuál fue la diferencia de tiempo de ejecución. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def top_resultados(grid, top=5):\n",
    "    ## Si quieren, pueden utilizar esta función para imprimir las mejores combinaciones de su grid\n",
    "    print(\"Top {} combinaciones\".format(top))\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    display(df.sort_values(by=\"mean_score_validation\", ascending=False).head(top))\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: comparar y explorar distintas combinaciones de parámetros para los algoritmos importados arriba\n",
    "\n",
    "X_eval_np = np.array(X_eval)\n",
    "y_eval_np = np.array(y_eval).ravel()\n",
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "X_train=X_dev_np\n",
    "X_test=X_eval_np\n",
    "y_train=y_dev_np\n",
    "y_test=y_eval_np\n",
    "\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un wrapper a ```GridSearchCV``` para conveniencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def GridSearch(model, tuned_parameters):\n",
    "    scores = ['roc_auc']\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(model, tuned_parameters, cv=5,\n",
    "                           scoring='%s' % score, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        #print(\"Detailed classification report:\")\n",
    "        #print()\n",
    "        #print(\"The model is trained on the full development set.\")\n",
    "        #print(\"The scores are computed on the full evaluation set.\")\n",
    "        #print()\n",
    "        #y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        #print(classification_report(y_true, y_pred))\n",
    "        #print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutamos efectivamente la grid search\n",
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_neighbors': 51, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.707 (+/-0.153) for {'n_neighbors': 1, 'p': 1, 'weights': 'uniform'}\n",
      "0.707 (+/-0.153) for {'n_neighbors': 1, 'p': 1, 'weights': 'distance'}\n",
      "0.708 (+/-0.070) for {'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}\n",
      "0.708 (+/-0.070) for {'n_neighbors': 1, 'p': 2, 'weights': 'distance'}\n",
      "0.866 (+/-0.081) for {'n_neighbors': 11, 'p': 1, 'weights': 'uniform'}\n",
      "0.871 (+/-0.086) for {'n_neighbors': 11, 'p': 1, 'weights': 'distance'}\n",
      "0.867 (+/-0.098) for {'n_neighbors': 11, 'p': 2, 'weights': 'uniform'}\n",
      "0.872 (+/-0.098) for {'n_neighbors': 11, 'p': 2, 'weights': 'distance'}\n",
      "0.874 (+/-0.106) for {'n_neighbors': 21, 'p': 1, 'weights': 'uniform'}\n",
      "0.876 (+/-0.108) for {'n_neighbors': 21, 'p': 1, 'weights': 'distance'}\n",
      "0.878 (+/-0.113) for {'n_neighbors': 21, 'p': 2, 'weights': 'uniform'}\n",
      "0.880 (+/-0.110) for {'n_neighbors': 21, 'p': 2, 'weights': 'distance'}\n",
      "0.876 (+/-0.108) for {'n_neighbors': 31, 'p': 1, 'weights': 'uniform'}\n",
      "0.878 (+/-0.109) for {'n_neighbors': 31, 'p': 1, 'weights': 'distance'}\n",
      "0.877 (+/-0.098) for {'n_neighbors': 31, 'p': 2, 'weights': 'uniform'}\n",
      "0.880 (+/-0.096) for {'n_neighbors': 31, 'p': 2, 'weights': 'distance'}\n",
      "0.875 (+/-0.111) for {'n_neighbors': 41, 'p': 1, 'weights': 'uniform'}\n",
      "0.878 (+/-0.108) for {'n_neighbors': 41, 'p': 1, 'weights': 'distance'}\n",
      "0.879 (+/-0.136) for {'n_neighbors': 41, 'p': 2, 'weights': 'uniform'}\n",
      "0.881 (+/-0.132) for {'n_neighbors': 41, 'p': 2, 'weights': 'distance'}\n",
      "0.884 (+/-0.123) for {'n_neighbors': 51, 'p': 1, 'weights': 'uniform'}\n",
      "0.885 (+/-0.121) for {'n_neighbors': 51, 'p': 1, 'weights': 'distance'}\n",
      "0.871 (+/-0.134) for {'n_neighbors': 51, 'p': 2, 'weights': 'uniform'}\n",
      "0.875 (+/-0.134) for {'n_neighbors': 51, 'p': 2, 'weights': 'distance'}\n",
      "0.877 (+/-0.125) for {'n_neighbors': 61, 'p': 1, 'weights': 'uniform'}\n",
      "0.880 (+/-0.122) for {'n_neighbors': 61, 'p': 1, 'weights': 'distance'}\n",
      "0.876 (+/-0.119) for {'n_neighbors': 61, 'p': 2, 'weights': 'uniform'}\n",
      "0.877 (+/-0.118) for {'n_neighbors': 61, 'p': 2, 'weights': 'distance'}\n",
      "0.874 (+/-0.129) for {'n_neighbors': 71, 'p': 1, 'weights': 'uniform'}\n",
      "0.876 (+/-0.129) for {'n_neighbors': 71, 'p': 1, 'weights': 'distance'}\n",
      "0.870 (+/-0.122) for {'n_neighbors': 71, 'p': 2, 'weights': 'uniform'}\n",
      "0.874 (+/-0.123) for {'n_neighbors': 71, 'p': 2, 'weights': 'distance'}\n",
      "0.874 (+/-0.119) for {'n_neighbors': 81, 'p': 1, 'weights': 'uniform'}\n",
      "0.875 (+/-0.117) for {'n_neighbors': 81, 'p': 1, 'weights': 'distance'}\n",
      "0.868 (+/-0.130) for {'n_neighbors': 81, 'p': 2, 'weights': 'uniform'}\n",
      "0.872 (+/-0.131) for {'n_neighbors': 81, 'p': 2, 'weights': 'distance'}\n",
      "0.867 (+/-0.125) for {'n_neighbors': 91, 'p': 1, 'weights': 'uniform'}\n",
      "0.868 (+/-0.120) for {'n_neighbors': 91, 'p': 1, 'weights': 'distance'}\n",
      "0.865 (+/-0.126) for {'n_neighbors': 91, 'p': 2, 'weights': 'uniform'}\n",
      "0.867 (+/-0.127) for {'n_neighbors': 91, 'p': 2, 'weights': 'distance'}\n",
      "0.865 (+/-0.126) for {'n_neighbors': 101, 'p': 1, 'weights': 'uniform'}\n",
      "0.866 (+/-0.126) for {'n_neighbors': 101, 'p': 1, 'weights': 'distance'}\n",
      "0.865 (+/-0.129) for {'n_neighbors': 101, 'p': 2, 'weights': 'uniform'}\n",
      "0.868 (+/-0.132) for {'n_neighbors': 101, 'p': 2, 'weights': 'distance'}\n",
      "0.866 (+/-0.127) for {'n_neighbors': 111, 'p': 1, 'weights': 'uniform'}\n",
      "0.866 (+/-0.127) for {'n_neighbors': 111, 'p': 1, 'weights': 'distance'}\n",
      "0.862 (+/-0.132) for {'n_neighbors': 111, 'p': 2, 'weights': 'uniform'}\n",
      "0.866 (+/-0.134) for {'n_neighbors': 111, 'p': 2, 'weights': 'distance'}\n",
      "0.868 (+/-0.136) for {'n_neighbors': 121, 'p': 1, 'weights': 'uniform'}\n",
      "0.868 (+/-0.137) for {'n_neighbors': 121, 'p': 1, 'weights': 'distance'}\n",
      "0.862 (+/-0.138) for {'n_neighbors': 121, 'p': 2, 'weights': 'uniform'}\n",
      "0.865 (+/-0.133) for {'n_neighbors': 121, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "CPU times: user 242 ms, sys: 129 ms, total: 371 ms\n",
      "Wall time: 3.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'n_neighbors':range(1, len(X_test)//2, 10), 'weights':[\"uniform\", \"distance\"], 'p':range(1, 3)}]\n",
    "%time GridSearch(KNeighborsClassifier(), tuned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'criterion': 'entropy', 'max_depth': 2}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.702 (+/-0.087) for {'criterion': 'gini', 'max_depth': 1}\n",
      "0.753 (+/-0.145) for {'criterion': 'gini', 'max_depth': 2}\n",
      "0.752 (+/-0.170) for {'criterion': 'gini', 'max_depth': 3}\n",
      "0.711 (+/-0.149) for {'criterion': 'gini', 'max_depth': 4}\n",
      "0.664 (+/-0.132) for {'criterion': 'gini', 'max_depth': 5}\n",
      "0.692 (+/-0.120) for {'criterion': 'entropy', 'max_depth': 1}\n",
      "0.765 (+/-0.096) for {'criterion': 'entropy', 'max_depth': 2}\n",
      "0.745 (+/-0.089) for {'criterion': 'entropy', 'max_depth': 3}\n",
      "0.741 (+/-0.081) for {'criterion': 'entropy', 'max_depth': 4}\n",
      "0.720 (+/-0.100) for {'criterion': 'entropy', 'max_depth': 5}\n",
      "\n",
      "CPU times: user 45.9 ms, sys: 8.07 ms, total: 54 ms\n",
      "Wall time: 314 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'max_depth':range(1,6) , 'criterion': [\"gini\", \"entropy\"]}]\n",
    "%time GridSearch(DecisionTreeClassifier(), tuned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.538 (+/-0.070) for {'solver': 'svd'}\n",
      "0.547 (+/-0.187) for {'shrinkage': None, 'solver': 'lsqr'}\n",
      "0.875 (+/-0.107) for {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
      "\n",
      "75.8 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#tuned_parameters = [{'solver':['svd','lsqr']}]\n",
    "#tuned_parameters = [{'solver':['lsqr'],'shrinkage':[None,'auto']}]\n",
    "#param_grid = [{'solver':['svd']},{'solver':['lsqr'],'shrinkage':[None,'auto']}]\n",
    "\n",
    "tuned_parameters= [{'solver':['svd']},{'solver':['lsqr'],'shrinkage':[None,'auto']}]\n",
    "\n",
    "%timeit GridSearch(LinearDiscriminantAnalysis(), tuned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = [{'priors':[None,[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.4,0.6],[0.5,0.5],[0.6,0.4],[0.7,0.3],[0.8,0.2],[0.9,0.1]]}]\n",
    "% timeit GridSearch(GaussianNB(), tuned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.0014677992676220691}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.869 (+/-0.126) for {'C': 0.0001}\n",
      "0.871 (+/-0.126) for {'C': 0.0001467799267622069}\n",
      "0.874 (+/-0.121) for {'C': 0.00021544346900318845}\n",
      "0.878 (+/-0.120) for {'C': 0.00031622776601683794}\n",
      "0.879 (+/-0.115) for {'C': 0.00046415888336127773}\n",
      "0.883 (+/-0.111) for {'C': 0.0006812920690579609}\n",
      "0.885 (+/-0.109) for {'C': 0.001}\n",
      "0.886 (+/-0.102) for {'C': 0.0014677992676220691}\n",
      "0.883 (+/-0.100) for {'C': 0.002154434690031882}\n",
      "0.884 (+/-0.098) for {'C': 0.0031622776601683794}\n",
      "0.883 (+/-0.103) for {'C': 0.004641588833612777}\n",
      "0.876 (+/-0.101) for {'C': 0.006812920690579608}\n",
      "0.875 (+/-0.099) for {'C': 0.01}\n",
      "0.868 (+/-0.102) for {'C': 0.01467799267622069}\n",
      "0.864 (+/-0.108) for {'C': 0.021544346900318822}\n",
      "0.861 (+/-0.106) for {'C': 0.03162277660168379}\n",
      "0.854 (+/-0.109) for {'C': 0.046415888336127774}\n",
      "0.853 (+/-0.111) for {'C': 0.06812920690579609}\n",
      "0.851 (+/-0.109) for {'C': 0.1}\n",
      "0.849 (+/-0.109) for {'C': 0.1467799267622069}\n",
      "0.848 (+/-0.110) for {'C': 0.21544346900318823}\n",
      "0.849 (+/-0.109) for {'C': 0.31622776601683794}\n",
      "0.848 (+/-0.110) for {'C': 0.46415888336127775}\n",
      "0.848 (+/-0.111) for {'C': 0.6812920690579608}\n",
      "0.848 (+/-0.112) for {'C': 1.0}\n",
      "0.848 (+/-0.113) for {'C': 1.4677992676220675}\n",
      "0.848 (+/-0.112) for {'C': 2.154434690031882}\n",
      "0.848 (+/-0.110) for {'C': 3.1622776601683795}\n",
      "0.848 (+/-0.111) for {'C': 4.641588833612772}\n",
      "0.848 (+/-0.111) for {'C': 6.812920690579608}\n",
      "0.848 (+/-0.109) for {'C': 10.0}\n",
      "0.848 (+/-0.109) for {'C': 14.677992676220676}\n",
      "0.848 (+/-0.109) for {'C': 21.54434690031882}\n",
      "0.848 (+/-0.109) for {'C': 31.622776601683793}\n",
      "0.848 (+/-0.109) for {'C': 46.41588833612773}\n",
      "0.848 (+/-0.109) for {'C': 68.12920690579608}\n",
      "0.848 (+/-0.109) for {'C': 100.0}\n",
      "0.848 (+/-0.109) for {'C': 146.77992676220674}\n",
      "0.848 (+/-0.109) for {'C': 215.44346900318823}\n",
      "0.848 (+/-0.109) for {'C': 316.22776601683796}\n",
      "0.848 (+/-0.109) for {'C': 464.1588833612773}\n",
      "0.848 (+/-0.109) for {'C': 681.2920690579608}\n",
      "0.848 (+/-0.109) for {'C': 1000.0}\n",
      "0.848 (+/-0.109) for {'C': 1467.7992676220676}\n",
      "0.848 (+/-0.109) for {'C': 2154.4346900318824}\n",
      "0.848 (+/-0.109) for {'C': 3162.2776601683795}\n",
      "0.848 (+/-0.109) for {'C': 4641.588833612773}\n",
      "0.848 (+/-0.109) for {'C': 6812.920690579608}\n",
      "0.848 (+/-0.109) for {'C': 10000.0}\n",
      "\n",
      "CPU times: user 77.9 ms, sys: 16 ms, total: 93.9 ms\n",
      "Wall time: 304 ms\n",
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.001539926526059492}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.885 (+/-0.109) for {'C': 0.001}\n",
      "0.886 (+/-0.107) for {'C': 0.0010491397291363103}\n",
      "0.885 (+/-0.106) for {'C': 0.0011006941712522092}\n",
      "0.885 (+/-0.109) for {'C': 0.0011547819846894581}\n",
      "0.884 (+/-0.109) for {'C': 0.001211527658628589}\n",
      "0.885 (+/-0.108) for {'C': 0.0012710617996147444}\n",
      "0.885 (+/-0.106) for {'C': 0.001333521432163324}\n",
      "0.885 (+/-0.108) for {'C': 0.0013990503141372943}\n",
      "0.886 (+/-0.102) for {'C': 0.0014677992676220691}\n",
      "0.887 (+/-0.100) for {'C': 0.001539926526059492}\n",
      "0.886 (+/-0.100) for {'C': 0.0016155980984398745}\n",
      "0.886 (+/-0.100) for {'C': 0.0016949881513903461}\n",
      "0.886 (+/-0.100) for {'C': 0.0017782794100389228}\n",
      "0.885 (+/-0.101) for {'C': 0.001865663578576913}\n",
      "0.885 (+/-0.100) for {'C': 0.0019573417814876598}\n",
      "0.884 (+/-0.100) for {'C': 0.002053525026457146}\n",
      "0.883 (+/-0.100) for {'C': 0.0021544346900318843}\n",
      "0.883 (+/-0.101) for {'C': 0.0022603030271419193}\n",
      "0.884 (+/-0.098) for {'C': 0.0023713737056616554}\n",
      "0.884 (+/-0.097) for {'C': 0.002487902367238837}\n",
      "0.884 (+/-0.098) for {'C': 0.0026101572156825357}\n",
      "0.884 (+/-0.098) for {'C': 0.0027384196342643613}\n",
      "0.884 (+/-0.099) for {'C': 0.0028729848333536655}\n",
      "0.885 (+/-0.098) for {'C': 0.003014162529877389}\n",
      "0.884 (+/-0.098) for {'C': 0.0031622776601683794}\n",
      "0.885 (+/-0.099) for {'C': 0.0033176711278428547}\n",
      "0.885 (+/-0.100) for {'C': 0.0034807005884284095}\n",
      "0.885 (+/-0.099) for {'C': 0.003651741272548377}\n",
      "0.885 (+/-0.101) for {'C': 0.003831186849557285}\n",
      "0.884 (+/-0.103) for {'C': 0.004019450333615123}\n",
      "0.884 (+/-0.104) for {'C': 0.004216965034285823}\n",
      "0.882 (+/-0.104) for {'C': 0.004424185553847914}\n",
      "0.883 (+/-0.103) for {'C': 0.004641588833612777}\n",
      "0.882 (+/-0.104) for {'C': 0.004869675251658631}\n",
      "0.882 (+/-0.104) for {'C': 0.005108969774506924}\n",
      "0.881 (+/-0.103) for {'C': 0.00536002316539179}\n",
      "0.879 (+/-0.103) for {'C': 0.005623413251903491}\n",
      "0.878 (+/-0.101) for {'C': 0.005899746255923559}\n",
      "0.878 (+/-0.102) for {'C': 0.006189658188912603}\n",
      "0.878 (+/-0.103) for {'C': 0.006493816315762113}\n",
      "0.876 (+/-0.101) for {'C': 0.006812920690579608}\n",
      "0.876 (+/-0.101) for {'C': 0.007147705767941853}\n",
      "0.875 (+/-0.100) for {'C': 0.007498942093324558}\n",
      "0.876 (+/-0.102) for {'C': 0.007867438076599394}\n",
      "0.876 (+/-0.101) for {'C': 0.008254041852680182}\n",
      "0.877 (+/-0.099) for {'C': 0.008659643233600654}\n",
      "0.876 (+/-0.099) for {'C': 0.009085175756516862}\n",
      "0.875 (+/-0.100) for {'C': 0.009531618832347872}\n",
      "0.875 (+/-0.099) for {'C': 0.01}\n",
      "\n",
      "CPU times: user 76.9 ms, sys: 4.1 ms, total: 81 ms\n",
      "Wall time: 255 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "a=np.logspace(-4, 4, num=49, endpoint=True, base=10.0, dtype=None)\n",
    "tuned_parameters = [{'C':a}]\n",
    "%time GridSearch(LinearSVC(), tuned_parameters)\n",
    "\n",
    "a=np.logspace(-3, -2, num=49, endpoint=True, base=10.0, dtype=None)\n",
    "tuned_parameters = [{'C':a}]\n",
    "%time GridSearch(LinearSVC(), tuned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Grid Search\n",
    "\n",
    "Presentamos a continuación los diferentes modelos utilizados, con los hiper-parámetros barridos, el mejor set, y la performance de dicho set. En el modelo LinearSVC se realizaron dos barridos. En el modelo GaussianNB todos los barridos obtuvieron exactamente la misma performance. En el modelo LDA presentamos la performance de todos los sets de hiper-parámetros.\n",
    "\n",
    "**KNN**: \n",
    "\n",
    "Tuned parameters: {'n_neighbors': range(1, 121, 10), 'weights': ['uniform', 'distance'], 'p': range(1, 3)}<br>\n",
    "**Best** parameters set: {'n_neighbors': 51, 'p': 1, 'weights': 'distance'}  <br> \n",
    "Best score: 0.885 (+/-0.121)\n",
    "\n",
    "\n",
    "**Árboles**:\n",
    "\n",
    "Tuned parameters: {'max_depth': range(1,6), 'criterion': ['gini', 'entropy']}<br>\n",
    "**Best** parameters set: {'criterion': 'entropy', 'max_depth': 2}<br>\n",
    "Best score: 0.765 (+/-0.096)\n",
    "\n",
    "**LDA**:\n",
    "\n",
    "Tuned parameters: {'solver': ['svd']}, {'solver': ['lsqr'], 'shrinkage': [None,'auto']}<br>\n",
    "**Best** parameters set: {'shrinkage': 'auto', 'solver': 'lsqr'}<br>\n",
    "Best score: 0.875 (+/-0.107)<br>\n",
    "Other scores:<br>\n",
    "0.538 (+/-0.070) for {'solver': 'svd'}<br>\n",
    "0.480 (+/-0.206) for {'shrinkage': None, 'solver': 'lsqr'}\n",
    "\n",
    "\n",
    "**GaussianNB**:\n",
    "\n",
    "Tuned parameters: {'priors': [None, [0.1, 0.9], [0.2, 0.8], [0.3, 0.7], [0.4, 0.6], [0.5, 0.5], [0.6, 0.4], [0.7, 0.3], [0.8, 0.2], [0.9, 0.1]]}<br>\n",
    "**Best** parameters set: {'priors': None}<br>\n",
    "Best score: 0.877 (+/-0.083)\n",
    "\n",
    "**LinearSVC**:\n",
    "\n",
    "Tuned parameters: {'C': np.logspace(-4, 4, num=49, base=10.0)}<br>\n",
    "&<br>\n",
    "Tuned parameters: {'C': np.logspace(-3, -2, num=49, base=10.0)}<br>\n",
    "**Best** parameters set: {'C': 0.001539926526059492}<br>\n",
    "Best score: 0.887 (+/-0.100)\n",
    "\n",
    "**Conclusiones**:\n",
    "\n",
    "Exceptuando el método de árboles, en todos los métodos obtenemos una performance similar de aproximadamente $0.88 \\pm 0.1$\n",
    "\n",
    "En el caso del método LDA vemos que la performance mejora considerablemente en el mejor set contra los otros dos, algo que no ocurre con el resto, en los que los resultados se mantienen similares entre sí. En la documentación del algoritmo encontramos que la herramienta \"shrinkage\" mejora la estimación de la matriz de covarianza en el caso en que el número de instancias de entrenamiento es chico comparado con el número de atributos. En nuestro caso tenemos 250 instancias de entrenamiento y 200 atributos en cada una de ellas, por lo que resulta coherente este resultado.\n",
    "\n",
    "El mejor resultado obtenido lo encontramos utilizando LinearSVC, aunque por un margen pequeño. Es impotante destacar que todos los modelos tienen performances similares y que ninguno de ellos supera el 88% en roc auc, con la excepción de los árboles de decisión, que tienen un rendimiento notoriamente inferior. Estos resultados sugieren que ningún modelo, en principio, superaria ese ~12% error y por tanto este porcentaje correspondería a ruido en la muestra. También parecen indicar que la muestra es (casi) linealmente separable, y por eso todos los modelos son parecidos entre sí (pues todos los modelos que se usan son lineales). El caso del arbol de decisión indicaría que los datos no son linealmente separables _en una dimensión_, pero viendo los resultados de otros clasificadores sí lo serían considerando las 200 dimensiones de cada instancia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def RandomSearch(model, tuned_parameters, n_iter):\n",
    "    scores = ['roc_auc']\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = RandomizedSearchCV(model,param_distributions=tuned_parameters, n_iter=n_iter,cv=5,\n",
    "                           scoring='%s' % score, random_state = 1234, n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'weights': 'distance', 'p': 1, 'n_neighbors': 51}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.865 (+/-0.126) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 101}\n",
      "0.877 (+/-0.118) for {'weights': 'distance', 'p': 2, 'n_neighbors': 61}\n",
      "0.867 (+/-0.127) for {'weights': 'distance', 'p': 2, 'n_neighbors': 91}\n",
      "0.866 (+/-0.127) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 111}\n",
      "0.867 (+/-0.098) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 11}\n",
      "0.862 (+/-0.132) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 111}\n",
      "0.878 (+/-0.109) for {'weights': 'distance', 'p': 1, 'n_neighbors': 31}\n",
      "0.874 (+/-0.106) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 21}\n",
      "0.868 (+/-0.137) for {'weights': 'distance', 'p': 1, 'n_neighbors': 121}\n",
      "0.874 (+/-0.123) for {'weights': 'distance', 'p': 2, 'n_neighbors': 71}\n",
      "0.866 (+/-0.081) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 11}\n",
      "0.885 (+/-0.121) for {'weights': 'distance', 'p': 1, 'n_neighbors': 51}\n",
      "0.866 (+/-0.127) for {'weights': 'distance', 'p': 1, 'n_neighbors': 111}\n",
      "0.865 (+/-0.133) for {'weights': 'distance', 'p': 2, 'n_neighbors': 121}\n",
      "0.879 (+/-0.136) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 41}\n",
      "0.884 (+/-0.123) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 51}\n",
      "0.878 (+/-0.113) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 21}\n",
      "0.872 (+/-0.098) for {'weights': 'distance', 'p': 2, 'n_neighbors': 11}\n",
      "0.877 (+/-0.098) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 31}\n",
      "0.878 (+/-0.108) for {'weights': 'distance', 'p': 1, 'n_neighbors': 41}\n",
      "0.871 (+/-0.134) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 51}\n",
      "0.880 (+/-0.122) for {'weights': 'distance', 'p': 1, 'n_neighbors': 61}\n",
      "0.876 (+/-0.129) for {'weights': 'distance', 'p': 1, 'n_neighbors': 71}\n",
      "0.707 (+/-0.153) for {'weights': 'distance', 'p': 1, 'n_neighbors': 1}\n",
      "0.872 (+/-0.131) for {'weights': 'distance', 'p': 2, 'n_neighbors': 81}\n",
      "0.862 (+/-0.138) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 121}\n",
      "0.708 (+/-0.070) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 1}\n",
      "0.875 (+/-0.117) for {'weights': 'distance', 'p': 1, 'n_neighbors': 81}\n",
      "0.868 (+/-0.120) for {'weights': 'distance', 'p': 1, 'n_neighbors': 91}\n",
      "0.874 (+/-0.119) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 81}\n",
      "0.867 (+/-0.125) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 91}\n",
      "0.707 (+/-0.153) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 1}\n",
      "0.880 (+/-0.110) for {'weights': 'distance', 'p': 2, 'n_neighbors': 21}\n",
      "0.708 (+/-0.070) for {'weights': 'distance', 'p': 2, 'n_neighbors': 1}\n",
      "0.868 (+/-0.130) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 81}\n",
      "0.868 (+/-0.136) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 121}\n",
      "0.876 (+/-0.108) for {'weights': 'distance', 'p': 1, 'n_neighbors': 21}\n",
      "0.875 (+/-0.111) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 41}\n",
      "0.871 (+/-0.086) for {'weights': 'distance', 'p': 1, 'n_neighbors': 11}\n",
      "0.874 (+/-0.129) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 71}\n",
      "0.868 (+/-0.132) for {'weights': 'distance', 'p': 2, 'n_neighbors': 101}\n",
      "0.865 (+/-0.129) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 101}\n",
      "0.870 (+/-0.122) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 71}\n",
      "0.876 (+/-0.119) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 61}\n",
      "0.866 (+/-0.126) for {'weights': 'distance', 'p': 1, 'n_neighbors': 101}\n",
      "0.875 (+/-0.134) for {'weights': 'distance', 'p': 2, 'n_neighbors': 51}\n",
      "0.880 (+/-0.096) for {'weights': 'distance', 'p': 2, 'n_neighbors': 31}\n",
      "0.877 (+/-0.125) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 61}\n",
      "0.876 (+/-0.108) for {'weights': 'uniform', 'p': 1, 'n_neighbors': 31}\n",
      "0.865 (+/-0.126) for {'weights': 'uniform', 'p': 2, 'n_neighbors': 91}\n",
      "\n",
      "CPU times: user 104 ms, sys: 8.4 ms, total: 112 ms\n",
      "Wall time: 693 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {'n_neighbors':range(1, len(X_test)//2, 10), 'weights':[\"uniform\", \"distance\"], 'p':range(1, 3)}\n",
    "%time RandomSearch(KNeighborsClassifier(), tuned_parameters,n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 3, 'criterion': 'gini'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.683 (+/-0.110) for {'max_depth': 9, 'criterion': 'gini'}\n",
      "0.753 (+/-0.145) for {'max_depth': 2, 'criterion': 'gini'}\n",
      "0.690 (+/-0.122) for {'max_depth': 11, 'criterion': 'entropy'}\n",
      "0.669 (+/-0.135) for {'max_depth': 5, 'criterion': 'gini'}\n",
      "0.693 (+/-0.173) for {'max_depth': 4, 'criterion': 'gini'}\n",
      "0.666 (+/-0.175) for {'max_depth': 8, 'criterion': 'gini'}\n",
      "0.719 (+/-0.134) for {'max_depth': 5, 'criterion': 'entropy'}\n",
      "0.686 (+/-0.099) for {'max_depth': 11, 'criterion': 'gini'}\n",
      "0.692 (+/-0.120) for {'max_depth': 1, 'criterion': 'entropy'}\n",
      "0.652 (+/-0.128) for {'max_depth': 14, 'criterion': 'gini'}\n",
      "0.755 (+/-0.202) for {'max_depth': 3, 'criterion': 'gini'}\n",
      "0.679 (+/-0.096) for {'max_depth': 12, 'criterion': 'entropy'}\n",
      "0.683 (+/-0.124) for {'max_depth': 13, 'criterion': 'entropy'}\n",
      "0.702 (+/-0.087) for {'max_depth': 1, 'criterion': 'gini'}\n",
      "0.673 (+/-0.123) for {'max_depth': 6, 'criterion': 'gini'}\n",
      "0.686 (+/-0.087) for {'max_depth': 9, 'criterion': 'entropy'}\n",
      "0.740 (+/-0.085) for {'max_depth': 3, 'criterion': 'entropy'}\n",
      "0.688 (+/-0.093) for {'max_depth': 10, 'criterion': 'entropy'}\n",
      "0.683 (+/-0.104) for {'max_depth': 12, 'criterion': 'gini'}\n",
      "0.660 (+/-0.133) for {'max_depth': 10, 'criterion': 'gini'}\n",
      "\n",
      "CPU times: user 64.8 ms, sys: 303 µs, total: 65.1 ms\n",
      "Wall time: 445 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {'max_depth':range(1,15) , 'criterion': [\"gini\", \"entropy\"]}\n",
    "n_iter=20\n",
    "%time RandomSearch(DecisionTreeClassifier(), tuned_parameters,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.0015025598101093456}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.877 (+/-0.100) for {'C': 0.00850666436052429}\n",
      "0.885 (+/-0.101) for {'C': 0.0038078147056072194}\n",
      "0.883 (+/-0.101) for {'C': 0.002192115377602205}\n",
      "0.884 (+/-0.103) for {'C': 0.004038369582315153}\n",
      "0.876 (+/-0.102) for {'C': 0.007845288992683828}\n",
      "0.876 (+/-0.101) for {'C': 0.006696505941218151}\n",
      "0.883 (+/-0.101) for {'C': 0.002108595328061282}\n",
      "0.885 (+/-0.100) for {'C': 0.003629573526144963}\n",
      "0.883 (+/-0.096) for {'C': 0.0025846046655082725}\n",
      "0.885 (+/-0.108) for {'C': 0.0012430567176970216}\n",
      "0.886 (+/-0.101) for {'C': 0.00172608576369937}\n",
      "0.884 (+/-0.109) for {'C': 0.0012084281186108957}\n",
      "0.875 (+/-0.100) for {'C': 0.009542008440816466}\n",
      "0.883 (+/-0.100) for {'C': 0.0021852614025497023}\n",
      "0.883 (+/-0.097) for {'C': 0.002551784930795491}\n",
      "0.884 (+/-0.109) for {'C': 0.001203928806587987}\n",
      "0.880 (+/-0.101) for {'C': 0.005432345079311948}\n",
      "0.878 (+/-0.102) for {'C': 0.006197662495415088}\n",
      "0.878 (+/-0.102) for {'C': 0.00627029498869139}\n",
      "0.881 (+/-0.103) for {'C': 0.005392713127988336}\n",
      "0.877 (+/-0.099) for {'C': 0.008611907391906634}\n",
      "0.877 (+/-0.099) for {'C': 0.008614088951442136}\n",
      "0.885 (+/-0.097) for {'C': 0.003118781431367158}\n",
      "0.882 (+/-0.104) for {'C': 0.00479674696532278}\n",
      "0.884 (+/-0.100) for {'C': 0.002305226625880903}\n",
      "0.885 (+/-0.107) for {'C': 0.0010920446169200447}\n",
      "0.886 (+/-0.099) for {'C': 0.0016698298748205494}\n",
      "0.886 (+/-0.100) for {'C': 0.001643622803578521}\n",
      "0.878 (+/-0.102) for {'C': 0.006333707696066611}\n",
      "0.878 (+/-0.101) for {'C': 0.0059889098183147004}\n",
      "0.884 (+/-0.098) for {'C': 0.0032123663521415147}\n",
      "0.879 (+/-0.102) for {'C': 0.0058236819934767235}\n",
      "0.886 (+/-0.101) for {'C': 0.001725489691780242}\n",
      "0.884 (+/-0.097) for {'C': 0.0024861306928731838}\n",
      "0.886 (+/-0.100) for {'C': 0.0016302421682693542}\n",
      "0.885 (+/-0.106) for {'C': 0.0013215123031089277}\n",
      "0.879 (+/-0.102) for {'C': 0.005769359629414668}\n",
      "0.878 (+/-0.101) for {'C': 0.006595355237507737}\n",
      "0.884 (+/-0.098) for {'C': 0.003194074513636341}\n",
      "0.887 (+/-0.100) for {'C': 0.0015025598101093456}\n",
      "0.884 (+/-0.103) for {'C': 0.004008722835908645}\n",
      "0.885 (+/-0.099) for {'C': 0.002954028657439477}\n",
      "0.885 (+/-0.109) for {'C': 0.0011642082172402036}\n",
      "0.882 (+/-0.103) for {'C': 0.005012299348445966}\n",
      "0.885 (+/-0.107) for {'C': 0.0010887053801183683}\n",
      "0.885 (+/-0.108) for {'C': 0.001390917697691263}\n",
      "0.884 (+/-0.107) for {'C': 0.0013733188390735803}\n",
      "0.885 (+/-0.106) for {'C': 0.0012965256938672809}\n",
      "0.882 (+/-0.104) for {'C': 0.004881090783341671}\n",
      "0.885 (+/-0.102) for {'C': 0.003967125912558048}\n",
      "\n",
      "CPU times: user 83.2 ms, sys: 8.02 ms, total: 91.2 ms\n",
      "Wall time: 247 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/Downloads/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "a=np.logspace(-3, -2, num=100000, endpoint=True, base=10.0, dtype=None)\n",
    "tuned_parameters = {'C':a}\n",
    "n_iter=50\n",
    "%time RandomSearch(LinearSVC(), tuned_parameters,n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones Extra\n",
    "\n",
    "Presentamos los tres modelos en los que realizamos RandomSearch, con el mejor set de hiper-parámetros, la performance de dicho set, y el tiempo de ejecución (comparado con el de GridSearch).\n",
    "Recordemos que la idea de esta técnica es intentar encontrar un mejor set de hiperparámetros que pudieran no haber sido detectados debido a la discretización regular del espacio de búsqueda, pero sin empeorar los tiempos de cómputo, por lo que es esperable que sean similares a los del GridSearch.\n",
    "\n",
    "**KNN:** \n",
    "Best score: 0.885 (+/-0.121) for {'weights': 'distance', 'p': 1, 'n_neighbors': 51}\n",
    "RandomSearch wall time: 7.01 s\n",
    "GRIDSearch wall time: 7.56 s\n",
    "\n",
    "**Arboles:**\n",
    "Best score: 0.765 (+/-0.096) for {'max_depth': 2, 'criterion': 'entropy'}\n",
    "Wall time: 1.89 s\n",
    "GRID: Wall time: 1.6 s\n",
    "\n",
    "**Linear SVC:**\n",
    "Best score: 0.887 (+/-0.100) for {'C': 0.0015262013596464125}\n",
    "Wall time: 1.86 s\n",
    "GRID: Wall time: 1.82 s\n",
    "\n",
    "Comencemos destacando que los tiempos de ejecución entre ambas técnicas son similares y por tanto coincidentes con lo que esperábamos. En cuanto a los resultados, tanto en kNN como en Arboles de decision se encontraron los mismos conjuntos de hiperparámetros. Esto es así porque el espacio de búsqueda de hiperparámetros para ambos modelos es _discreto_ y por tanto RandomSearch no presenta mejoras posibles, dado que la técnica suele encontrar mejores parámetros en espacios de búsqueda densos (de hecho, como las posibilidades que se prueban se toman al azar, RandomSearch podría encontrar peores combinaciones de hiperparámetros que GridSearch en espacios discretos). En cuanto a LinearSVC, un mejor valor para C fue encontrado, lo cual es coherente tomando en cuenta que es un valor real y que por tanto la discretización del GridSearch es más proclive a saltear puntos o _zonas_ con mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: \n",
    "### Diagnóstico Sesgo-Varianza. \n",
    "\n",
    "En este punto, se pide inspeccionar dos de sus mejores modelos encontrados hasta ahora: el mejor modelo de tipo árbol de decisión y el mejor de tipo SVM. Para ello:\n",
    "\n",
    "1. Graficar curvas de complejidad para cada modelo, variando la profundidad en el caso de árboles, y el hiperparámetro C en el caso de SVM. Diagnosticar cómo afectan al sesgo y a la varianza esos dos hiperparámetros.\n",
    "2. Graficar curvas de aprendizaje para cada modelo. En base a estas curvas, sacar conclusiones sobre si los algoritmos parecen haber alcanzado su límite, o bien si aumentar la cantidad de datos debería ayudar.\n",
    "3. Construir un modelo RandomForest con 200 árboles. Explorar para qué sirve el hiperparámetro max_features y cómo afecta a la performance del algoritmo mediante una curva de complejidad. Explicar por qué creen que se dieron los resultados obtenidos. Por último, graficar una curva de aprendizaje sobre los parámetros elegidos para determinar si sería útil o no conseguir más datos (usar  grid search para encontrar una buena combinación de parámetros).  \n",
    "\n",
    "\n",
    "**Atención**: Tener en cuenta que debemos seguir utilizando ROC AUC como métrica para estas curvas.\n",
    "\n",
    "**ver**: http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\n",
    "\n",
    "----\n",
    "**EJERCICIO EXTRA:** Utilizar RandomizedSearchCV para explorar la performance del algoritmo de Gradient Boosting y comparar con los resultados obtenidos en el punto (c).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f00c3f660f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##CORREGIDO\n",
    "\n",
    "# Punto 1\n",
    "\n",
    "roc_auc_tree_train = []\n",
    "roc_auc_tree_test = []\n",
    "\n",
    "tree_lengths = np.arange(1, 20)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=1234)\n",
    "\n",
    "for length in tree_lengths:\n",
    "\n",
    "    lista_aucs_train = []\n",
    "    lista_aucs_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_dev):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "        y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "\n",
    "        arbol = DecisionTreeClassifier(max_depth=length, criterion='entropy')\n",
    "        arbol.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_test = arbol.predict_proba(X_test)\n",
    "        y_pred_train = arbol.predict_proba(X_train)\n",
    "\n",
    "        y_pred_test = y_pred_test[:,1]\n",
    "        y_pred_train = y_pred_train[:,1]\n",
    "        \n",
    "        #print(y_pred_train)\n",
    "        \n",
    "        roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test, sample_weight=None)\n",
    "        roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train, sample_weight=None)\n",
    "        lista_aucs_train.append(roc_auc_train)\n",
    "        lista_aucs_test.append(roc_auc_test)\n",
    "\n",
    "\n",
    "    roc_auc_tree_train.append( np.mean(lista_aucs_train) )\n",
    "    roc_auc_tree_test.append( np.mean(lista_aucs_test) )\n",
    "    \n",
    "print(y_pred_test)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(tree_lengths, roc_auc_tree_train, color = 'blue', label = 'datos de entrenamiento')\n",
    "plt.plot(tree_lengths, roc_auc_tree_test, color = 'red', label = 'datos de validación')\n",
    "plt.xlabel('Altura del árbol')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Curva de complejidad: Árboles de decisión')\n",
    "plt.grid(True)\n",
    "plt.legend()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "\n",
    "En el caso de los árboles de decisión, el hiperparámetro que indica la complejidad es su altura; al permitir nodos más profundos, el modelo separa las instancias en un mayor número de regiones, de esa manera complejizándose. \n",
    "Al graficar la ROC AUC en función de esta complejidad para los datos de entrenamiento, vemos que la métrica aumenta hasta alcanzar un valor perfecto (cerca de la altura 5). Teniendo en cuenta que la misma métrica es bastante más baja sobre los datos de validación para esas alturas, podemos concluir que el modelo empieza a _overfittear_. Esto implica que se tiene un sesgo bajo para alturas grandes, pero también una alta varianza, ya que diferentes conjuntos de entrenamiento resultarán en modelos que se (sobre)ajustarán de manera diferente a sus datos. \n",
    "\n",
    "En cambio, para una altura baja, si existieran _predictores fuertes_ que separen bien a los datos se tendría una varianza baja, puesto que los diferentes árboles resultarían similares al utilizar estos predictores cerca de la raiz.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8231 0.5159 0.7298 0.235  0.1055 0.5039 0.2913 0.744  0.4257 0.3298\n",
      " 0.1476 0.0573 0.4734 0.5154 0.5358 0.567  0.894  0.7111 0.3158 0.0568\n",
      " 0.7618 0.5122 0.1549 0.7717 0.4299 0.8062 0.3729 0.339  0.83   0.8954\n",
      " 0.4167 0.4606 0.5492 0.7513 0.0777 0.5017 0.0619 0.7394 0.2906 0.2879\n",
      " 0.4412 0.2439 0.5134 0.4136 0.5592 0.3499 0.4025 0.155  0.6722 0.5687]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f00c3ed62b0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#¿SVC PUEDE DEVOLVER PROBAS? SI NO PUEDE, ¿COMO SE HACE EL AUC ROC?\n",
    "# parece estar corregido \n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "roc_auc_svm_train = []\n",
    "roc_auc_svm_test = []\n",
    "\n",
    "svm_c = np.logspace(-5, 2, 100)\n",
    "\n",
    "for c in svm_c:\n",
    "    \n",
    "    lista_aucs_train = []\n",
    "    lista_aucs_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_dev):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "        y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "\n",
    "        svm = LinearSVC(C=c)\n",
    "        clf = CalibratedClassifierCV(svm) \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_test = clf.predict_proba(X_test)\n",
    "        y_pred_train = clf.predict_proba(X_train)\n",
    "        \n",
    "        y_pred_test = y_pred_test[:,1]\n",
    "        y_pred_train = y_pred_train[:,1]\n",
    "\n",
    "        roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test, sample_weight=None)\n",
    "        roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train, sample_weight=None)\n",
    "        lista_aucs_train.append(roc_auc_train)\n",
    "        lista_aucs_test.append(roc_auc_test)\n",
    "\n",
    "\n",
    "    roc_auc_svm_train.append( np.mean(lista_aucs_train) )\n",
    "    roc_auc_svm_test.append( np.mean(lista_aucs_test) )\n",
    "    \n",
    "print(y_pred_test)\n",
    "plt.figure()\n",
    "plt.semilogx(svm_c, np.array(roc_auc_svm_train), color = 'blue', label = 'datos de entrenamiento')\n",
    "plt.semilogx(svm_c, np.array(roc_auc_svm_test), color = 'red', label = 'datos de validación')\n",
    "plt.xlabel('Hiperparámetro C')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Curva de complejidad: Linear Vector Classifier')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "\n",
    "En el caso del SVM lineal, el hiperparámetro que cuantifica la complejidad del modelo es el $C$. Éste indica el \"presupuesto\" que se tiene para que instancias puedan caer dentro de un margen del hiperplano separador. Si se aumenta su valor, se permite que más instancias se ubiquen dentro de este margen. Esto hace al modelo más complejo, ya que habrán más opciones de separación. \n",
    "\n",
    "Estudiando la ROC AUC sobre datos de entrenamiento y validación en función de este hiperparámetro $C$, vemos un comportamiento similar al observado para los árboles de decisión. Se tiene un valor $\\left( C \\sim 10^{-2} \\right)$ a partir del cual esta métrica es perfecta para los datos de entrenamiento pero bastante inferior para los de validación.\n",
    "Nuevamente, para complejidad mayor (en este caso, un $C$ más grande) se obtienen modelos con menor sesgo pero mayor varianza. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 2\n",
    "\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f00c3ddcb38>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_sizes, train_scores_tree, valid_scores_tree = learning_curve(\n",
    "    DecisionTreeClassifier(max_depth=2, criterion='entropy'), X_dev_np, y_dev_np, train_sizes=range(5, 199, 5), cv=5, \n",
    "    scoring = 'roc_auc', n_jobs=-1)\n",
    "\n",
    "#train_sizes, train_scores_tree, valid_scores_tree = learning_curve(\n",
    "#    DecisionTreeClassifier(max_depth=2, criterion='entropy'), X, y, train_sizes=range(10, 399, 10), cv=5, \n",
    "#    scoring = 'roc_auc')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, 1-np.mean(train_scores_tree, axis = 1), color = 'blue', label = 'Error de entrenamiento')\n",
    "plt.plot(train_sizes, 1-np.mean(valid_scores_tree, axis = 1), color = 'red', label = 'Error de validación')\n",
    "plt.ylim(0, 0.6)\n",
    "plt.xlabel('Datos de entrenamiento')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Curva de aprendizaje: Árboles de decisión')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "train_sizes, train_scores_svm, valid_scores_svm = learning_curve(\n",
    "    LinearSVC(C=0.0015), X_dev_np, y_dev_np, train_sizes=range(5, 199, 5), cv=5, \n",
    "    scoring = 'roc_auc')\n",
    "\n",
    "#train_sizes, train_scores_svm, valid_scores_svm = learning_curve(\n",
    "#    LinearSVC(C=0.0015), X, y,train_sizes=range(10, 399, 10), cv=5, \n",
    "#    scoring = 'roc_auc')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, 1-np.mean(train_scores_svm, axis = 1), color = 'blue', label = 'Error de entrenamiento')\n",
    "plt.plot(train_sizes, 1-np.mean(valid_scores_svm, axis = 1), color = 'red', label = 'Error de validación')\n",
    "plt.ylim(0,0.6)\n",
    "plt.xlabel('Datos de entrenamiento')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Curva de aprendizaje: Linear Vector Classifier')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "En el caso del árbol de decisión vemos que la varianza (en el gŕafico vista como la distancia entre las curvas de error de validación y entrenamiento) disminuye a medida que aumenta la cantidad de datos de entrenamiento. Por otro lado, vemos también que aumenta el sesgo, siendo éste el error sobre los datos de entrenamiento. Este aumento sólo es pronunciado _hasta los 100_ datos de entrenamiento, y luego parece mantenerse aproximadamente consante; mientras que el error de validación continúa disminuyendo en el mismo rango. Estos dos factores nos permiten pensar que al aumentar la cantidad de datos continuarán acercándose ambas curvas, ya que el error de validación baja y el de entrenamiento se mantiene. \n",
    "Entonces, podemos concluir que es conveniente aumentar la cantidad de datos para el entrenamiento; puesto que el sesgo se mantendrá y la varianza disminuirá. \n",
    "\n",
    "En cambio, para la SVM vemos otro comportamiento. El sesgo es relativamente bajo (en comparación con el árbol de decisión) y no parece aumentar notablemente. El error de validación disminuye ligeramente a medida que se incrementan los datos de entrenamiento; no obstante, su disminución se reduce al aumentar los datos. De esa forma, vemos que la varianza se achica gradualmente, pero parece estabilizarse al tomar el total de los datos. \n",
    "Por lo tanto, en este caso no consideramos que sea útil aumentar la cantidad de datos, pues ya parece llegarse al límite del algoritmo y no habrá mayor reducción de varianza.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto 3\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#forest = RandomForestClassifier(n_estimators = 200, max_features='auto')\n",
    "# max_features impone cuántos atributos (del total) se consideran al partir desde un nodo. El 'auto' es sqrt(n_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08  0.17  0.795 0.15  0.185 0.125 0.805 0.12  0.895 0.21  0.12  0.12\n",
      " 0.14  0.2   0.175 0.19  0.865 0.84  0.86  0.115 0.1   0.13  0.715 0.075\n",
      " 0.235 0.785 0.12  0.755 0.865 0.07  0.19  0.8   0.78  0.18  0.15  0.85\n",
      " 0.82  0.19  0.175 0.205 0.06  0.825 0.87  0.085 0.235 0.11  0.18  0.14\n",
      " 0.17  0.11  0.11  0.785 0.805 0.12  0.22  0.85  0.125 0.08  0.82  0.125\n",
      " 0.145 0.19  0.21  0.9   0.795 0.86  0.125 0.88  0.16  0.81  0.155 0.77\n",
      " 0.18  0.775 0.105 0.83  0.09  0.085 0.865 0.08  0.115 0.11  0.84  0.09\n",
      " 0.18  0.895 0.785 0.165 0.18  0.225 0.855 0.855 0.09  0.14  0.825 0.165\n",
      " 0.8   0.795 0.11  0.76  0.835 0.145 0.815 0.135 0.83  0.91  0.9   0.71\n",
      " 0.18  0.845 0.84  0.875 0.155 0.12  0.125 0.875 0.915 0.905 0.115 0.85\n",
      " 0.175 0.855 0.11  0.13  0.11  0.11  0.9   0.245 0.825 0.815 0.135 0.085\n",
      " 0.145 0.1   0.125 0.175 0.79  0.865 0.75  0.86  0.78  0.845 0.775 0.835\n",
      " 0.885 0.91  0.13  0.165 0.685 0.85  0.89  0.805 0.865 0.175 0.08  0.155\n",
      " 0.09  0.85  0.835 0.88  0.12  0.165 0.235 0.805 0.21  0.75  0.865 0.885\n",
      " 0.235 0.13  0.85  0.84  0.12  0.855 0.235 0.88  0.17  0.755 0.845 0.905\n",
      " 0.15  0.165 0.155 0.9   0.115 0.815 0.11  0.155 0.155 0.745 0.835 0.135\n",
      " 0.795 0.13  0.22  0.155 0.75  0.16  0.845 0.785]\n",
      "[0.885 0.88  0.88  0.845 0.18  0.14  0.815 0.1   0.2   0.84  0.115 0.825\n",
      " 0.89  0.135 0.89  0.84  0.115 0.12  0.13  0.845 0.15  0.19  0.225 0.83\n",
      " 0.875 0.245 0.175 0.875 0.9   0.115 0.865 0.175 0.12  0.805 0.235 0.095\n",
      " 0.885 0.225 0.835 0.895 0.81  0.25  0.11  0.885 0.865 0.1   0.89  0.825\n",
      " 0.9   0.11  0.09  0.835 0.785 0.13  0.2   0.845 0.08  0.1   0.8   0.185\n",
      " 0.125 0.22  0.24  0.91  0.84  0.84  0.1   0.86  0.165 0.8   0.155 0.795\n",
      " 0.16  0.8   0.14  0.895 0.125 0.09  0.9   0.095 0.2   0.09  0.925 0.085\n",
      " 0.16  0.85  0.825 0.22  0.195 0.21  0.87  0.855 0.095 0.14  0.845 0.165\n",
      " 0.845 0.775 0.17  0.865 0.875 0.175 0.825 0.135 0.87  0.84  0.91  0.785\n",
      " 0.155 0.905 0.85  0.895 0.17  0.165 0.175 0.875 0.88  0.89  0.15  0.835\n",
      " 0.215 0.885 0.21  0.13  0.23  0.115 0.915 0.21  0.83  0.895 0.175 0.165\n",
      " 0.105 0.135 0.11  0.16  0.855 0.835 0.825 0.875 0.795 0.8   0.83  0.83\n",
      " 0.865 0.85  0.115 0.165 0.75  0.89  0.92  0.78  0.88  0.16  0.08  0.18\n",
      " 0.165 0.9   0.815 0.83  0.145 0.15  0.225 0.875 0.26  0.835 0.875 0.875\n",
      " 0.185 0.15  0.905 0.875 0.11  0.9   0.18  0.85  0.165 0.775 0.915 0.9\n",
      " 0.105 0.19  0.19  0.92  0.14  0.87  0.125 0.195 0.175 0.79  0.87  0.19\n",
      " 0.77  0.215 0.155 0.155 0.85  0.175 0.85  0.87 ]\n",
      "[0.815 0.835 0.875 0.83  0.135 0.125 0.825 0.105 0.175 0.805 0.09  0.84\n",
      " 0.85  0.145 0.835 0.79  0.145 0.105 0.11  0.81  0.11  0.175 0.25  0.76\n",
      " 0.8   0.195 0.105 0.84  0.87  0.11  0.82  0.175 0.07  0.84  0.215 0.14\n",
      " 0.865 0.195 0.785 0.87  0.81  0.25  0.085 0.835 0.845 0.12  0.895 0.83\n",
      " 0.825 0.13  0.08  0.135 0.785 0.205 0.15  0.17  0.805 0.105 0.845 0.24\n",
      " 0.12  0.12  0.15  0.225 0.205 0.2   0.86  0.855 0.83  0.07  0.12  0.1\n",
      " 0.795 0.085 0.205 0.795 0.11  0.82  0.885 0.12  0.145 0.76  0.77  0.18\n",
      " 0.165 0.855 0.88  0.16  0.165 0.19  0.105 0.865 0.86  0.09  0.225 0.135\n",
      " 0.22  0.14  0.21  0.085 0.87  0.155 0.8   0.09  0.845 0.915 0.89  0.74\n",
      " 0.18  0.9   0.85  0.8   0.155 0.16  0.105 0.85  0.92  0.815 0.155 0.855\n",
      " 0.15  0.895 0.21  0.095 0.105 0.1   0.895 0.17  0.785 0.885 0.18  0.105\n",
      " 0.115 0.095 0.09  0.155 0.825 0.845 0.82  0.86  0.81  0.83  0.775 0.845\n",
      " 0.87  0.825 0.2   0.195 0.7   0.87  0.89  0.725 0.885 0.095 0.075 0.145\n",
      " 0.145 0.85  0.825 0.795 0.125 0.135 0.26  0.855 0.235 0.825 0.86  0.91\n",
      " 0.19  0.1   0.89  0.895 0.12  0.92  0.185 0.915 0.19  0.815 0.88  0.835\n",
      " 0.175 0.135 0.235 0.9   0.125 0.815 0.135 0.19  0.18  0.75  0.755 0.17\n",
      " 0.78  0.12  0.165 0.135 0.795 0.175 0.9   0.82 ]\n",
      "[0.8   0.84  0.87  0.75  0.165 0.09  0.775 0.115 0.185 0.815 0.105 0.805\n",
      " 0.855 0.115 0.805 0.775 0.17  0.165 0.115 0.85  0.13  0.165 0.205 0.82\n",
      " 0.79  0.125 0.125 0.885 0.885 0.16  0.86  0.15  0.085 0.8   0.19  0.125\n",
      " 0.84  0.175 0.835 0.84  0.795 0.185 0.07  0.88  0.89  0.15  0.855 0.795\n",
      " 0.85  0.135 0.085 0.13  0.755 0.2   0.16  0.115 0.82  0.095 0.84  0.24\n",
      " 0.105 0.145 0.115 0.215 0.2   0.235 0.855 0.85  0.845 0.065 0.125 0.105\n",
      " 0.765 0.1   0.175 0.82  0.105 0.745 0.82  0.095 0.175 0.805 0.775 0.185\n",
      " 0.165 0.865 0.86  0.13  0.2   0.145 0.105 0.765 0.84  0.095 0.22  0.095\n",
      " 0.19  0.16  0.235 0.095 0.15  0.84  0.805 0.125 0.185 0.82  0.1   0.085\n",
      " 0.81  0.155 0.11  0.185 0.19  0.93  0.795 0.81  0.09  0.885 0.16  0.805\n",
      " 0.11  0.75  0.13  0.785 0.1   0.81  0.13  0.145 0.83  0.09  0.125 0.055\n",
      " 0.85  0.08  0.18  0.835 0.775 0.205 0.155 0.165 0.94  0.875 0.12  0.135\n",
      " 0.835 0.1   0.8   0.795 0.125 0.815 0.885 0.8   0.87  0.11  0.1   0.175\n",
      " 0.15  0.835 0.815 0.825 0.155 0.12  0.245 0.83  0.2   0.805 0.88  0.855\n",
      " 0.17  0.095 0.88  0.82  0.13  0.86  0.165 0.86  0.2   0.71  0.86  0.885\n",
      " 0.19  0.16  0.17  0.85  0.12  0.795 0.165 0.15  0.125 0.82  0.79  0.13\n",
      " 0.765 0.1   0.155 0.1   0.83  0.175 0.805 0.785]\n",
      "[0.835 0.815 0.845 0.825 0.18  0.11  0.795 0.055 0.145 0.74  0.085 0.845\n",
      " 0.905 0.14  0.865 0.75  0.145 0.12  0.115 0.815 0.115 0.175 0.185 0.8\n",
      " 0.82  0.135 0.12  0.87  0.88  0.14  0.88  0.175 0.115 0.765 0.2   0.155\n",
      " 0.865 0.2   0.83  0.9   0.775 0.245 0.105 0.85  0.88  0.16  0.875 0.835\n",
      " 0.865 0.11  0.075 0.17  0.785 0.185 0.15  0.21  0.83  0.1   0.87  0.255\n",
      " 0.145 0.11  0.145 0.19  0.11  0.215 0.835 0.825 0.84  0.085 0.13  0.085\n",
      " 0.735 0.125 0.195 0.785 0.14  0.77  0.795 0.12  0.185 0.74  0.8   0.2\n",
      " 0.115 0.835 0.875 0.205 0.22  0.215 0.065 0.835 0.86  0.14  0.21  0.15\n",
      " 0.225 0.115 0.21  0.13  0.125 0.785 0.795 0.155 0.225 0.795 0.145 0.11\n",
      " 0.84  0.16  0.145 0.205 0.205 0.905 0.82  0.9   0.105 0.87  0.21  0.815\n",
      " 0.135 0.735 0.13  0.845 0.135 0.895 0.115 0.135 0.845 0.11  0.13  0.165\n",
      " 0.85  0.09  0.15  0.835 0.76  0.21  0.14  0.175 0.89  0.805 0.125 0.13\n",
      " 0.845 0.11  0.775 0.77  0.08  0.84  0.88  0.12  0.835 0.09  0.835 0.855\n",
      " 0.875 0.78  0.175 0.89  0.88  0.83  0.14  0.14  0.12  0.855 0.86  0.825\n",
      " 0.115 0.91  0.18  0.895 0.245 0.11  0.09  0.11  0.885 0.17  0.795 0.795\n",
      " 0.15  0.135 0.135 0.155 0.11  0.165 0.845 0.855 0.82  0.88  0.805 0.855\n",
      " 0.81  0.79  0.86  0.89  0.125 0.165 0.72  0.86 ]\n",
      "[0.05  0.18  0.745 0.25  0.125 0.14  0.785 0.045 0.91  0.275 0.11  0.145\n",
      " 0.105 0.25  0.18  0.235 0.895 0.89  0.82  0.08  0.075 0.07  0.71  0.065\n",
      " 0.2   0.74  0.115 0.775 0.89  0.1   0.145 0.79  0.785 0.225 0.15  0.905\n",
      " 0.875 0.135 0.15  0.16  0.06  0.79  0.82  0.07  0.195 0.07  0.2   0.11\n",
      " 0.185 0.045 0.065 0.825 0.775 0.06  0.23  0.83  0.065 0.05  0.815 0.12\n",
      " 0.105 0.175 0.155 0.925 0.765 0.845 0.075 0.91  0.125 0.84  0.16  0.73\n",
      " 0.145 0.855 0.11  0.865 0.08  0.11  0.955 0.08  0.085 0.075 0.875 0.07\n",
      " 0.135 0.9   0.81  0.145 0.065 0.18  0.95  0.875 0.08  0.05  0.905 0.065\n",
      " 0.865 0.735 0.105 0.83  0.9   0.145 0.815 0.03  0.88  0.91  0.965 0.71\n",
      " 0.165 0.915 0.92  0.88  0.11  0.075 0.1   0.935 0.945 0.86  0.15  0.825\n",
      " 0.115 0.895 0.24  0.095 0.08  0.105 0.925 0.145 0.795 0.84  0.12  0.105\n",
      " 0.12  0.115 0.05  0.13  0.895 0.925 0.765 0.9   0.755 0.765 0.855 0.88\n",
      " 0.885 0.885 0.085 0.165 0.685 0.91  0.945 0.735 0.925 0.09  0.06  0.15\n",
      " 0.115 0.945 0.9   0.835 0.105 0.06  0.21  0.815 0.26  0.775 0.9   0.92\n",
      " 0.11  0.08  0.925 0.825 0.095 0.895 0.215 0.9   0.155 0.75  0.91  0.945\n",
      " 0.155 0.12  0.21  0.91  0.04  0.805 0.075 0.185 0.135 0.775 0.775 0.11\n",
      " 0.685 0.13  0.16  0.08  0.75  0.15  0.885 0.815]\n",
      "[0.92  0.91  0.95  0.81  0.165 0.105 0.765 0.055 0.175 0.835 0.065 0.835\n",
      " 0.935 0.1   0.88  0.865 0.095 0.08  0.07  0.865 0.06  0.195 0.315 0.785\n",
      " 0.835 0.145 0.1   0.96  0.94  0.075 0.905 0.19  0.05  0.86  0.2   0.1\n",
      " 0.935 0.255 0.805 0.905 0.86  0.22  0.06  0.96  0.94  0.065 0.97  0.78\n",
      " 0.925 0.06  0.115 0.845 0.745 0.1   0.18  0.9   0.065 0.08  0.87  0.12\n",
      " 0.085 0.215 0.235 0.96  0.83  0.88  0.075 0.95  0.215 0.805 0.125 0.755\n",
      " 0.19  0.85  0.12  0.91  0.105 0.09  0.945 0.065 0.14  0.06  0.91  0.065\n",
      " 0.125 0.93  0.83  0.195 0.095 0.19  0.925 0.875 0.085 0.055 0.92  0.095\n",
      " 0.93  0.81  0.07  0.895 0.93  0.115 0.85  0.04  0.915 0.9   0.97  0.785\n",
      " 0.125 0.925 0.91  0.9   0.155 0.075 0.095 0.95  0.965 0.885 0.14  0.88\n",
      " 0.16  0.94  0.235 0.105 0.105 0.09  0.97  0.18  0.84  0.865 0.11  0.11\n",
      " 0.11  0.075 0.08  0.12  0.91  0.945 0.815 0.905 0.81  0.845 0.86  0.875\n",
      " 0.935 0.94  0.07  0.18  0.66  0.93  0.885 0.71  0.93  0.135 0.05  0.175\n",
      " 0.135 0.98  0.89  0.86  0.1   0.07  0.25  0.895 0.255 0.8   0.96  0.94\n",
      " 0.15  0.06  0.96  0.91  0.07  0.945 0.19  0.96  0.17  0.73  0.92  0.945\n",
      " 0.175 0.13  0.235 0.965 0.06  0.885 0.12  0.205 0.135 0.835 0.795 0.14\n",
      " 0.8   0.15  0.18  0.075 0.84  0.165 0.92  0.85 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.905 0.855 0.93  0.83  0.13  0.13  0.785 0.055 0.15  0.81  0.095 0.81\n",
      " 0.925 0.09  0.875 0.805 0.12  0.065 0.065 0.885 0.035 0.18  0.27  0.82\n",
      " 0.82  0.16  0.145 0.91  0.945 0.075 0.875 0.18  0.065 0.82  0.16  0.115\n",
      " 0.905 0.215 0.82  0.89  0.85  0.245 0.035 0.935 0.935 0.1   0.915 0.765\n",
      " 0.885 0.065 0.055 0.21  0.71  0.145 0.105 0.19  0.775 0.06  0.94  0.28\n",
      " 0.09  0.125 0.11  0.25  0.18  0.205 0.83  0.92  0.87  0.07  0.09  0.07\n",
      " 0.7   0.06  0.135 0.715 0.105 0.785 0.905 0.055 0.155 0.785 0.79  0.245\n",
      " 0.11  0.9   0.825 0.17  0.23  0.21  0.055 0.825 0.895 0.09  0.2   0.125\n",
      " 0.185 0.09  0.225 0.095 0.895 0.155 0.755 0.05  0.87  0.945 0.945 0.74\n",
      " 0.175 0.92  0.91  0.89  0.175 0.07  0.105 0.895 0.965 0.87  0.155 0.87\n",
      " 0.1   0.945 0.19  0.075 0.085 0.065 0.955 0.18  0.815 0.815 0.09  0.105\n",
      " 0.05  0.105 0.09  0.125 0.82  0.875 0.81  0.915 0.74  0.815 0.805 0.85\n",
      " 0.83  0.915 0.095 0.215 0.63  0.91  0.93  0.72  0.92  0.13  0.055 0.21\n",
      " 0.145 0.945 0.875 0.895 0.14  0.065 0.225 0.855 0.265 0.78  0.94  0.945\n",
      " 0.135 0.05  0.94  0.855 0.1   0.93  0.18  0.94  0.145 0.765 0.895 0.97\n",
      " 0.14  0.115 0.21  0.93  0.06  0.785 0.07  0.145 0.125 0.8   0.825 0.125\n",
      " 0.75  0.135 0.215 0.05  0.785 0.165 0.86  0.78 ]\n",
      "[0.84  0.845 0.915 0.805 0.165 0.09  0.705 0.07  0.17  0.74  0.06  0.77\n",
      " 0.915 0.09  0.885 0.865 0.11  0.055 0.07  0.89  0.045 0.165 0.225 0.785\n",
      " 0.795 0.11  0.11  0.92  0.925 0.115 0.87  0.135 0.05  0.82  0.145 0.13\n",
      " 0.93  0.225 0.795 0.865 0.87  0.235 0.035 0.88  0.89  0.06  0.925 0.75\n",
      " 0.925 0.055 0.065 0.155 0.66  0.155 0.12  0.155 0.79  0.06  0.95  0.22\n",
      " 0.11  0.145 0.085 0.165 0.19  0.175 0.885 0.89  0.905 0.075 0.06  0.06\n",
      " 0.735 0.065 0.185 0.755 0.085 0.805 0.875 0.045 0.135 0.79  0.785 0.22\n",
      " 0.155 0.905 0.85  0.18  0.155 0.22  0.1   0.8   0.845 0.075 0.195 0.115\n",
      " 0.2   0.08  0.25  0.11  0.08  0.82  0.81  0.04  0.23  0.885 0.045 0.075\n",
      " 0.825 0.145 0.11  0.12  0.195 0.96  0.795 0.85  0.075 0.885 0.155 0.86\n",
      " 0.13  0.75  0.095 0.815 0.1   0.82  0.08  0.08  0.91  0.05  0.15  0.09\n",
      " 0.88  0.045 0.165 0.915 0.785 0.2   0.17  0.195 0.92  0.81  0.12  0.07\n",
      " 0.92  0.04  0.885 0.825 0.15  0.84  0.91  0.745 0.915 0.115 0.065 0.155\n",
      " 0.105 0.93  0.875 0.835 0.13  0.08  0.17  0.815 0.21  0.73  0.945 0.96\n",
      " 0.085 0.025 0.915 0.87  0.075 0.85  0.17  0.97  0.155 0.74  0.895 0.96\n",
      " 0.175 0.095 0.165 0.91  0.06  0.82  0.04  0.13  0.145 0.78  0.755 0.12\n",
      " 0.71  0.11  0.145 0.09  0.775 0.155 0.915 0.76 ]\n",
      "[0.88  0.88  0.93  0.835 0.245 0.1   0.775 0.05  0.155 0.8   0.055 0.845\n",
      " 0.945 0.08  0.9   0.76  0.12  0.07  0.1   0.86  0.065 0.155 0.23  0.71\n",
      " 0.76  0.135 0.125 0.91  0.98  0.095 0.865 0.21  0.035 0.815 0.17  0.16\n",
      " 0.915 0.19  0.8   0.905 0.845 0.235 0.025 0.91  0.915 0.085 0.915 0.785\n",
      " 0.86  0.05  0.04  0.12  0.765 0.235 0.175 0.175 0.725 0.035 0.935 0.25\n",
      " 0.105 0.12  0.18  0.2   0.195 0.19  0.875 0.875 0.84  0.09  0.085 0.085\n",
      " 0.725 0.06  0.17  0.805 0.085 0.775 0.87  0.13  0.19  0.805 0.81  0.205\n",
      " 0.135 0.885 0.875 0.11  0.21  0.18  0.06  0.79  0.87  0.085 0.17  0.105\n",
      " 0.165 0.075 0.21  0.055 0.08  0.8   0.71  0.075 0.21  0.87  0.09  0.065\n",
      " 0.855 0.125 0.06  0.165 0.215 0.95  0.8   0.845 0.05  0.94  0.195 0.86\n",
      " 0.13  0.775 0.13  0.88  0.115 0.83  0.075 0.105 0.91  0.075 0.125 0.045\n",
      " 0.86  0.06  0.15  0.895 0.815 0.165 0.095 0.215 0.91  0.835 0.105 0.08\n",
      " 0.87  0.08  0.87  0.855 0.105 0.855 0.855 0.16  0.79  0.035 0.88  0.91\n",
      " 0.925 0.755 0.105 0.9   0.885 0.915 0.135 0.06  0.065 0.94  0.94  0.82\n",
      " 0.15  0.89  0.16  0.87  0.22  0.055 0.085 0.08  0.93  0.13  0.82  0.835\n",
      " 0.145 0.075 0.1   0.085 0.105 0.11  0.895 0.915 0.795 0.875 0.82  0.795\n",
      " 0.79  0.845 0.875 0.925 0.095 0.2   0.7   0.93 ]\n",
      "[0.065 0.14  0.76  0.16  0.13  0.13  0.82  0.035 0.945 0.26  0.09  0.12\n",
      " 0.13  0.155 0.15  0.225 0.9   0.865 0.935 0.05  0.07  0.095 0.7   0.07\n",
      " 0.235 0.785 0.07  0.795 0.865 0.055 0.16  0.73  0.77  0.27  0.15  0.89\n",
      " 0.865 0.16  0.2   0.165 0.07  0.845 0.865 0.06  0.28  0.095 0.195 0.085\n",
      " 0.195 0.07  0.125 0.825 0.805 0.05  0.135 0.86  0.045 0.045 0.9   0.08\n",
      " 0.12  0.205 0.21  0.965 0.82  0.885 0.05  0.9   0.2   0.83  0.125 0.755\n",
      " 0.1   0.84  0.1   0.865 0.08  0.065 0.92  0.085 0.115 0.05  0.905 0.04\n",
      " 0.13  0.895 0.755 0.165 0.07  0.155 0.975 0.83  0.1   0.075 0.89  0.08\n",
      " 0.85  0.83  0.075 0.87  0.87  0.15  0.81  0.035 0.865 0.93  0.98  0.71\n",
      " 0.185 0.905 0.91  0.835 0.145 0.065 0.095 0.93  0.97  0.82  0.12  0.895\n",
      " 0.105 0.925 0.205 0.055 0.05  0.1   0.935 0.21  0.805 0.825 0.095 0.085\n",
      " 0.085 0.08  0.115 0.17  0.88  0.955 0.81  0.9   0.75  0.82  0.84  0.815\n",
      " 0.89  0.92  0.11  0.15  0.735 0.925 0.89  0.755 0.895 0.095 0.08  0.155\n",
      " 0.135 0.98  0.87  0.8   0.075 0.06  0.235 0.85  0.27  0.81  0.95  0.95\n",
      " 0.145 0.07  0.92  0.88  0.075 0.935 0.19  0.96  0.14  0.74  0.94  0.97\n",
      " 0.16  0.115 0.205 0.935 0.07  0.845 0.065 0.175 0.12  0.765 0.755 0.125\n",
      " 0.68  0.09  0.21  0.08  0.8   0.165 0.87  0.795]\n",
      "[0.88  0.905 0.93  0.86  0.165 0.075 0.76  0.035 0.155 0.79  0.055 0.875\n",
      " 0.935 0.08  0.9   0.91  0.095 0.045 0.08  0.89  0.04  0.18  0.31  0.795\n",
      " 0.865 0.125 0.09  0.94  0.95  0.105 0.855 0.22  0.035 0.845 0.205 0.07\n",
      " 0.945 0.27  0.83  0.945 0.88  0.245 0.03  0.95  0.965 0.08  0.96  0.815\n",
      " 0.955 0.035 0.105 0.815 0.74  0.11  0.21  0.93  0.045 0.05  0.89  0.125\n",
      " 0.08  0.16  0.235 0.955 0.855 0.895 0.045 0.975 0.16  0.875 0.13  0.8\n",
      " 0.195 0.825 0.085 0.87  0.135 0.1   0.95  0.05  0.15  0.035 0.87  0.025\n",
      " 0.095 0.905 0.795 0.18  0.115 0.2   0.985 0.84  0.085 0.115 0.94  0.05\n",
      " 0.91  0.845 0.135 0.91  0.915 0.155 0.795 0.08  0.93  0.95  0.975 0.74\n",
      " 0.15  0.93  0.88  0.925 0.155 0.09  0.05  0.94  0.98  0.84  0.135 0.89\n",
      " 0.115 0.935 0.245 0.05  0.12  0.07  0.955 0.215 0.805 0.895 0.105 0.105\n",
      " 0.085 0.09  0.085 0.135 0.945 0.93  0.77  0.86  0.765 0.77  0.815 0.885\n",
      " 0.925 0.925 0.125 0.195 0.705 0.97  0.91  0.655 0.94  0.08  0.04  0.135\n",
      " 0.125 0.985 0.9   0.865 0.075 0.055 0.16  0.89  0.205 0.875 0.955 0.97\n",
      " 0.12  0.05  0.96  0.87  0.08  0.935 0.265 0.97  0.16  0.725 0.96  0.935\n",
      " 0.15  0.175 0.26  0.96  0.05  0.895 0.045 0.16  0.11  0.795 0.775 0.095\n",
      " 0.77  0.105 0.15  0.07  0.795 0.165 0.915 0.79 ]\n",
      "[0.9   0.91  0.895 0.815 0.185 0.085 0.68  0.06  0.155 0.765 0.075 0.775\n",
      " 0.95  0.14  0.85  0.825 0.12  0.065 0.06  0.88  0.04  0.1   0.265 0.81\n",
      " 0.835 0.105 0.11  0.925 0.955 0.06  0.925 0.185 0.025 0.805 0.255 0.13\n",
      " 0.935 0.225 0.815 0.935 0.83  0.165 0.05  0.895 0.955 0.05  0.935 0.705\n",
      " 0.89  0.065 0.025 0.215 0.725 0.22  0.12  0.19  0.78  0.06  0.955 0.275\n",
      " 0.1   0.12  0.115 0.22  0.25  0.195 0.92  0.885 0.905 0.045 0.085 0.075\n",
      " 0.7   0.05  0.215 0.79  0.12  0.81  0.86  0.085 0.16  0.71  0.79  0.22\n",
      " 0.125 0.905 0.81  0.1   0.225 0.215 0.06  0.825 0.875 0.06  0.17  0.1\n",
      " 0.17  0.1   0.29  0.075 0.885 0.15  0.77  0.07  0.84  0.93  0.97  0.76\n",
      " 0.17  0.935 0.905 0.905 0.175 0.105 0.065 0.905 0.925 0.85  0.135 0.9\n",
      " 0.11  0.97  0.175 0.025 0.03  0.08  0.975 0.205 0.835 0.87  0.095 0.09\n",
      " 0.095 0.095 0.07  0.15  0.875 0.905 0.835 0.925 0.78  0.805 0.855 0.85\n",
      " 0.885 0.905 0.065 0.165 0.69  0.96  0.895 0.675 0.935 0.08  0.08  0.15\n",
      " 0.135 0.98  0.865 0.845 0.085 0.06  0.24  0.84  0.235 0.845 0.915 0.955\n",
      " 0.125 0.055 0.91  0.84  0.05  0.93  0.225 0.955 0.165 0.82  0.91  0.965\n",
      " 0.135 0.135 0.18  0.92  0.115 0.82  0.065 0.2   0.125 0.83  0.76  0.16\n",
      " 0.73  0.135 0.175 0.035 0.815 0.2   0.885 0.77 ]\n",
      "[0.91  0.865 0.91  0.845 0.195 0.06  0.735 0.05  0.145 0.75  0.055 0.815\n",
      " 0.95  0.1   0.87  0.89  0.125 0.045 0.075 0.87  0.025 0.115 0.215 0.8\n",
      " 0.8   0.085 0.145 0.905 0.95  0.1   0.875 0.21  0.05  0.795 0.19  0.065\n",
      " 0.91  0.22  0.83  0.91  0.82  0.255 0.025 0.91  0.945 0.04  0.915 0.75\n",
      " 0.905 0.06  0.04  0.13  0.735 0.19  0.115 0.15  0.735 0.03  0.935 0.26\n",
      " 0.085 0.135 0.1   0.195 0.175 0.245 0.875 0.895 0.945 0.065 0.08  0.085\n",
      " 0.74  0.06  0.13  0.735 0.11  0.76  0.86  0.075 0.135 0.76  0.77  0.25\n",
      " 0.16  0.89  0.845 0.13  0.18  0.215 0.065 0.805 0.88  0.035 0.225 0.055\n",
      " 0.165 0.095 0.215 0.085 0.1   0.785 0.765 0.06  0.16  0.89  0.045 0.03\n",
      " 0.88  0.115 0.075 0.21  0.16  0.93  0.79  0.875 0.025 0.915 0.185 0.855\n",
      " 0.085 0.735 0.145 0.845 0.15  0.84  0.07  0.13  0.915 0.035 0.11  0.045\n",
      " 0.9   0.04  0.105 0.89  0.805 0.16  0.07  0.17  0.975 0.805 0.085 0.055\n",
      " 0.9   0.07  0.9   0.78  0.09  0.86  0.93  0.66  0.91  0.1   0.045 0.145\n",
      " 0.115 0.945 0.865 0.83  0.075 0.055 0.265 0.86  0.17  0.775 0.905 0.935\n",
      " 0.14  0.02  0.94  0.865 0.08  0.905 0.2   0.935 0.175 0.725 0.94  0.98\n",
      " 0.155 0.135 0.18  0.92  0.04  0.87  0.045 0.165 0.085 0.795 0.805 0.095\n",
      " 0.755 0.155 0.165 0.06  0.78  0.155 0.915 0.75 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92  0.925 0.9   0.875 0.175 0.09  0.755 0.055 0.11  0.76  0.05  0.835\n",
      " 0.96  0.09  0.885 0.855 0.09  0.055 0.065 0.87  0.025 0.135 0.275 0.775\n",
      " 0.815 0.115 0.125 0.925 0.935 0.135 0.9   0.195 0.05  0.805 0.16  0.09\n",
      " 0.955 0.22  0.865 0.92  0.87  0.27  0.04  0.91  0.965 0.065 0.915 0.795\n",
      " 0.89  0.06  0.06  0.15  0.735 0.195 0.105 0.195 0.765 0.055 0.95  0.285\n",
      " 0.09  0.105 0.095 0.23  0.2   0.16  0.925 0.9   0.92  0.075 0.04  0.065\n",
      " 0.635 0.045 0.165 0.74  0.095 0.8   0.85  0.06  0.185 0.805 0.78  0.2\n",
      " 0.155 0.905 0.865 0.125 0.185 0.175 0.025 0.82  0.88  0.055 0.195 0.07\n",
      " 0.18  0.08  0.275 0.045 0.11  0.835 0.795 0.055 0.245 0.845 0.07  0.035\n",
      " 0.88  0.14  0.08  0.205 0.18  0.945 0.835 0.865 0.055 0.915 0.22  0.81\n",
      " 0.145 0.82  0.12  0.83  0.085 0.895 0.1   0.13  0.915 0.045 0.105 0.045\n",
      " 0.865 0.05  0.155 0.935 0.77  0.14  0.08  0.21  0.915 0.875 0.095 0.07\n",
      " 0.905 0.085 0.88  0.8   0.11  0.895 0.925 0.175 0.85  0.04  0.9   0.935\n",
      " 0.945 0.78  0.18  0.945 0.92  0.875 0.13  0.065 0.07  0.915 0.97  0.85\n",
      " 0.13  0.91  0.155 0.915 0.21  0.045 0.1   0.04  0.94  0.185 0.875 0.88\n",
      " 0.07  0.105 0.085 0.06  0.055 0.09  0.93  0.915 0.77  0.925 0.765 0.775\n",
      " 0.805 0.865 0.89  0.94  0.1   0.215 0.595 0.92 ]\n",
      "[0.05  0.155 0.79  0.195 0.115 0.205 0.785 0.035 0.98  0.24  0.085 0.16\n",
      " 0.135 0.205 0.21  0.2   0.91  0.875 0.91  0.08  0.08  0.05  0.735 0.045\n",
      " 0.17  0.75  0.09  0.79  0.865 0.06  0.17  0.715 0.76  0.23  0.145 0.945\n",
      " 0.86  0.17  0.145 0.235 0.095 0.775 0.885 0.08  0.175 0.07  0.105 0.1\n",
      " 0.225 0.065 0.06  0.78  0.765 0.08  0.135 0.87  0.09  0.065 0.835 0.06\n",
      " 0.1   0.185 0.205 0.94  0.775 0.87  0.065 0.94  0.185 0.805 0.085 0.735\n",
      " 0.11  0.87  0.1   0.88  0.085 0.06  0.95  0.07  0.09  0.03  0.875 0.035\n",
      " 0.075 0.92  0.76  0.155 0.065 0.185 0.975 0.865 0.06  0.06  0.895 0.07\n",
      " 0.91  0.735 0.105 0.89  0.93  0.21  0.81  0.045 0.88  0.935 0.955 0.72\n",
      " 0.145 0.955 0.925 0.835 0.145 0.05  0.07  0.945 0.98  0.815 0.12  0.875\n",
      " 0.13  0.96  0.225 0.025 0.095 0.105 0.955 0.16  0.78  0.805 0.09  0.07\n",
      " 0.09  0.115 0.05  0.12  0.845 0.94  0.835 0.93  0.8   0.85  0.805 0.835\n",
      " 0.93  0.95  0.065 0.145 0.6   0.945 0.935 0.75  0.89  0.06  0.09  0.155\n",
      " 0.155 0.975 0.86  0.86  0.075 0.065 0.29  0.895 0.285 0.805 0.925 0.96\n",
      " 0.075 0.045 0.89  0.9   0.095 0.92  0.21  0.965 0.105 0.74  0.935 0.965\n",
      " 0.175 0.11  0.15  0.95  0.065 0.86  0.075 0.17  0.13  0.765 0.74  0.12\n",
      " 0.775 0.125 0.125 0.045 0.795 0.21  0.93  0.805]\n",
      "[0.89  0.93  0.95  0.875 0.195 0.105 0.845 0.065 0.185 0.77  0.055 0.825\n",
      " 0.97  0.075 0.89  0.88  0.055 0.07  0.075 0.895 0.055 0.165 0.29  0.795\n",
      " 0.86  0.165 0.085 0.96  0.925 0.1   0.845 0.2   0.03  0.83  0.15  0.115\n",
      " 0.955 0.32  0.79  0.93  0.875 0.25  0.035 0.945 0.97  0.08  0.975 0.815\n",
      " 0.95  0.03  0.095 0.85  0.775 0.04  0.215 0.925 0.065 0.035 0.895 0.14\n",
      " 0.075 0.19  0.225 0.985 0.855 0.88  0.03  0.955 0.21  0.835 0.15  0.755\n",
      " 0.205 0.835 0.105 0.865 0.115 0.105 0.955 0.045 0.125 0.045 0.895 0.055\n",
      " 0.1   0.935 0.755 0.19  0.085 0.2   0.925 0.815 0.055 0.07  0.94  0.095\n",
      " 0.93  0.76  0.145 0.925 0.93  0.17  0.79  0.03  0.9   0.96  0.985 0.785\n",
      " 0.175 0.95  0.935 0.93  0.19  0.04  0.065 0.93  0.96  0.87  0.19  0.91\n",
      " 0.15  0.945 0.215 0.03  0.115 0.12  0.97  0.225 0.905 0.87  0.04  0.075\n",
      " 0.085 0.08  0.075 0.15  0.91  0.965 0.755 0.84  0.795 0.88  0.85  0.89\n",
      " 0.945 0.92  0.1   0.195 0.595 0.965 0.945 0.76  0.96  0.07  0.035 0.16\n",
      " 0.145 0.97  0.875 0.845 0.1   0.06  0.21  0.84  0.28  0.865 0.96  0.97\n",
      " 0.13  0.04  0.965 0.905 0.06  0.95  0.245 0.98  0.14  0.83  0.96  0.975\n",
      " 0.23  0.15  0.19  0.96  0.04  0.91  0.05  0.195 0.11  0.79  0.8   0.125\n",
      " 0.715 0.15  0.195 0.045 0.86  0.235 0.92  0.745]\n",
      "[0.89  0.9   0.905 0.845 0.105 0.085 0.76  0.07  0.125 0.755 0.045 0.83\n",
      " 0.94  0.085 0.905 0.835 0.085 0.065 0.07  0.865 0.05  0.16  0.28  0.725\n",
      " 0.85  0.125 0.135 0.925 0.94  0.1   0.86  0.22  0.04  0.75  0.2   0.085\n",
      " 0.92  0.295 0.765 0.93  0.8   0.2   0.05  0.93  0.945 0.085 0.96  0.82\n",
      " 0.93  0.045 0.035 0.165 0.74  0.18  0.13  0.21  0.735 0.035 0.955 0.27\n",
      " 0.085 0.165 0.125 0.25  0.175 0.2   0.93  0.845 0.92  0.055 0.07  0.055\n",
      " 0.73  0.07  0.22  0.78  0.1   0.8   0.865 0.1   0.16  0.795 0.805 0.225\n",
      " 0.125 0.925 0.885 0.16  0.225 0.225 0.105 0.86  0.85  0.08  0.175 0.125\n",
      " 0.185 0.08  0.22  0.06  0.88  0.16  0.75  0.035 0.84  0.935 0.95  0.765\n",
      " 0.175 0.91  0.935 0.915 0.19  0.08  0.11  0.915 0.99  0.815 0.19  0.865\n",
      " 0.145 0.925 0.195 0.08  0.065 0.065 0.955 0.165 0.815 0.805 0.07  0.1\n",
      " 0.095 0.085 0.085 0.125 0.945 0.925 0.795 0.865 0.775 0.755 0.79  0.82\n",
      " 0.93  0.93  0.11  0.235 0.675 0.95  0.96  0.74  0.92  0.13  0.085 0.125\n",
      " 0.125 0.965 0.885 0.82  0.095 0.045 0.265 0.83  0.315 0.845 0.945 0.935\n",
      " 0.11  0.05  0.905 0.885 0.07  0.94  0.18  0.94  0.155 0.73  0.91  0.98\n",
      " 0.17  0.105 0.22  0.965 0.08  0.815 0.035 0.145 0.145 0.75  0.835 0.135\n",
      " 0.735 0.12  0.16  0.045 0.795 0.165 0.89  0.815]\n",
      "[0.865 0.875 0.9   0.815 0.205 0.065 0.735 0.08  0.13  0.715 0.105 0.79\n",
      " 0.94  0.07  0.865 0.845 0.1   0.05  0.05  0.825 0.04  0.13  0.265 0.76\n",
      " 0.845 0.1   0.115 0.94  0.96  0.085 0.825 0.18  0.035 0.81  0.18  0.095\n",
      " 0.91  0.235 0.81  0.895 0.82  0.235 0.02  0.9   0.95  0.08  0.925 0.76\n",
      " 0.9   0.045 0.045 0.17  0.75  0.21  0.135 0.245 0.75  0.015 0.95  0.225\n",
      " 0.065 0.095 0.105 0.235 0.205 0.155 0.905 0.865 0.885 0.07  0.04  0.095\n",
      " 0.755 0.03  0.175 0.77  0.05  0.775 0.9   0.05  0.115 0.79  0.78  0.24\n",
      " 0.16  0.88  0.84  0.15  0.19  0.21  0.085 0.76  0.85  0.07  0.15  0.095\n",
      " 0.22  0.07  0.25  0.08  0.07  0.74  0.74  0.12  0.23  0.835 0.06  0.045\n",
      " 0.88  0.105 0.065 0.17  0.225 0.935 0.785 0.86  0.035 0.92  0.18  0.785\n",
      " 0.09  0.775 0.16  0.845 0.085 0.885 0.07  0.105 0.91  0.02  0.085 0.06\n",
      " 0.845 0.02  0.14  0.95  0.745 0.165 0.08  0.195 0.945 0.815 0.095 0.085\n",
      " 0.885 0.07  0.88  0.79  0.125 0.85  0.865 0.75  0.925 0.05  0.045 0.095\n",
      " 0.12  0.945 0.88  0.87  0.1   0.04  0.2   0.785 0.205 0.77  0.92  0.925\n",
      " 0.125 0.015 0.93  0.845 0.065 0.93  0.195 0.93  0.175 0.705 0.9   0.955\n",
      " 0.165 0.125 0.215 0.945 0.065 0.845 0.02  0.185 0.1   0.835 0.735 0.135\n",
      " 0.775 0.16  0.2   0.045 0.8   0.21  0.895 0.805]\n",
      "[0.865 0.88  0.915 0.83  0.145 0.075 0.72  0.065 0.135 0.785 0.07  0.835\n",
      " 0.955 0.09  0.93  0.82  0.07  0.07  0.05  0.84  0.02  0.125 0.2   0.76\n",
      " 0.805 0.13  0.13  0.915 0.95  0.075 0.935 0.19  0.045 0.74  0.14  0.11\n",
      " 0.94  0.215 0.835 0.905 0.85  0.275 0.045 0.905 0.935 0.07  0.94  0.81\n",
      " 0.92  0.08  0.06  0.145 0.755 0.175 0.11  0.115 0.715 0.035 0.935 0.27\n",
      " 0.065 0.175 0.085 0.235 0.225 0.23  0.895 0.885 0.92  0.075 0.035 0.12\n",
      " 0.725 0.045 0.185 0.76  0.075 0.76  0.855 0.095 0.17  0.775 0.725 0.26\n",
      " 0.14  0.915 0.85  0.105 0.19  0.18  0.055 0.815 0.88  0.045 0.185 0.1\n",
      " 0.19  0.135 0.265 0.07  0.165 0.865 0.805 0.12  0.18  0.91  0.045 0.045\n",
      " 0.855 0.145 0.09  0.17  0.225 0.955 0.77  0.89  0.06  0.9   0.19  0.835\n",
      " 0.095 0.775 0.15  0.835 0.085 0.915 0.09  0.105 0.95  0.065 0.125 0.06\n",
      " 0.895 0.06  0.12  0.965 0.82  0.15  0.09  0.15  0.94  0.83  0.105 0.04\n",
      " 0.92  0.08  0.925 0.785 0.105 0.875 0.92  0.125 0.79  0.03  0.855 0.935\n",
      " 0.94  0.81  0.18  0.945 0.925 0.9   0.13  0.05  0.06  0.925 0.96  0.815\n",
      " 0.1   0.9   0.135 0.895 0.195 0.055 0.105 0.08  0.94  0.125 0.905 0.795\n",
      " 0.12  0.12  0.115 0.11  0.08  0.1   0.925 0.915 0.785 0.9   0.77  0.8\n",
      " 0.81  0.855 0.905 0.95  0.11  0.14  0.635 0.91 ]\n",
      "[0.045 0.17  0.74  0.18  0.15  0.195 0.81  0.075 0.96  0.22  0.065 0.13\n",
      " 0.095 0.255 0.175 0.16  0.885 0.865 0.89  0.09  0.07  0.07  0.795 0.05\n",
      " 0.21  0.75  0.095 0.78  0.8   0.105 0.165 0.77  0.85  0.27  0.105 0.91\n",
      " 0.85  0.145 0.17  0.22  0.085 0.775 0.84  0.06  0.165 0.04  0.165 0.06\n",
      " 0.225 0.055 0.1   0.795 0.77  0.105 0.155 0.885 0.03  0.02  0.87  0.12\n",
      " 0.1   0.25  0.185 0.98  0.77  0.835 0.055 0.9   0.165 0.885 0.125 0.785\n",
      " 0.105 0.835 0.1   0.895 0.075 0.12  0.93  0.055 0.13  0.055 0.885 0.06\n",
      " 0.09  0.915 0.775 0.135 0.07  0.16  0.93  0.81  0.065 0.05  0.91  0.08\n",
      " 0.86  0.81  0.105 0.885 0.92  0.14  0.815 0.04  0.845 0.94  0.97  0.73\n",
      " 0.13  0.905 0.915 0.88  0.11  0.07  0.07  0.93  0.955 0.795 0.145 0.865\n",
      " 0.115 0.9   0.225 0.04  0.075 0.085 0.955 0.16  0.815 0.83  0.13  0.095\n",
      " 0.08  0.06  0.04  0.12  0.885 0.93  0.78  0.905 0.795 0.795 0.795 0.855\n",
      " 0.855 0.95  0.085 0.195 0.685 0.95  0.93  0.71  0.97  0.045 0.05  0.19\n",
      " 0.11  0.965 0.87  0.84  0.07  0.06  0.195 0.905 0.265 0.81  0.915 0.975\n",
      " 0.1   0.05  0.895 0.915 0.12  0.885 0.17  0.925 0.165 0.735 0.945 0.975\n",
      " 0.14  0.165 0.2   0.95  0.07  0.825 0.03  0.185 0.105 0.82  0.795 0.1\n",
      " 0.76  0.14  0.1   0.055 0.8   0.185 0.905 0.815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.895 0.91  0.945 0.89  0.175 0.07  0.8   0.045 0.175 0.765 0.055 0.79\n",
      " 0.965 0.07  0.885 0.875 0.06  0.035 0.055 0.875 0.065 0.185 0.295 0.745\n",
      " 0.89  0.135 0.15  0.965 0.965 0.055 0.865 0.145 0.04  0.8   0.2   0.1\n",
      " 0.95  0.285 0.795 0.93  0.875 0.2   0.035 0.975 0.965 0.125 0.945 0.71\n",
      " 0.875 0.05  0.055 0.865 0.76  0.075 0.22  0.9   0.035 0.06  0.875 0.145\n",
      " 0.05  0.17  0.23  0.965 0.795 0.905 0.025 0.94  0.22  0.86  0.135 0.765\n",
      " 0.125 0.89  0.12  0.905 0.08  0.135 0.955 0.06  0.09  0.045 0.895 0.05\n",
      " 0.11  0.95  0.78  0.195 0.055 0.23  0.975 0.79  0.045 0.07  0.945 0.05\n",
      " 0.92  0.805 0.13  0.93  0.92  0.175 0.785 0.05  0.885 0.95  1.    0.72\n",
      " 0.115 0.94  0.955 0.905 0.1   0.06  0.055 0.955 0.975 0.85  0.17  0.89\n",
      " 0.13  0.935 0.25  0.065 0.07  0.11  0.985 0.16  0.865 0.87  0.07  0.1\n",
      " 0.085 0.095 0.085 0.16  0.915 0.965 0.825 0.86  0.795 0.855 0.83  0.85\n",
      " 0.905 0.92  0.09  0.185 0.64  0.95  0.93  0.755 0.955 0.075 0.04  0.13\n",
      " 0.115 0.99  0.9   0.845 0.07  0.055 0.205 0.865 0.18  0.88  0.925 0.98\n",
      " 0.095 0.03  0.96  0.88  0.06  0.96  0.25  0.96  0.16  0.695 0.96  0.96\n",
      " 0.17  0.16  0.17  0.975 0.09  0.925 0.065 0.19  0.115 0.82  0.8   0.135\n",
      " 0.735 0.13  0.195 0.045 0.83  0.145 0.935 0.785]\n",
      "[0.9   0.91  0.925 0.775 0.15  0.08  0.81  0.045 0.145 0.735 0.08  0.825\n",
      " 0.965 0.11  0.905 0.83  0.06  0.06  0.07  0.85  0.05  0.14  0.295 0.735\n",
      " 0.865 0.145 0.175 0.915 0.95  0.065 0.845 0.24  0.07  0.775 0.2   0.11\n",
      " 0.93  0.24  0.85  0.93  0.825 0.255 0.035 0.96  0.945 0.1   0.95  0.785\n",
      " 0.905 0.075 0.04  0.135 0.73  0.155 0.1   0.195 0.755 0.04  0.965 0.255\n",
      " 0.065 0.135 0.11  0.21  0.155 0.22  0.91  0.875 0.915 0.045 0.065 0.05\n",
      " 0.74  0.075 0.17  0.725 0.09  0.79  0.91  0.12  0.105 0.775 0.82  0.23\n",
      " 0.145 0.925 0.875 0.115 0.205 0.18  0.06  0.795 0.845 0.08  0.215 0.105\n",
      " 0.185 0.09  0.265 0.05  0.87  0.14  0.83  0.04  0.895 0.965 0.94  0.71\n",
      " 0.115 0.945 0.93  0.91  0.14  0.075 0.095 0.925 0.94  0.82  0.17  0.9\n",
      " 0.125 0.915 0.205 0.07  0.1   0.06  0.96  0.25  0.805 0.835 0.115 0.08\n",
      " 0.095 0.105 0.09  0.14  0.88  0.905 0.835 0.91  0.795 0.84  0.83  0.855\n",
      " 0.9   0.95  0.13  0.21  0.705 0.955 0.915 0.71  0.905 0.065 0.095 0.13\n",
      " 0.145 0.96  0.86  0.78  0.1   0.1   0.285 0.86  0.23  0.815 0.925 0.96\n",
      " 0.13  0.035 0.93  0.84  0.055 0.925 0.2   0.945 0.135 0.715 0.945 0.96\n",
      " 0.205 0.15  0.195 0.945 0.1   0.78  0.05  0.22  0.155 0.805 0.795 0.105\n",
      " 0.74  0.11  0.19  0.11  0.755 0.155 0.93  0.805]\n",
      "[0.885 0.85  0.965 0.805 0.18  0.055 0.73  0.085 0.15  0.735 0.075 0.83\n",
      " 0.97  0.08  0.865 0.86  0.11  0.045 0.055 0.875 0.045 0.12  0.26  0.8\n",
      " 0.765 0.13  0.14  0.905 0.94  0.11  0.865 0.17  0.025 0.825 0.19  0.105\n",
      " 0.91  0.23  0.775 0.915 0.785 0.24  0.03  0.895 0.94  0.035 0.955 0.755\n",
      " 0.92  0.05  0.015 0.11  0.725 0.155 0.09  0.235 0.735 0.05  0.945 0.32\n",
      " 0.08  0.08  0.095 0.19  0.205 0.205 0.89  0.905 0.875 0.06  0.055 0.07\n",
      " 0.79  0.035 0.22  0.76  0.04  0.84  0.89  0.065 0.185 0.775 0.695 0.235\n",
      " 0.155 0.955 0.88  0.21  0.175 0.18  0.07  0.825 0.825 0.03  0.22  0.09\n",
      " 0.165 0.055 0.26  0.05  0.07  0.79  0.695 0.06  0.17  0.85  0.04  0.02\n",
      " 0.895 0.115 0.05  0.2   0.155 0.92  0.8   0.84  0.03  0.89  0.195 0.875\n",
      " 0.1   0.775 0.14  0.85  0.105 0.86  0.085 0.115 0.935 0.02  0.105 0.04\n",
      " 0.87  0.04  0.155 0.9   0.74  0.235 0.11  0.175 0.96  0.78  0.1   0.04\n",
      " 0.885 0.085 0.89  0.74  0.1   0.835 0.905 0.695 0.95  0.085 0.06  0.19\n",
      " 0.165 0.96  0.905 0.875 0.08  0.045 0.215 0.865 0.235 0.78  0.895 0.94\n",
      " 0.12  0.015 0.95  0.895 0.08  0.945 0.175 0.915 0.15  0.655 0.95  0.95\n",
      " 0.125 0.125 0.215 0.945 0.05  0.865 0.05  0.175 0.16  0.78  0.805 0.125\n",
      " 0.785 0.14  0.125 0.03  0.78  0.19  0.9   0.725]\n",
      "[0.92  0.915 0.945 0.815 0.175 0.085 0.765 0.03  0.165 0.67  0.055 0.825\n",
      " 0.94  0.095 0.92  0.83  0.11  0.03  0.07  0.815 0.055 0.205 0.235 0.765\n",
      " 0.785 0.075 0.075 0.92  0.92  0.1   0.92  0.185 0.04  0.795 0.205 0.095\n",
      " 0.965 0.225 0.88  0.945 0.795 0.275 0.01  0.95  0.94  0.06  0.94  0.78\n",
      " 0.88  0.06  0.015 0.145 0.725 0.29  0.145 0.2   0.745 0.04  0.975 0.305\n",
      " 0.06  0.14  0.09  0.24  0.23  0.275 0.915 0.9   0.92  0.07  0.05  0.065\n",
      " 0.705 0.055 0.16  0.78  0.08  0.77  0.87  0.06  0.1   0.775 0.81  0.22\n",
      " 0.145 0.92  0.865 0.125 0.205 0.215 0.075 0.78  0.875 0.055 0.205 0.095\n",
      " 0.175 0.11  0.235 0.06  0.065 0.815 0.825 0.07  0.195 0.875 0.035 0.05\n",
      " 0.835 0.13  0.06  0.26  0.165 0.98  0.795 0.915 0.045 0.885 0.165 0.82\n",
      " 0.08  0.78  0.125 0.855 0.085 0.825 0.075 0.075 0.94  0.015 0.09  0.04\n",
      " 0.9   0.025 0.13  0.945 0.825 0.175 0.08  0.135 0.955 0.885 0.105 0.04\n",
      " 0.95  0.095 0.945 0.79  0.115 0.895 0.9   0.145 0.79  0.04  0.9   0.925\n",
      " 0.955 0.86  0.135 0.95  0.925 0.915 0.17  0.075 0.04  0.955 0.97  0.89\n",
      " 0.105 0.885 0.135 0.91  0.2   0.05  0.045 0.065 0.925 0.165 0.865 0.825\n",
      " 0.09  0.085 0.105 0.065 0.06  0.14  0.85  0.935 0.82  0.925 0.77  0.83\n",
      " 0.835 0.785 0.865 0.95  0.11  0.165 0.595 0.93 ]\n",
      "[0.025 0.17  0.765 0.155 0.13  0.17  0.725 0.035 0.96  0.3   0.07  0.07\n",
      " 0.095 0.24  0.19  0.17  0.925 0.885 0.89  0.09  0.07  0.125 0.72  0.065\n",
      " 0.2   0.775 0.085 0.845 0.82  0.1   0.115 0.755 0.785 0.245 0.105 0.94\n",
      " 0.875 0.145 0.195 0.215 0.1   0.825 0.875 0.09  0.23  0.09  0.17  0.065\n",
      " 0.27  0.07  0.095 0.785 0.77  0.035 0.19  0.88  0.05  0.04  0.875 0.085\n",
      " 0.05  0.235 0.22  0.955 0.82  0.88  0.075 0.93  0.22  0.815 0.115 0.755\n",
      " 0.15  0.86  0.07  0.9   0.055 0.11  0.965 0.07  0.12  0.03  0.895 0.055\n",
      " 0.085 0.915 0.815 0.14  0.08  0.175 0.955 0.875 0.05  0.075 0.88  0.07\n",
      " 0.9   0.745 0.08  0.87  0.91  0.13  0.815 0.05  0.885 0.945 0.975 0.735\n",
      " 0.11  0.925 0.93  0.885 0.145 0.035 0.075 0.93  0.97  0.835 0.1   0.87\n",
      " 0.15  0.91  0.205 0.015 0.045 0.115 0.965 0.19  0.815 0.825 0.11  0.105\n",
      " 0.09  0.095 0.06  0.135 0.86  0.94  0.78  0.93  0.75  0.805 0.845 0.86\n",
      " 0.93  0.93  0.1   0.195 0.64  0.96  0.95  0.77  0.92  0.045 0.09  0.165\n",
      " 0.12  0.965 0.865 0.845 0.07  0.065 0.25  0.88  0.23  0.755 0.89  0.96\n",
      " 0.11  0.025 0.88  0.895 0.05  0.915 0.19  0.94  0.155 0.705 0.935 0.985\n",
      " 0.145 0.12  0.22  0.945 0.06  0.82  0.09  0.13  0.11  0.79  0.79  0.14\n",
      " 0.745 0.12  0.125 0.025 0.8   0.13  0.885 0.78 ]\n",
      "[0.895 0.905 0.92  0.86  0.205 0.11  0.825 0.05  0.155 0.775 0.07  0.82\n",
      " 0.95  0.065 0.91  0.845 0.085 0.05  0.05  0.915 0.025 0.16  0.265 0.805\n",
      " 0.825 0.11  0.095 0.96  0.97  0.1   0.84  0.25  0.03  0.825 0.17  0.105\n",
      " 0.945 0.305 0.84  0.955 0.87  0.26  0.035 0.945 0.95  0.055 0.98  0.74\n",
      " 0.905 0.06  0.1   0.845 0.73  0.07  0.195 0.935 0.055 0.025 0.9   0.09\n",
      " 0.055 0.18  0.2   0.965 0.835 0.935 0.03  0.965 0.105 0.9   0.12  0.79\n",
      " 0.125 0.865 0.08  0.905 0.135 0.13  0.945 0.06  0.1   0.045 0.905 0.035\n",
      " 0.18  0.955 0.74  0.18  0.05  0.18  0.94  0.775 0.105 0.09  0.915 0.08\n",
      " 0.935 0.79  0.095 0.93  0.905 0.19  0.805 0.045 0.945 0.945 0.98  0.785\n",
      " 0.14  0.975 0.925 0.915 0.135 0.045 0.075 0.965 0.98  0.9   0.155 0.87\n",
      " 0.1   0.97  0.25  0.06  0.05  0.055 0.965 0.18  0.88  0.885 0.04  0.105\n",
      " 0.09  0.115 0.09  0.145 0.985 0.945 0.79  0.865 0.84  0.845 0.825 0.875\n",
      " 0.935 0.93  0.12  0.17  0.64  0.975 0.915 0.64  0.96  0.07  0.06  0.205\n",
      " 0.1   0.985 0.86  0.88  0.08  0.065 0.185 0.89  0.29  0.815 0.965 0.985\n",
      " 0.13  0.045 0.955 0.915 0.065 0.945 0.245 0.97  0.18  0.8   0.935 0.975\n",
      " 0.19  0.135 0.245 0.98  0.045 0.9   0.065 0.16  0.09  0.825 0.775 0.125\n",
      " 0.755 0.125 0.14  0.035 0.83  0.215 0.96  0.755]\n",
      "[0.89  0.905 0.905 0.86  0.145 0.125 0.795 0.055 0.16  0.755 0.055 0.77\n",
      " 0.96  0.075 0.885 0.86  0.075 0.065 0.055 0.92  0.04  0.145 0.25  0.805\n",
      " 0.855 0.12  0.11  0.945 0.955 0.095 0.895 0.215 0.05  0.81  0.175 0.07\n",
      " 0.955 0.255 0.82  0.91  0.805 0.23  0.045 0.945 0.965 0.065 0.94  0.785\n",
      " 0.915 0.07  0.045 0.17  0.7   0.21  0.12  0.22  0.71  0.04  0.95  0.28\n",
      " 0.08  0.13  0.12  0.305 0.22  0.195 0.885 0.905 0.87  0.03  0.07  0.08\n",
      " 0.725 0.03  0.245 0.75  0.085 0.81  0.86  0.1   0.145 0.75  0.76  0.265\n",
      " 0.06  0.86  0.845 0.17  0.24  0.26  0.055 0.815 0.875 0.07  0.22  0.11\n",
      " 0.185 0.08  0.275 0.05  0.925 0.135 0.795 0.03  0.835 0.935 0.95  0.72\n",
      " 0.18  0.94  0.92  0.92  0.18  0.065 0.07  0.93  0.94  0.86  0.185 0.875\n",
      " 0.12  0.905 0.205 0.065 0.1   0.05  0.975 0.19  0.815 0.865 0.12  0.08\n",
      " 0.085 0.085 0.06  0.165 0.87  0.935 0.865 0.95  0.8   0.755 0.805 0.805\n",
      " 0.88  0.955 0.11  0.19  0.655 0.91  0.905 0.78  0.93  0.145 0.095 0.19\n",
      " 0.11  0.955 0.865 0.875 0.1   0.045 0.28  0.825 0.325 0.865 0.88  0.955\n",
      " 0.12  0.045 0.94  0.88  0.05  0.94  0.215 0.925 0.155 0.725 0.945 0.94\n",
      " 0.155 0.145 0.185 0.965 0.06  0.86  0.05  0.19  0.13  0.755 0.77  0.185\n",
      " 0.765 0.12  0.125 0.07  0.785 0.215 0.895 0.85 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85  0.895 0.94  0.85  0.115 0.06  0.755 0.04  0.17  0.72  0.085 0.805\n",
      " 0.94  0.095 0.865 0.78  0.115 0.06  0.06  0.855 0.04  0.165 0.205 0.815\n",
      " 0.785 0.11  0.125 0.935 0.915 0.095 0.835 0.15  0.04  0.835 0.17  0.11\n",
      " 0.905 0.265 0.81  0.95  0.83  0.245 0.02  0.915 0.965 0.045 0.92  0.775\n",
      " 0.885 0.045 0.03  0.17  0.745 0.19  0.095 0.245 0.76  0.06  0.945 0.3\n",
      " 0.09  0.11  0.125 0.2   0.185 0.165 0.905 0.84  0.9   0.06  0.035 0.085\n",
      " 0.72  0.055 0.17  0.785 0.08  0.795 0.845 0.065 0.14  0.73  0.7   0.265\n",
      " 0.125 0.86  0.88  0.135 0.145 0.185 0.065 0.775 0.815 0.075 0.25  0.06\n",
      " 0.235 0.07  0.255 0.075 0.08  0.795 0.73  0.02  0.22  0.895 0.06  0.035\n",
      " 0.865 0.085 0.075 0.17  0.19  0.945 0.79  0.85  0.03  0.88  0.14  0.87\n",
      " 0.11  0.825 0.135 0.835 0.11  0.895 0.09  0.115 0.935 0.01  0.145 0.04\n",
      " 0.875 0.025 0.08  0.94  0.77  0.165 0.065 0.125 0.94  0.83  0.1   0.085\n",
      " 0.9   0.065 0.895 0.75  0.145 0.895 0.895 0.72  0.95  0.09  0.03  0.16\n",
      " 0.115 0.88  0.885 0.85  0.08  0.06  0.215 0.85  0.205 0.84  0.915 0.95\n",
      " 0.095 0.04  0.9   0.845 0.09  0.89  0.145 0.935 0.165 0.69  0.895 0.965\n",
      " 0.15  0.09  0.19  0.96  0.035 0.875 0.03  0.16  0.09  0.74  0.78  0.09\n",
      " 0.8   0.195 0.14  0.03  0.79  0.205 0.885 0.74 ]\n",
      "[0.895 0.87  0.915 0.8   0.23  0.115 0.745 0.045 0.155 0.74  0.05  0.85\n",
      " 0.965 0.085 0.915 0.84  0.05  0.09  0.06  0.81  0.035 0.16  0.305 0.79\n",
      " 0.785 0.13  0.085 0.915 0.955 0.105 0.865 0.275 0.05  0.805 0.185 0.06\n",
      " 0.92  0.245 0.83  0.895 0.865 0.235 0.02  0.94  0.925 0.08  0.905 0.845\n",
      " 0.895 0.06  0.045 0.17  0.77  0.25  0.095 0.15  0.74  0.045 0.95  0.235\n",
      " 0.04  0.16  0.165 0.3   0.22  0.255 0.93  0.9   0.93  0.05  0.04  0.06\n",
      " 0.79  0.055 0.18  0.775 0.105 0.765 0.915 0.075 0.175 0.775 0.795 0.23\n",
      " 0.155 0.925 0.86  0.14  0.195 0.245 0.055 0.85  0.915 0.065 0.185 0.07\n",
      " 0.21  0.115 0.29  0.075 0.065 0.815 0.77  0.1   0.25  0.885 0.06  0.06\n",
      " 0.86  0.11  0.14  0.26  0.225 0.955 0.84  0.905 0.035 0.93  0.23  0.815\n",
      " 0.07  0.795 0.14  0.835 0.1   0.89  0.12  0.105 0.93  0.03  0.115 0.05\n",
      " 0.92  0.05  0.14  0.94  0.815 0.15  0.08  0.2   0.975 0.88  0.06  0.07\n",
      " 0.905 0.07  0.94  0.805 0.15  0.885 0.865 0.135 0.825 0.03  0.825 0.93\n",
      " 0.965 0.685 0.13  0.935 0.895 0.895 0.11  0.08  0.03  0.96  0.935 0.88\n",
      " 0.05  0.89  0.115 0.885 0.21  0.05  0.075 0.07  0.965 0.165 0.875 0.845\n",
      " 0.085 0.125 0.09  0.05  0.07  0.135 0.92  0.945 0.85  0.925 0.715 0.835\n",
      " 0.8   0.83  0.885 0.93  0.1   0.13  0.68  0.905]\n",
      "[0.05  0.195 0.73  0.205 0.11  0.165 0.785 0.03  0.95  0.25  0.05  0.12\n",
      " 0.105 0.215 0.165 0.205 0.925 0.88  0.91  0.07  0.07  0.065 0.705 0.04\n",
      " 0.19  0.75  0.095 0.79  0.82  0.06  0.165 0.785 0.795 0.235 0.1   0.915\n",
      " 0.805 0.11  0.195 0.19  0.115 0.79  0.82  0.05  0.215 0.095 0.2   0.07\n",
      " 0.2   0.075 0.06  0.825 0.72  0.055 0.12  0.875 0.07  0.03  0.885 0.105\n",
      " 0.08  0.245 0.195 0.975 0.845 0.875 0.05  0.89  0.185 0.85  0.145 0.755\n",
      " 0.1   0.85  0.095 0.885 0.11  0.13  0.97  0.02  0.115 0.07  0.845 0.06\n",
      " 0.085 0.93  0.825 0.175 0.075 0.155 0.955 0.85  0.065 0.11  0.92  0.085\n",
      " 0.9   0.745 0.055 0.88  0.915 0.17  0.735 0.05  0.855 0.935 0.975 0.725\n",
      " 0.105 0.93  0.95  0.93  0.14  0.055 0.07  0.925 0.99  0.88  0.09  0.885\n",
      " 0.09  0.945 0.195 0.04  0.055 0.065 0.965 0.155 0.88  0.83  0.125 0.085\n",
      " 0.1   0.085 0.06  0.115 0.875 0.965 0.78  0.89  0.77  0.765 0.82  0.875\n",
      " 0.935 0.935 0.105 0.2   0.64  0.945 0.935 0.76  0.925 0.03  0.06  0.17\n",
      " 0.1   0.975 0.815 0.92  0.15  0.06  0.225 0.83  0.32  0.765 0.895 0.97\n",
      " 0.115 0.035 0.94  0.9   0.065 0.915 0.215 0.97  0.17  0.695 0.945 0.965\n",
      " 0.14  0.135 0.225 0.95  0.02  0.83  0.06  0.185 0.13  0.795 0.8   0.095\n",
      " 0.755 0.11  0.145 0.05  0.765 0.16  0.87  0.775]\n",
      "[0.86  0.915 0.975 0.885 0.195 0.08  0.78  0.065 0.185 0.74  0.09  0.785\n",
      " 0.96  0.075 0.895 0.86  0.105 0.035 0.04  0.92  0.035 0.215 0.255 0.79\n",
      " 0.87  0.12  0.105 0.955 0.975 0.105 0.83  0.195 0.015 0.79  0.21  0.08\n",
      " 0.96  0.315 0.835 0.98  0.825 0.21  0.02  0.96  0.965 0.055 0.99  0.745\n",
      " 0.885 0.04  0.05  0.875 0.78  0.08  0.165 0.92  0.06  0.035 0.88  0.1\n",
      " 0.07  0.185 0.2   0.98  0.85  0.93  0.035 0.97  0.17  0.83  0.145 0.855\n",
      " 0.165 0.86  0.085 0.885 0.11  0.095 0.96  0.025 0.085 0.015 0.91  0.025\n",
      " 0.1   0.965 0.8   0.18  0.07  0.2   0.965 0.84  0.09  0.055 0.955 0.09\n",
      " 0.935 0.785 0.105 0.94  0.925 0.175 0.765 0.06  0.89  0.96  0.975 0.745\n",
      " 0.155 0.955 0.915 0.85  0.205 0.075 0.055 0.975 0.96  0.885 0.14  0.915\n",
      " 0.11  0.985 0.235 0.04  0.03  0.05  0.97  0.175 0.81  0.88  0.03  0.07\n",
      " 0.075 0.085 0.1   0.14  0.935 0.935 0.765 0.845 0.795 0.84  0.86  0.855\n",
      " 0.955 0.94  0.1   0.23  0.665 0.97  0.935 0.755 0.955 0.075 0.04  0.19\n",
      " 0.12  0.98  0.9   0.865 0.105 0.075 0.205 0.865 0.235 0.845 0.95  0.98\n",
      " 0.07  0.02  0.955 0.885 0.065 0.95  0.185 0.975 0.2   0.745 0.97  0.97\n",
      " 0.165 0.1   0.125 0.955 0.07  0.915 0.055 0.165 0.11  0.795 0.765 0.195\n",
      " 0.73  0.11  0.135 0.025 0.88  0.195 0.925 0.775]\n",
      "[0.895 0.885 0.91  0.845 0.11  0.095 0.725 0.06  0.16  0.77  0.085 0.825\n",
      " 0.93  0.135 0.875 0.85  0.085 0.11  0.05  0.84  0.06  0.185 0.255 0.72\n",
      " 0.795 0.13  0.135 0.965 0.975 0.09  0.92  0.235 0.045 0.77  0.17  0.085\n",
      " 0.91  0.195 0.835 0.915 0.825 0.2   0.035 0.935 0.94  0.08  0.955 0.785\n",
      " 0.925 0.07  0.02  0.205 0.755 0.18  0.115 0.16  0.66  0.065 0.95  0.265\n",
      " 0.07  0.1   0.09  0.24  0.15  0.205 0.9   0.885 0.925 0.045 0.045 0.065\n",
      " 0.785 0.015 0.21  0.77  0.085 0.81  0.93  0.095 0.17  0.815 0.755 0.22\n",
      " 0.14  0.905 0.855 0.13  0.2   0.175 0.05  0.835 0.865 0.1   0.26  0.075\n",
      " 0.2   0.115 0.255 0.06  0.88  0.09  0.795 0.04  0.83  0.92  0.965 0.75\n",
      " 0.15  0.955 0.915 0.925 0.115 0.055 0.09  0.915 0.955 0.835 0.17  0.885\n",
      " 0.13  0.945 0.23  0.08  0.08  0.065 0.965 0.22  0.83  0.82  0.105 0.08\n",
      " 0.125 0.09  0.085 0.165 0.905 0.915 0.835 0.87  0.785 0.765 0.75  0.8\n",
      " 0.905 0.96  0.08  0.165 0.645 0.935 0.9   0.79  0.915 0.08  0.08  0.125\n",
      " 0.13  0.97  0.87  0.845 0.095 0.04  0.25  0.81  0.24  0.79  0.88  0.97\n",
      " 0.08  0.025 0.93  0.895 0.08  0.96  0.135 0.955 0.17  0.725 0.91  0.95\n",
      " 0.145 0.135 0.175 0.93  0.105 0.84  0.09  0.16  0.15  0.78  0.74  0.14\n",
      " 0.745 0.12  0.15  0.04  0.8   0.19  0.93  0.77 ]\n",
      "[0.89  0.88  0.92  0.88  0.18  0.06  0.745 0.04  0.115 0.745 0.065 0.81\n",
      " 0.935 0.075 0.895 0.8   0.145 0.065 0.08  0.845 0.04  0.125 0.225 0.755\n",
      " 0.77  0.09  0.12  0.92  0.96  0.08  0.835 0.14  0.055 0.795 0.18  0.09\n",
      " 0.915 0.32  0.785 0.955 0.835 0.205 0.015 0.935 0.945 0.055 0.92  0.795\n",
      " 0.94  0.07  0.03  0.15  0.785 0.205 0.095 0.135 0.745 0.03  0.955 0.23\n",
      " 0.08  0.105 0.095 0.25  0.155 0.17  0.91  0.91  0.9   0.03  0.04  0.035\n",
      " 0.7   0.055 0.185 0.845 0.09  0.8   0.865 0.07  0.16  0.735 0.78  0.195\n",
      " 0.115 0.895 0.845 0.12  0.195 0.21  0.075 0.815 0.78  0.05  0.155 0.045\n",
      " 0.245 0.08  0.215 0.05  0.09  0.805 0.71  0.05  0.165 0.89  0.05  0.045\n",
      " 0.86  0.16  0.065 0.14  0.175 0.95  0.77  0.855 0.03  0.95  0.155 0.84\n",
      " 0.095 0.765 0.13  0.83  0.125 0.815 0.085 0.115 0.92  0.025 0.1   0.015\n",
      " 0.865 0.03  0.14  0.945 0.775 0.225 0.09  0.155 0.955 0.855 0.11  0.055\n",
      " 0.915 0.045 0.915 0.77  0.09  0.91  0.9   0.69  0.945 0.09  0.08  0.19\n",
      " 0.135 0.945 0.86  0.89  0.08  0.04  0.185 0.86  0.205 0.795 0.915 0.965\n",
      " 0.09  0.02  0.945 0.9   0.09  0.935 0.125 0.93  0.18  0.675 0.93  0.965\n",
      " 0.105 0.085 0.2   0.92  0.06  0.845 0.025 0.13  0.13  0.79  0.765 0.1\n",
      " 0.725 0.17  0.14  0.05  0.815 0.13  0.925 0.78 ]\n",
      "[0.895 0.895 0.915 0.815 0.18  0.07  0.725 0.05  0.16  0.735 0.06  0.84\n",
      " 0.94  0.07  0.915 0.825 0.095 0.09  0.06  0.82  0.025 0.155 0.285 0.755\n",
      " 0.77  0.09  0.105 0.905 0.95  0.08  0.89  0.195 0.03  0.755 0.21  0.095\n",
      " 0.945 0.235 0.8   0.945 0.825 0.28  0.035 0.95  0.955 0.09  0.95  0.82\n",
      " 0.91  0.05  0.04  0.12  0.785 0.28  0.135 0.14  0.745 0.045 0.94  0.28\n",
      " 0.06  0.14  0.11  0.255 0.17  0.275 0.92  0.915 0.945 0.075 0.035 0.045\n",
      " 0.66  0.06  0.215 0.785 0.035 0.77  0.9   0.085 0.185 0.765 0.755 0.245\n",
      " 0.1   0.91  0.84  0.1   0.16  0.265 0.05  0.78  0.87  0.05  0.205 0.06\n",
      " 0.19  0.135 0.24  0.065 0.08  0.83  0.69  0.1   0.205 0.92  0.06  0.04\n",
      " 0.84  0.11  0.09  0.195 0.26  0.97  0.79  0.86  0.025 0.905 0.12  0.82\n",
      " 0.075 0.77  0.155 0.84  0.11  0.9   0.085 0.095 0.94  0.025 0.11  0.075\n",
      " 0.895 0.045 0.085 0.925 0.785 0.115 0.085 0.16  0.955 0.815 0.1   0.02\n",
      " 0.885 0.075 0.945 0.765 0.07  0.93  0.88  0.15  0.76  0.025 0.875 0.94\n",
      " 0.97  0.75  0.155 0.945 0.885 0.91  0.085 0.05  0.03  0.905 0.945 0.885\n",
      " 0.09  0.87  0.13  0.955 0.215 0.065 0.045 0.065 0.95  0.165 0.895 0.835\n",
      " 0.085 0.12  0.085 0.085 0.05  0.14  0.935 0.915 0.835 0.88  0.79  0.88\n",
      " 0.82  0.815 0.915 0.955 0.145 0.17  0.61  0.92 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.035 0.155 0.755 0.175 0.115 0.145 0.725 0.04  0.97  0.21  0.08  0.1\n",
      " 0.095 0.28  0.165 0.255 0.905 0.88  0.89  0.09  0.035 0.115 0.795 0.025\n",
      " 0.185 0.76  0.08  0.8   0.845 0.11  0.15  0.785 0.72  0.245 0.105 0.945\n",
      " 0.8   0.11  0.155 0.2   0.11  0.785 0.82  0.07  0.2   0.09  0.225 0.095\n",
      " 0.185 0.075 0.08  0.795 0.785 0.09  0.13  0.865 0.075 0.035 0.86  0.08\n",
      " 0.09  0.205 0.16  0.92  0.815 0.905 0.035 0.92  0.245 0.84  0.095 0.735\n",
      " 0.055 0.825 0.1   0.875 0.065 0.105 0.97  0.075 0.08  0.045 0.86  0.075\n",
      " 0.155 0.915 0.83  0.135 0.08  0.165 0.98  0.83  0.095 0.035 0.905 0.07\n",
      " 0.89  0.79  0.095 0.855 0.935 0.14  0.72  0.035 0.9   0.915 0.97  0.75\n",
      " 0.11  0.96  0.92  0.93  0.14  0.04  0.08  0.95  0.99  0.845 0.085 0.93\n",
      " 0.095 0.935 0.17  0.02  0.065 0.065 0.965 0.18  0.78  0.82  0.13  0.07\n",
      " 0.09  0.09  0.065 0.14  0.87  0.94  0.82  0.885 0.785 0.765 0.885 0.84\n",
      " 0.925 0.935 0.055 0.23  0.655 0.945 0.96  0.72  0.925 0.08  0.05  0.14\n",
      " 0.095 0.98  0.825 0.895 0.09  0.1   0.245 0.845 0.27  0.755 0.91  0.98\n",
      " 0.1   0.05  0.92  0.915 0.065 0.91  0.17  0.955 0.175 0.79  0.955 0.955\n",
      " 0.095 0.135 0.145 0.98  0.05  0.795 0.075 0.16  0.11  0.81  0.82  0.13\n",
      " 0.68  0.13  0.13  0.055 0.78  0.16  0.885 0.815]\n",
      "[0.855 0.905 0.97  0.84  0.105 0.08  0.815 0.065 0.205 0.76  0.045 0.795\n",
      " 0.975 0.05  0.89  0.825 0.085 0.045 0.085 0.92  0.055 0.16  0.325 0.8\n",
      " 0.84  0.155 0.135 0.985 0.95  0.085 0.86  0.195 0.02  0.805 0.195 0.09\n",
      " 0.96  0.26  0.835 0.96  0.88  0.215 0.02  0.975 0.94  0.05  0.95  0.795\n",
      " 0.935 0.06  0.095 0.9   0.76  0.04  0.25  0.945 0.045 0.045 0.905 0.115\n",
      " 0.055 0.21  0.265 0.96  0.84  0.93  0.035 0.95  0.2   0.885 0.1   0.815\n",
      " 0.19  0.855 0.145 0.92  0.13  0.12  0.97  0.03  0.095 0.03  0.905 0.02\n",
      " 0.075 0.965 0.8   0.16  0.07  0.155 0.955 0.795 0.055 0.055 0.935 0.075\n",
      " 0.96  0.725 0.11  0.895 0.95  0.19  0.78  0.04  0.955 0.96  0.965 0.81\n",
      " 0.19  0.945 0.91  0.925 0.135 0.045 0.03  0.985 0.98  0.905 0.11  0.92\n",
      " 0.12  0.985 0.25  0.08  0.035 0.08  0.99  0.22  0.85  0.855 0.04  0.095\n",
      " 0.125 0.115 0.09  0.115 0.96  0.925 0.795 0.87  0.75  0.86  0.835 0.88\n",
      " 0.94  0.96  0.08  0.22  0.64  0.99  0.915 0.695 0.935 0.085 0.025 0.215\n",
      " 0.155 0.995 0.89  0.895 0.12  0.03  0.235 0.845 0.295 0.895 0.93  0.975\n",
      " 0.07  0.03  0.96  0.92  0.09  0.96  0.26  0.96  0.18  0.78  0.95  0.975\n",
      " 0.17  0.07  0.22  0.96  0.06  0.915 0.075 0.175 0.11  0.85  0.825 0.14\n",
      " 0.76  0.165 0.165 0.03  0.865 0.155 0.95  0.795]\n",
      "[0.87  0.935 0.93  0.805 0.175 0.11  0.795 0.055 0.135 0.71  0.06  0.845\n",
      " 0.955 0.115 0.905 0.84  0.085 0.065 0.07  0.855 0.07  0.175 0.26  0.735\n",
      " 0.83  0.1   0.16  0.955 0.98  0.11  0.865 0.225 0.06  0.81  0.165 0.09\n",
      " 0.96  0.2   0.78  0.935 0.81  0.26  0.015 0.94  0.955 0.085 0.955 0.765\n",
      " 0.89  0.055 0.04  0.19  0.73  0.165 0.105 0.23  0.745 0.04  0.965 0.29\n",
      " 0.1   0.16  0.095 0.285 0.165 0.19  0.905 0.905 0.9   0.08  0.06  0.045\n",
      " 0.675 0.06  0.175 0.74  0.075 0.83  0.91  0.1   0.16  0.725 0.735 0.3\n",
      " 0.17  0.875 0.845 0.125 0.24  0.23  0.035 0.825 0.845 0.055 0.255 0.135\n",
      " 0.235 0.065 0.25  0.105 0.94  0.105 0.75  0.05  0.9   0.955 0.985 0.72\n",
      " 0.19  0.94  0.905 0.895 0.11  0.065 0.055 0.9   0.96  0.825 0.155 0.875\n",
      " 0.16  0.92  0.19  0.09  0.065 0.025 0.965 0.15  0.88  0.83  0.065 0.08\n",
      " 0.085 0.075 0.06  0.145 0.92  0.94  0.825 0.905 0.81  0.77  0.84  0.84\n",
      " 0.93  0.945 0.07  0.185 0.7   0.945 0.925 0.76  0.915 0.065 0.08  0.2\n",
      " 0.15  0.96  0.83  0.83  0.1   0.085 0.285 0.845 0.24  0.81  0.915 0.97\n",
      " 0.11  0.05  0.96  0.875 0.095 0.96  0.205 0.96  0.135 0.75  0.945 0.955\n",
      " 0.15  0.14  0.24  0.945 0.065 0.9   0.065 0.195 0.165 0.825 0.76  0.125\n",
      " 0.765 0.13  0.18  0.055 0.805 0.15  0.91  0.83 ]\n",
      "[0.825 0.855 0.925 0.83  0.15  0.08  0.795 0.055 0.205 0.76  0.04  0.78\n",
      " 0.96  0.095 0.885 0.825 0.11  0.06  0.06  0.845 0.02  0.16  0.215 0.77\n",
      " 0.82  0.085 0.11  0.94  0.915 0.085 0.86  0.175 0.015 0.83  0.185 0.065\n",
      " 0.92  0.19  0.835 0.925 0.84  0.28  0.025 0.925 0.98  0.065 0.945 0.745\n",
      " 0.9   0.045 0.03  0.135 0.73  0.165 0.085 0.215 0.79  0.055 0.955 0.27\n",
      " 0.065 0.18  0.08  0.25  0.175 0.14  0.935 0.87  0.875 0.06  0.055 0.08\n",
      " 0.66  0.035 0.225 0.775 0.07  0.81  0.93  0.08  0.165 0.745 0.72  0.25\n",
      " 0.13  0.92  0.84  0.135 0.19  0.205 0.1   0.82  0.815 0.065 0.195 0.09\n",
      " 0.18  0.085 0.27  0.085 0.05  0.81  0.73  0.085 0.19  0.885 0.05  0.045\n",
      " 0.865 0.115 0.06  0.19  0.165 0.96  0.855 0.875 0.015 0.93  0.22  0.855\n",
      " 0.12  0.795 0.15  0.83  0.08  0.865 0.105 0.12  0.92  0.025 0.075 0.04\n",
      " 0.825 0.035 0.145 0.925 0.77  0.16  0.095 0.15  0.955 0.795 0.14  0.045\n",
      " 0.94  0.07  0.92  0.755 0.085 0.9   0.9   0.73  0.95  0.1   0.07  0.205\n",
      " 0.135 0.96  0.94  0.905 0.105 0.025 0.17  0.82  0.24  0.845 0.91  0.955\n",
      " 0.07  0.05  0.925 0.85  0.085 0.92  0.16  0.945 0.15  0.71  0.935 0.98\n",
      " 0.16  0.08  0.23  0.935 0.055 0.87  0.02  0.18  0.12  0.8   0.78  0.135\n",
      " 0.785 0.145 0.17  0.06  0.83  0.175 0.905 0.815]\n",
      "[0.865 0.905 0.935 0.745 0.165 0.08  0.775 0.05  0.14  0.74  0.04  0.825\n",
      " 0.97  0.095 0.92  0.835 0.08  0.05  0.085 0.85  0.035 0.155 0.305 0.775\n",
      " 0.75  0.12  0.105 0.94  0.94  0.095 0.885 0.215 0.03  0.71  0.165 0.08\n",
      " 0.96  0.29  0.83  0.915 0.81  0.265 0.04  0.925 0.92  0.085 0.92  0.75\n",
      " 0.87  0.03  0.025 0.16  0.75  0.255 0.135 0.23  0.74  0.045 0.97  0.27\n",
      " 0.055 0.125 0.14  0.25  0.245 0.22  0.905 0.91  0.93  0.055 0.03  0.05\n",
      " 0.735 0.05  0.18  0.79  0.1   0.75  0.87  0.08  0.16  0.8   0.8   0.24\n",
      " 0.145 0.92  0.845 0.135 0.27  0.215 0.05  0.83  0.85  0.04  0.2   0.08\n",
      " 0.175 0.13  0.25  0.045 0.115 0.795 0.78  0.07  0.22  0.89  0.025 0.04\n",
      " 0.86  0.11  0.065 0.22  0.23  0.96  0.795 0.89  0.035 0.92  0.195 0.84\n",
      " 0.085 0.805 0.125 0.905 0.1   0.91  0.09  0.12  0.945 0.035 0.105 0.015\n",
      " 0.895 0.035 0.125 0.945 0.84  0.16  0.08  0.155 0.965 0.82  0.1   0.055\n",
      " 0.92  0.065 0.95  0.775 0.13  0.895 0.915 0.1   0.785 0.015 0.88  0.945\n",
      " 0.955 0.755 0.135 0.935 0.9   0.885 0.14  0.04  0.08  0.925 0.97  0.86\n",
      " 0.08  0.9   0.075 0.91  0.2   0.045 0.05  0.1   0.925 0.165 0.86  0.82\n",
      " 0.06  0.075 0.055 0.06  0.065 0.125 0.955 0.915 0.835 0.91  0.785 0.78\n",
      " 0.83  0.795 0.955 0.925 0.11  0.205 0.695 0.885]\n",
      "[0.065 0.15  0.675 0.15  0.14  0.19  0.775 0.045 0.94  0.225 0.105 0.13\n",
      " 0.095 0.21  0.155 0.22  0.9   0.86  0.87  0.06  0.05  0.085 0.74  0.05\n",
      " 0.215 0.79  0.07  0.76  0.76  0.085 0.155 0.76  0.815 0.24  0.135 0.915\n",
      " 0.85  0.135 0.165 0.19  0.06  0.875 0.84  0.045 0.215 0.06  0.205 0.09\n",
      " 0.24  0.07  0.065 0.79  0.77  0.045 0.175 0.88  0.065 0.035 0.83  0.085\n",
      " 0.075 0.245 0.22  0.97  0.765 0.92  0.045 0.92  0.225 0.825 0.1   0.815\n",
      " 0.06  0.84  0.085 0.91  0.1   0.095 0.95  0.035 0.12  0.05  0.91  0.05\n",
      " 0.13  0.92  0.825 0.185 0.07  0.17  0.97  0.865 0.07  0.035 0.93  0.025\n",
      " 0.885 0.74  0.06  0.9   0.925 0.165 0.735 0.035 0.86  0.93  0.96  0.71\n",
      " 0.15  0.945 0.945 0.925 0.135 0.055 0.075 0.945 0.965 0.86  0.12  0.87\n",
      " 0.14  0.95  0.24  0.035 0.065 0.115 0.97  0.205 0.8   0.84  0.065 0.085\n",
      " 0.1   0.08  0.04  0.17  0.9   0.945 0.81  0.93  0.75  0.845 0.815 0.84\n",
      " 0.935 0.94  0.13  0.21  0.655 0.955 0.96  0.705 0.915 0.06  0.08  0.16\n",
      " 0.135 0.945 0.88  0.87  0.1   0.05  0.305 0.835 0.26  0.79  0.895 0.975\n",
      " 0.075 0.02  0.93  0.895 0.05  0.945 0.195 0.92  0.135 0.72  0.925 0.945\n",
      " 0.12  0.125 0.185 0.94  0.05  0.895 0.06  0.16  0.115 0.735 0.785 0.13\n",
      " 0.735 0.12  0.18  0.05  0.75  0.18  0.89  0.775]\n",
      "[0.885 0.935 0.96  0.835 0.185 0.08  0.81  0.06  0.245 0.795 0.045 0.835\n",
      " 0.955 0.07  0.93  0.885 0.08  0.065 0.07  0.87  0.035 0.175 0.3   0.79\n",
      " 0.885 0.1   0.135 0.98  0.95  0.115 0.89  0.21  0.035 0.78  0.2   0.07\n",
      " 0.935 0.33  0.81  0.955 0.835 0.29  0.03  0.975 0.96  0.07  0.965 0.77\n",
      " 0.935 0.04  0.075 0.905 0.73  0.065 0.175 0.945 0.075 0.04  0.91  0.145\n",
      " 0.11  0.17  0.22  0.99  0.835 0.905 0.015 0.96  0.17  0.85  0.14  0.83\n",
      " 0.17  0.845 0.095 0.915 0.11  0.085 0.97  0.025 0.06  0.07  0.94  0.025\n",
      " 0.1   0.94  0.8   0.22  0.05  0.15  0.95  0.78  0.09  0.03  0.955 0.045\n",
      " 0.945 0.785 0.085 0.93  0.91  0.225 0.815 0.03  0.93  0.975 0.985 0.765\n",
      " 0.165 0.99  0.915 0.905 0.145 0.03  0.075 0.945 0.975 0.905 0.155 0.87\n",
      " 0.08  0.99  0.225 0.02  0.065 0.055 0.97  0.23  0.88  0.875 0.075 0.07\n",
      " 0.065 0.075 0.085 0.095 0.965 0.93  0.765 0.905 0.82  0.86  0.825 0.875\n",
      " 0.945 0.97  0.08  0.29  0.725 0.96  0.955 0.66  0.955 0.095 0.05  0.18\n",
      " 0.12  0.995 0.895 0.85  0.085 0.075 0.19  0.89  0.27  0.825 0.965 0.96\n",
      " 0.075 0.03  0.945 0.91  0.145 0.94  0.28  0.975 0.15  0.725 0.95  0.965\n",
      " 0.19  0.1   0.17  0.98  0.045 0.91  0.07  0.22  0.12  0.83  0.805 0.18\n",
      " 0.755 0.125 0.17  0.055 0.83  0.205 0.955 0.8  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.885 0.895 0.895 0.835 0.18  0.14  0.83  0.065 0.195 0.695 0.075 0.855\n",
      " 0.945 0.075 0.87  0.84  0.055 0.06  0.065 0.865 0.06  0.185 0.18  0.775\n",
      " 0.82  0.085 0.16  0.93  0.935 0.09  0.845 0.195 0.045 0.77  0.135 0.075\n",
      " 0.95  0.26  0.795 0.92  0.79  0.205 0.04  0.925 0.95  0.105 0.935 0.755\n",
      " 0.915 0.06  0.025 0.185 0.75  0.185 0.115 0.18  0.725 0.05  0.965 0.295\n",
      " 0.095 0.105 0.075 0.265 0.165 0.195 0.895 0.88  0.895 0.075 0.025 0.105\n",
      " 0.74  0.075 0.16  0.775 0.09  0.81  0.9   0.12  0.205 0.83  0.855 0.245\n",
      " 0.13  0.915 0.885 0.14  0.255 0.225 0.065 0.81  0.88  0.05  0.185 0.105\n",
      " 0.185 0.11  0.24  0.095 0.885 0.14  0.78  0.03  0.83  0.925 0.965 0.75\n",
      " 0.135 0.955 0.935 0.91  0.1   0.055 0.105 0.91  0.98  0.86  0.15  0.87\n",
      " 0.145 0.97  0.185 0.05  0.095 0.05  0.93  0.24  0.81  0.815 0.105 0.075\n",
      " 0.1   0.055 0.07  0.145 0.915 0.97  0.815 0.915 0.855 0.805 0.845 0.845\n",
      " 0.925 0.925 0.105 0.235 0.65  0.955 0.925 0.75  0.95  0.09  0.085 0.135\n",
      " 0.18  0.945 0.86  0.835 0.09  0.05  0.3   0.83  0.26  0.79  0.91  0.96\n",
      " 0.145 0.04  0.925 0.875 0.12  0.94  0.235 0.92  0.155 0.675 0.925 0.96\n",
      " 0.15  0.145 0.195 0.955 0.07  0.835 0.05  0.185 0.13  0.745 0.795 0.115\n",
      " 0.75  0.12  0.135 0.065 0.765 0.15  0.94  0.845]\n",
      "[0.855 0.905 0.925 0.83  0.215 0.04  0.685 0.04  0.13  0.725 0.04  0.765\n",
      " 0.94  0.07  0.86  0.825 0.095 0.06  0.115 0.865 0.045 0.21  0.23  0.725\n",
      " 0.845 0.09  0.115 0.915 0.92  0.06  0.865 0.185 0.045 0.84  0.185 0.11\n",
      " 0.91  0.19  0.805 0.92  0.83  0.265 0.02  0.93  0.975 0.03  0.94  0.8\n",
      " 0.9   0.045 0.04  0.135 0.72  0.17  0.08  0.18  0.78  0.04  0.95  0.285\n",
      " 0.07  0.17  0.115 0.225 0.215 0.185 0.9   0.89  0.895 0.04  0.045 0.085\n",
      " 0.71  0.05  0.19  0.79  0.13  0.79  0.88  0.07  0.125 0.73  0.76  0.195\n",
      " 0.125 0.865 0.865 0.17  0.17  0.235 0.07  0.805 0.885 0.085 0.2   0.07\n",
      " 0.235 0.08  0.17  0.045 0.075 0.815 0.765 0.07  0.18  0.885 0.065 0.035\n",
      " 0.845 0.12  0.05  0.135 0.18  0.955 0.81  0.855 0.045 0.94  0.16  0.84\n",
      " 0.1   0.805 0.13  0.84  0.065 0.845 0.08  0.08  0.955 0.045 0.115 0.055\n",
      " 0.875 0.025 0.145 0.945 0.815 0.2   0.1   0.125 0.95  0.825 0.085 0.08\n",
      " 0.925 0.085 0.84  0.79  0.11  0.895 0.91  0.745 0.935 0.095 0.075 0.125\n",
      " 0.12  0.93  0.865 0.855 0.075 0.045 0.265 0.83  0.16  0.82  0.92  0.955\n",
      " 0.06  0.02  0.95  0.835 0.075 0.925 0.155 0.91  0.175 0.705 0.92  0.96\n",
      " 0.165 0.13  0.17  0.935 0.08  0.87  0.065 0.085 0.14  0.8   0.79  0.12\n",
      " 0.76  0.11  0.19  0.06  0.775 0.165 0.87  0.845]\n",
      "[0.93  0.915 0.915 0.795 0.185 0.095 0.785 0.05  0.18  0.795 0.035 0.835\n",
      " 0.95  0.075 0.945 0.83  0.055 0.055 0.065 0.83  0.04  0.125 0.29  0.755\n",
      " 0.76  0.065 0.115 0.95  0.955 0.06  0.91  0.25  0.045 0.755 0.16  0.085\n",
      " 0.945 0.22  0.845 0.895 0.8   0.275 0.05  0.925 0.96  0.085 0.925 0.795\n",
      " 0.895 0.06  0.055 0.215 0.76  0.205 0.09  0.165 0.785 0.045 0.945 0.3\n",
      " 0.035 0.08  0.125 0.22  0.22  0.245 0.9   0.94  0.92  0.065 0.04  0.08\n",
      " 0.67  0.03  0.205 0.735 0.06  0.745 0.91  0.11  0.17  0.715 0.75  0.26\n",
      " 0.1   0.945 0.845 0.155 0.16  0.205 0.065 0.83  0.87  0.05  0.19  0.08\n",
      " 0.165 0.09  0.25  0.065 0.075 0.875 0.725 0.055 0.22  0.91  0.05  0.045\n",
      " 0.825 0.125 0.05  0.195 0.21  0.95  0.825 0.915 0.    0.915 0.235 0.82\n",
      " 0.125 0.755 0.155 0.815 0.095 0.87  0.135 0.125 0.965 0.03  0.14  0.025\n",
      " 0.94  0.025 0.13  0.98  0.785 0.12  0.05  0.145 0.96  0.845 0.08  0.05\n",
      " 0.88  0.095 0.94  0.755 0.125 0.89  0.94  0.205 0.765 0.01  0.885 0.96\n",
      " 0.975 0.77  0.17  0.945 0.905 0.895 0.115 0.035 0.06  0.925 0.96  0.86\n",
      " 0.08  0.895 0.07  0.93  0.205 0.03  0.08  0.095 0.98  0.22  0.87  0.825\n",
      " 0.08  0.07  0.085 0.065 0.05  0.125 0.925 0.935 0.815 0.89  0.825 0.86\n",
      " 0.795 0.83  0.9   0.945 0.1   0.16  0.63  0.95 ]\n",
      "[0.02  0.175 0.77  0.245 0.16  0.17  0.85  0.04  0.955 0.205 0.115 0.135\n",
      " 0.08  0.245 0.18  0.24  0.925 0.865 0.92  0.085 0.04  0.055 0.69  0.075\n",
      " 0.205 0.76  0.075 0.765 0.85  0.07  0.17  0.735 0.74  0.22  0.09  0.92\n",
      " 0.815 0.175 0.195 0.23  0.065 0.79  0.86  0.055 0.235 0.085 0.17  0.075\n",
      " 0.23  0.065 0.08  0.825 0.735 0.065 0.225 0.885 0.05  0.055 0.895 0.115\n",
      " 0.06  0.19  0.25  0.965 0.815 0.93  0.04  0.915 0.205 0.85  0.115 0.81\n",
      " 0.12  0.87  0.11  0.89  0.145 0.055 0.955 0.045 0.07  0.08  0.89  0.05\n",
      " 0.13  0.96  0.815 0.175 0.06  0.15  0.98  0.865 0.06  0.085 0.94  0.035\n",
      " 0.915 0.74  0.11  0.88  0.935 0.19  0.755 0.025 0.9   0.915 0.965 0.735\n",
      " 0.135 0.93  0.915 0.9   0.125 0.045 0.06  0.96  0.96  0.875 0.1   0.895\n",
      " 0.115 0.97  0.195 0.05  0.065 0.105 0.94  0.2   0.81  0.83  0.125 0.06\n",
      " 0.085 0.06  0.065 0.185 0.87  0.945 0.78  0.91  0.75  0.85  0.815 0.815\n",
      " 0.98  0.955 0.11  0.16  0.68  0.96  0.94  0.685 0.915 0.04  0.09  0.185\n",
      " 0.1   0.965 0.86  0.865 0.17  0.04  0.245 0.865 0.305 0.81  0.91  0.97\n",
      " 0.09  0.045 0.9   0.935 0.09  0.915 0.25  0.95  0.125 0.72  0.94  0.965\n",
      " 0.14  0.13  0.125 0.96  0.09  0.86  0.025 0.145 0.085 0.81  0.735 0.15\n",
      " 0.75  0.145 0.13  0.05  0.81  0.175 0.875 0.785]\n",
      "[0.9   0.93  0.96  0.85  0.165 0.085 0.82  0.06  0.205 0.665 0.075 0.83\n",
      " 0.96  0.06  0.9   0.88  0.09  0.07  0.06  0.91  0.035 0.23  0.325 0.83\n",
      " 0.885 0.12  0.095 0.95  0.975 0.1   0.87  0.21  0.035 0.795 0.225 0.075\n",
      " 0.96  0.265 0.845 0.945 0.885 0.21  0.02  0.945 0.94  0.09  0.975 0.73\n",
      " 0.915 0.03  0.09  0.905 0.725 0.07  0.23  0.92  0.05  0.04  0.92  0.1\n",
      " 0.085 0.2   0.22  0.99  0.86  0.925 0.025 0.985 0.16  0.775 0.08  0.79\n",
      " 0.21  0.855 0.11  0.88  0.085 0.11  0.965 0.035 0.1   0.03  0.875 0.025\n",
      " 0.14  0.945 0.805 0.24  0.05  0.22  0.96  0.81  0.05  0.035 0.97  0.045\n",
      " 0.955 0.81  0.135 0.92  0.935 0.16  0.725 0.03  0.93  0.935 0.985 0.745\n",
      " 0.165 0.975 0.935 0.945 0.195 0.07  0.025 0.92  0.98  0.865 0.15  0.9\n",
      " 0.12  0.975 0.21  0.045 0.045 0.075 0.94  0.21  0.845 0.905 0.05  0.03\n",
      " 0.105 0.07  0.045 0.09  0.965 0.945 0.785 0.85  0.81  0.815 0.82  0.925\n",
      " 0.92  0.955 0.055 0.22  0.69  0.96  0.925 0.695 0.935 0.055 0.045 0.19\n",
      " 0.155 0.985 0.87  0.88  0.085 0.02  0.195 0.85  0.23  0.865 0.965 0.98\n",
      " 0.075 0.03  0.965 0.915 0.1   0.955 0.22  0.97  0.175 0.765 0.975 0.97\n",
      " 0.16  0.13  0.22  0.965 0.055 0.92  0.04  0.185 0.135 0.81  0.8   0.145\n",
      " 0.705 0.115 0.18  0.025 0.87  0.17  0.9   0.775]\n",
      "[0.885 0.895 0.895 0.845 0.135 0.09  0.82  0.07  0.155 0.725 0.045 0.805\n",
      " 0.97  0.115 0.89  0.85  0.125 0.075 0.07  0.855 0.04  0.11  0.255 0.77\n",
      " 0.835 0.12  0.12  0.96  0.96  0.125 0.895 0.175 0.06  0.84  0.205 0.12\n",
      " 0.97  0.255 0.8   0.875 0.78  0.155 0.03  0.94  0.96  0.065 0.955 0.83\n",
      " 0.92  0.045 0.05  0.175 0.725 0.22  0.155 0.19  0.745 0.055 0.945 0.27\n",
      " 0.08  0.135 0.15  0.27  0.185 0.235 0.885 0.94  0.9   0.035 0.055 0.075\n",
      " 0.715 0.04  0.14  0.75  0.045 0.77  0.895 0.09  0.2   0.745 0.765 0.27\n",
      " 0.145 0.915 0.815 0.135 0.225 0.205 0.035 0.775 0.84  0.06  0.2   0.085\n",
      " 0.2   0.15  0.23  0.095 0.905 0.115 0.755 0.05  0.905 0.955 0.965 0.74\n",
      " 0.14  0.94  0.945 0.895 0.14  0.06  0.05  0.92  0.97  0.865 0.175 0.89\n",
      " 0.125 0.935 0.19  0.085 0.095 0.095 0.93  0.18  0.825 0.865 0.07  0.105\n",
      " 0.095 0.11  0.09  0.13  0.895 0.96  0.805 0.85  0.815 0.825 0.875 0.845\n",
      " 0.895 0.935 0.115 0.185 0.685 0.95  0.895 0.72  0.965 0.1   0.045 0.15\n",
      " 0.15  0.965 0.83  0.855 0.09  0.045 0.31  0.835 0.205 0.855 0.92  0.98\n",
      " 0.095 0.045 0.93  0.925 0.09  0.955 0.225 0.94  0.16  0.775 0.93  0.98\n",
      " 0.205 0.125 0.175 0.935 0.04  0.865 0.055 0.19  0.09  0.75  0.775 0.17\n",
      " 0.725 0.16  0.175 0.055 0.805 0.195 0.88  0.82 ]\n",
      "[0.87  0.86  0.925 0.83  0.225 0.075 0.755 0.04  0.155 0.74  0.06  0.805\n",
      " 0.925 0.11  0.865 0.8   0.11  0.055 0.065 0.855 0.01  0.175 0.26  0.725\n",
      " 0.8   0.08  0.095 0.915 0.945 0.085 0.855 0.19  0.025 0.835 0.155 0.06\n",
      " 0.94  0.21  0.795 0.93  0.81  0.25  0.025 0.92  0.955 0.055 0.92  0.72\n",
      " 0.88  0.05  0.025 0.185 0.735 0.19  0.085 0.24  0.755 0.02  0.96  0.31\n",
      " 0.08  0.115 0.085 0.225 0.2   0.205 0.91  0.875 0.915 0.04  0.035 0.075\n",
      " 0.685 0.035 0.15  0.74  0.06  0.795 0.865 0.11  0.115 0.745 0.775 0.205\n",
      " 0.11  0.955 0.88  0.145 0.21  0.245 0.07  0.72  0.855 0.065 0.16  0.055\n",
      " 0.21  0.09  0.32  0.09  0.045 0.765 0.745 0.075 0.175 0.88  0.05  0.035\n",
      " 0.865 0.13  0.055 0.195 0.185 0.96  0.815 0.9   0.015 0.91  0.23  0.81\n",
      " 0.05  0.77  0.14  0.84  0.08  0.92  0.07  0.105 0.95  0.045 0.13  0.035\n",
      " 0.88  0.05  0.155 0.95  0.835 0.185 0.11  0.2   0.975 0.79  0.13  0.04\n",
      " 0.895 0.065 0.915 0.755 0.13  0.88  0.93  0.775 0.945 0.07  0.03  0.13\n",
      " 0.13  0.93  0.89  0.865 0.1   0.06  0.27  0.85  0.245 0.825 0.89  0.95\n",
      " 0.07  0.025 0.935 0.875 0.055 0.93  0.105 0.905 0.125 0.72  0.935 0.95\n",
      " 0.15  0.095 0.16  0.955 0.04  0.905 0.055 0.165 0.115 0.78  0.75  0.12\n",
      " 0.71  0.095 0.19  0.065 0.775 0.13  0.92  0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9   0.9   0.92  0.8   0.185 0.085 0.775 0.045 0.165 0.76  0.06  0.835\n",
      " 0.965 0.115 0.935 0.81  0.095 0.055 0.065 0.825 0.04  0.145 0.335 0.745\n",
      " 0.815 0.11  0.13  0.94  0.97  0.095 0.935 0.3   0.02  0.735 0.155 0.1\n",
      " 0.935 0.245 0.885 0.95  0.86  0.245 0.055 0.935 0.935 0.065 0.94  0.79\n",
      " 0.895 0.05  0.03  0.205 0.715 0.255 0.115 0.175 0.705 0.04  0.955 0.27\n",
      " 0.055 0.13  0.11  0.25  0.225 0.27  0.915 0.905 0.92  0.06  0.04  0.08\n",
      " 0.74  0.05  0.175 0.755 0.065 0.765 0.89  0.12  0.2   0.765 0.75  0.285\n",
      " 0.155 0.95  0.795 0.135 0.21  0.26  0.035 0.845 0.875 0.065 0.205 0.105\n",
      " 0.205 0.075 0.26  0.07  0.1   0.84  0.74  0.085 0.25  0.885 0.045 0.055\n",
      " 0.85  0.065 0.06  0.195 0.205 0.975 0.785 0.905 0.04  0.895 0.2   0.835\n",
      " 0.11  0.845 0.105 0.86  0.1   0.885 0.085 0.13  0.945 0.055 0.115 0.08\n",
      " 0.915 0.03  0.12  0.94  0.79  0.125 0.09  0.135 0.96  0.83  0.07  0.02\n",
      " 0.925 0.07  0.89  0.77  0.165 0.915 0.925 0.145 0.805 0.01  0.865 0.97\n",
      " 0.97  0.735 0.13  0.94  0.905 0.92  0.145 0.075 0.06  0.925 0.945 0.83\n",
      " 0.12  0.925 0.06  0.895 0.31  0.04  0.05  0.055 0.96  0.195 0.915 0.86\n",
      " 0.05  0.1   0.11  0.095 0.05  0.14  0.935 0.925 0.855 0.92  0.795 0.855\n",
      " 0.825 0.835 0.935 0.93  0.08  0.185 0.67  0.915]\n",
      "[0.045 0.19  0.745 0.19  0.1   0.195 0.805 0.02  0.96  0.25  0.065 0.145\n",
      " 0.105 0.215 0.115 0.175 0.94  0.83  0.945 0.11  0.05  0.08  0.705 0.055\n",
      " 0.21  0.78  0.075 0.785 0.81  0.075 0.16  0.68  0.73  0.275 0.125 0.945\n",
      " 0.89  0.145 0.145 0.2   0.075 0.805 0.86  0.045 0.235 0.06  0.175 0.08\n",
      " 0.225 0.085 0.06  0.795 0.755 0.045 0.175 0.89  0.05  0.065 0.875 0.07\n",
      " 0.07  0.155 0.18  0.955 0.76  0.88  0.035 0.935 0.195 0.78  0.065 0.78\n",
      " 0.11  0.87  0.075 0.88  0.075 0.09  0.96  0.03  0.105 0.055 0.915 0.035\n",
      " 0.115 0.9   0.82  0.125 0.045 0.17  0.99  0.855 0.05  0.025 0.94  0.09\n",
      " 0.92  0.775 0.105 0.8   0.91  0.1   0.735 0.045 0.88  0.93  0.985 0.7\n",
      " 0.14  0.925 0.94  0.895 0.13  0.035 0.06  0.93  0.98  0.81  0.08  0.875\n",
      " 0.115 0.935 0.2   0.045 0.04  0.09  0.965 0.18  0.865 0.815 0.095 0.12\n",
      " 0.095 0.055 0.045 0.08  0.89  0.94  0.82  0.865 0.715 0.845 0.855 0.85\n",
      " 0.93  0.955 0.085 0.175 0.625 0.93  0.95  0.685 0.935 0.065 0.065 0.125\n",
      " 0.155 0.97  0.85  0.875 0.13  0.045 0.275 0.83  0.28  0.77  0.91  0.935\n",
      " 0.07  0.04  0.885 0.885 0.065 0.89  0.205 0.945 0.125 0.675 0.93  0.965\n",
      " 0.185 0.115 0.175 0.98  0.075 0.86  0.025 0.155 0.125 0.8   0.81  0.135\n",
      " 0.685 0.08  0.12  0.03  0.755 0.135 0.91  0.775]\n",
      "[0.89  0.91  0.98  0.905 0.15  0.105 0.77  0.06  0.165 0.75  0.07  0.82\n",
      " 0.965 0.055 0.92  0.895 0.075 0.055 0.05  0.85  0.06  0.155 0.255 0.75\n",
      " 0.85  0.165 0.12  0.955 0.95  0.12  0.87  0.225 0.05  0.805 0.29  0.06\n",
      " 0.955 0.29  0.85  0.97  0.85  0.315 0.03  0.965 0.97  0.065 0.945 0.755\n",
      " 0.915 0.035 0.09  0.875 0.755 0.095 0.17  0.92  0.05  0.04  0.86  0.105\n",
      " 0.055 0.15  0.205 0.995 0.84  0.89  0.025 0.96  0.095 0.86  0.135 0.825\n",
      " 0.14  0.855 0.105 0.9   0.13  0.125 0.955 0.03  0.075 0.04  0.9   0.02\n",
      " 0.135 0.945 0.8   0.19  0.065 0.205 0.955 0.83  0.04  0.045 0.945 0.06\n",
      " 0.955 0.705 0.13  0.885 0.895 0.17  0.755 0.04  0.925 0.945 0.975 0.79\n",
      " 0.13  0.965 0.905 0.94  0.215 0.04  0.045 0.96  0.985 0.905 0.12  0.87\n",
      " 0.115 1.    0.22  0.02  0.06  0.06  0.965 0.22  0.845 0.875 0.07  0.055\n",
      " 0.09  0.1   0.09  0.135 0.95  0.955 0.76  0.88  0.88  0.895 0.835 0.87\n",
      " 0.94  0.925 0.095 0.17  0.715 0.99  0.94  0.68  0.94  0.07  0.07  0.225\n",
      " 0.125 0.965 0.885 0.865 0.11  0.04  0.205 0.85  0.28  0.83  0.94  0.965\n",
      " 0.085 0.025 0.945 0.925 0.07  0.955 0.25  0.955 0.195 0.78  0.95  0.98\n",
      " 0.145 0.105 0.19  0.97  0.045 0.915 0.055 0.17  0.08  0.81  0.795 0.13\n",
      " 0.675 0.145 0.145 0.04  0.87  0.2   0.925 0.795]\n",
      "[0.905 0.92  0.92  0.785 0.135 0.115 0.815 0.045 0.135 0.715 0.07  0.83\n",
      " 0.925 0.105 0.895 0.8   0.08  0.075 0.09  0.88  0.025 0.105 0.315 0.73\n",
      " 0.795 0.08  0.115 0.945 0.935 0.08  0.88  0.155 0.06  0.73  0.125 0.105\n",
      " 0.94  0.245 0.845 0.92  0.81  0.165 0.025 0.94  0.99  0.08  0.955 0.805\n",
      " 0.92  0.03  0.02  0.145 0.695 0.2   0.125 0.16  0.76  0.055 0.965 0.27\n",
      " 0.105 0.16  0.105 0.285 0.165 0.275 0.89  0.915 0.885 0.065 0.06  0.08\n",
      " 0.77  0.045 0.165 0.755 0.08  0.785 0.825 0.15  0.185 0.755 0.765 0.23\n",
      " 0.13  0.91  0.885 0.12  0.235 0.23  0.09  0.825 0.855 0.075 0.225 0.085\n",
      " 0.26  0.115 0.24  0.065 0.91  0.1   0.755 0.025 0.885 0.935 0.965 0.825\n",
      " 0.2   0.925 0.935 0.91  0.15  0.07  0.08  0.94  0.96  0.9   0.18  0.84\n",
      " 0.125 0.965 0.2   0.05  0.06  0.065 0.97  0.215 0.815 0.85  0.075 0.085\n",
      " 0.11  0.095 0.04  0.12  0.88  0.94  0.875 0.89  0.805 0.8   0.85  0.875\n",
      " 0.89  0.945 0.105 0.2   0.685 0.935 0.935 0.77  0.93  0.1   0.06  0.175\n",
      " 0.135 0.97  0.87  0.85  0.085 0.09  0.24  0.78  0.275 0.8   0.91  0.95\n",
      " 0.1   0.065 0.91  0.86  0.1   0.955 0.25  0.94  0.175 0.665 0.915 0.97\n",
      " 0.25  0.105 0.175 0.95  0.085 0.84  0.075 0.19  0.135 0.74  0.83  0.15\n",
      " 0.72  0.135 0.155 0.04  0.74  0.155 0.925 0.82 ]\n",
      "[0.9   0.88  0.92  0.835 0.195 0.05  0.71  0.055 0.155 0.71  0.045 0.795\n",
      " 0.95  0.095 0.915 0.855 0.135 0.06  0.065 0.815 0.04  0.2   0.29  0.74\n",
      " 0.78  0.11  0.125 0.935 0.94  0.045 0.895 0.225 0.035 0.79  0.24  0.14\n",
      " 0.935 0.25  0.84  0.945 0.84  0.225 0.015 0.93  0.95  0.055 0.94  0.76\n",
      " 0.895 0.045 0.025 0.155 0.715 0.21  0.095 0.16  0.725 0.035 0.93  0.295\n",
      " 0.085 0.145 0.05  0.26  0.175 0.195 0.89  0.885 0.91  0.045 0.025 0.065\n",
      " 0.7   0.07  0.16  0.765 0.06  0.725 0.885 0.08  0.17  0.755 0.775 0.25\n",
      " 0.13  0.895 0.855 0.15  0.165 0.27  0.04  0.74  0.845 0.055 0.15  0.075\n",
      " 0.175 0.085 0.305 0.045 0.105 0.785 0.74  0.05  0.175 0.925 0.085 0.025\n",
      " 0.89  0.11  0.045 0.155 0.18  0.98  0.84  0.855 0.035 0.925 0.185 0.85\n",
      " 0.105 0.755 0.14  0.84  0.07  0.865 0.055 0.1   0.935 0.035 0.09  0.015\n",
      " 0.91  0.035 0.155 0.94  0.76  0.195 0.12  0.205 0.94  0.795 0.11  0.02\n",
      " 0.935 0.04  0.885 0.7   0.085 0.925 0.905 0.715 0.945 0.08  0.08  0.185\n",
      " 0.105 0.945 0.85  0.81  0.085 0.07  0.245 0.885 0.235 0.75  0.91  0.96\n",
      " 0.085 0.025 0.94  0.83  0.065 0.9   0.17  0.9   0.155 0.74  0.915 0.975\n",
      " 0.155 0.095 0.135 0.945 0.035 0.86  0.05  0.12  0.105 0.7   0.83  0.085\n",
      " 0.74  0.1   0.16  0.03  0.75  0.21  0.87  0.755]\n",
      "[0.89  0.9   0.935 0.82  0.17  0.12  0.75  0.105 0.125 0.72  0.065 0.82\n",
      " 0.975 0.05  0.925 0.8   0.055 0.055 0.1   0.84  0.07  0.15  0.22  0.8\n",
      " 0.75  0.085 0.135 0.89  0.955 0.065 0.89  0.265 0.05  0.805 0.145 0.09\n",
      " 0.975 0.26  0.865 0.955 0.82  0.24  0.025 0.95  0.955 0.07  0.915 0.795\n",
      " 0.85  0.045 0.06  0.155 0.765 0.21  0.125 0.19  0.735 0.055 0.945 0.3\n",
      " 0.055 0.11  0.115 0.28  0.24  0.23  0.955 0.92  0.94  0.055 0.03  0.065\n",
      " 0.73  0.05  0.135 0.75  0.055 0.775 0.87  0.055 0.185 0.725 0.815 0.255\n",
      " 0.13  0.945 0.89  0.12  0.2   0.195 0.03  0.805 0.895 0.035 0.16  0.08\n",
      " 0.21  0.115 0.24  0.06  0.11  0.87  0.745 0.07  0.255 0.875 0.05  0.045\n",
      " 0.815 0.145 0.04  0.29  0.225 0.955 0.795 0.885 0.03  0.92  0.19  0.855\n",
      " 0.09  0.84  0.165 0.87  0.08  0.895 0.07  0.12  0.92  0.03  0.08  0.035\n",
      " 0.91  0.035 0.135 0.95  0.765 0.17  0.09  0.135 0.955 0.865 0.105 0.05\n",
      " 0.92  0.07  0.905 0.76  0.165 0.895 0.93  0.135 0.795 0.025 0.905 0.9\n",
      " 0.965 0.755 0.12  0.94  0.905 0.925 0.135 0.025 0.08  0.935 0.96  0.86\n",
      " 0.08  0.875 0.13  0.875 0.205 0.04  0.06  0.075 0.905 0.15  0.87  0.84\n",
      " 0.045 0.1   0.115 0.055 0.08  0.125 0.9   0.915 0.81  0.885 0.79  0.835\n",
      " 0.735 0.84  0.895 0.935 0.13  0.185 0.68  0.9  ]\n",
      "[0.04  0.155 0.8   0.195 0.14  0.195 0.775 0.03  0.99  0.305 0.085 0.125\n",
      " 0.08  0.26  0.19  0.19  0.915 0.82  0.925 0.085 0.05  0.09  0.71  0.08\n",
      " 0.165 0.78  0.065 0.83  0.785 0.085 0.13  0.735 0.745 0.23  0.125 0.945\n",
      " 0.81  0.115 0.16  0.2   0.13  0.825 0.86  0.025 0.24  0.05  0.23  0.055\n",
      " 0.17  0.05  0.105 0.805 0.72  0.07  0.195 0.85  0.035 0.03  0.82  0.07\n",
      " 0.09  0.195 0.2   0.96  0.79  0.89  0.065 0.93  0.19  0.82  0.115 0.82\n",
      " 0.115 0.84  0.08  0.895 0.08  0.09  0.98  0.055 0.12  0.05  0.88  0.055\n",
      " 0.11  0.89  0.78  0.115 0.07  0.18  0.97  0.815 0.035 0.1   0.94  0.085\n",
      " 0.875 0.71  0.095 0.895 0.9   0.17  0.765 0.01  0.88  0.925 0.975 0.765\n",
      " 0.135 0.96  0.945 0.865 0.115 0.045 0.04  0.97  0.975 0.835 0.105 0.865\n",
      " 0.16  0.915 0.22  0.035 0.04  0.06  0.955 0.175 0.835 0.86  0.095 0.095\n",
      " 0.065 0.065 0.045 0.18  0.845 0.96  0.855 0.875 0.815 0.8   0.83  0.805\n",
      " 0.925 0.955 0.095 0.17  0.685 0.925 0.955 0.71  0.93  0.055 0.07  0.18\n",
      " 0.125 0.975 0.885 0.855 0.125 0.055 0.185 0.835 0.25  0.775 0.92  0.975\n",
      " 0.095 0.035 0.885 0.92  0.08  0.915 0.175 0.945 0.155 0.725 0.925 0.94\n",
      " 0.185 0.055 0.195 0.955 0.05  0.84  0.08  0.2   0.15  0.755 0.855 0.115\n",
      " 0.745 0.1   0.13  0.03  0.78  0.205 0.875 0.78 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.875 0.935 0.985 0.91  0.155 0.13  0.755 0.05  0.19  0.775 0.05  0.75\n",
      " 0.99  0.1   0.925 0.845 0.095 0.04  0.055 0.89  0.055 0.14  0.325 0.835\n",
      " 0.86  0.135 0.095 0.97  0.945 0.07  0.85  0.135 0.045 0.785 0.235 0.095\n",
      " 0.985 0.34  0.835 0.96  0.835 0.225 0.015 0.95  0.96  0.045 0.965 0.755\n",
      " 0.91  0.06  0.07  0.885 0.795 0.09  0.16  0.94  0.015 0.035 0.89  0.135\n",
      " 0.095 0.18  0.24  0.985 0.855 0.93  0.015 0.98  0.115 0.855 0.14  0.83\n",
      " 0.175 0.89  0.075 0.905 0.105 0.105 0.98  0.04  0.1   0.025 0.905 0.01\n",
      " 0.15  0.98  0.78  0.225 0.03  0.175 0.965 0.8   0.065 0.035 0.955 0.055\n",
      " 0.965 0.785 0.095 0.92  0.94  0.22  0.84  0.02  0.945 0.99  0.975 0.78\n",
      " 0.09  0.975 0.92  0.91  0.185 0.045 0.03  0.97  0.985 0.885 0.175 0.945\n",
      " 0.105 0.98  0.315 0.03  0.085 0.055 0.965 0.19  0.895 0.875 0.065 0.07\n",
      " 0.09  0.05  0.09  0.12  0.93  0.94  0.735 0.83  0.85  0.87  0.835 0.855\n",
      " 0.955 0.96  0.07  0.195 0.715 0.985 0.965 0.705 0.965 0.115 0.035 0.17\n",
      " 0.15  0.98  0.915 0.865 0.09  0.035 0.125 0.86  0.28  0.835 0.96  0.99\n",
      " 0.095 0.04  0.97  0.935 0.09  0.945 0.215 0.98  0.19  0.73  0.965 0.98\n",
      " 0.21  0.13  0.165 0.955 0.05  0.945 0.045 0.205 0.09  0.79  0.775 0.11\n",
      " 0.71  0.135 0.17  0.03  0.9   0.22  0.92  0.795]\n",
      "[0.915 0.885 0.93  0.835 0.14  0.135 0.745 0.045 0.175 0.705 0.075 0.8\n",
      " 0.955 0.12  0.915 0.835 0.11  0.07  0.065 0.825 0.07  0.17  0.275 0.785\n",
      " 0.815 0.125 0.125 0.95  0.96  0.095 0.895 0.215 0.04  0.805 0.19  0.09\n",
      " 0.95  0.205 0.83  0.93  0.87  0.285 0.035 0.955 0.955 0.095 0.97  0.77\n",
      " 0.93  0.04  0.035 0.135 0.715 0.215 0.1   0.235 0.725 0.05  0.965 0.34\n",
      " 0.085 0.125 0.125 0.22  0.21  0.225 0.91  0.925 0.89  0.05  0.03  0.085\n",
      " 0.73  0.035 0.16  0.745 0.065 0.81  0.865 0.075 0.145 0.735 0.76  0.28\n",
      " 0.175 0.89  0.85  0.16  0.225 0.23  0.04  0.81  0.88  0.065 0.215 0.09\n",
      " 0.255 0.1   0.27  0.13  0.885 0.135 0.76  0.025 0.885 0.945 0.955 0.73\n",
      " 0.15  0.955 0.925 0.93  0.07  0.06  0.075 0.94  0.95  0.82  0.15  0.855\n",
      " 0.175 0.97  0.225 0.06  0.065 0.085 0.94  0.23  0.85  0.78  0.12  0.095\n",
      " 0.08  0.085 0.1   0.105 0.92  0.925 0.775 0.91  0.79  0.78  0.76  0.815\n",
      " 0.915 0.95  0.12  0.16  0.665 0.96  0.9   0.725 0.915 0.09  0.095 0.185\n",
      " 0.1   0.96  0.88  0.9   0.11  0.04  0.225 0.82  0.275 0.855 0.925 0.97\n",
      " 0.1   0.02  0.945 0.865 0.115 0.95  0.19  0.955 0.19  0.665 0.95  0.95\n",
      " 0.135 0.07  0.21  0.98  0.05  0.86  0.055 0.225 0.175 0.77  0.8   0.175\n",
      " 0.7   0.115 0.17  0.035 0.73  0.21  0.94  0.805]\n",
      "[0.885 0.945 0.905 0.77  0.215 0.055 0.765 0.05  0.19  0.71  0.045 0.845\n",
      " 0.98  0.04  0.875 0.92  0.135 0.05  0.045 0.875 0.035 0.15  0.29  0.745\n",
      " 0.87  0.065 0.13  0.93  0.95  0.09  0.855 0.215 0.015 0.78  0.205 0.1\n",
      " 0.935 0.29  0.79  0.93  0.82  0.245 0.04  0.945 0.915 0.055 0.93  0.8\n",
      " 0.875 0.035 0.01  0.17  0.755 0.24  0.06  0.22  0.74  0.05  0.93  0.22\n",
      " 0.085 0.085 0.105 0.26  0.215 0.195 0.905 0.865 0.89  0.08  0.025 0.09\n",
      " 0.715 0.075 0.18  0.755 0.105 0.79  0.875 0.115 0.15  0.77  0.735 0.25\n",
      " 0.1   0.92  0.88  0.11  0.205 0.25  0.06  0.795 0.835 0.09  0.235 0.07\n",
      " 0.25  0.07  0.225 0.06  0.085 0.76  0.735 0.045 0.215 0.89  0.06  0.03\n",
      " 0.87  0.115 0.085 0.165 0.205 0.94  0.83  0.895 0.015 0.93  0.16  0.86\n",
      " 0.14  0.765 0.12  0.84  0.08  0.87  0.095 0.14  0.955 0.02  0.16  0.025\n",
      " 0.885 0.035 0.115 0.9   0.81  0.135 0.1   0.145 0.965 0.83  0.08  0.06\n",
      " 0.945 0.07  0.93  0.81  0.125 0.915 0.905 0.705 0.945 0.07  0.07  0.195\n",
      " 0.11  0.915 0.925 0.87  0.095 0.025 0.275 0.835 0.255 0.84  0.9   0.975\n",
      " 0.06  0.015 0.95  0.875 0.085 0.93  0.195 0.95  0.2   0.715 0.965 0.975\n",
      " 0.145 0.07  0.2   0.965 0.045 0.86  0.05  0.12  0.155 0.745 0.81  0.1\n",
      " 0.78  0.135 0.175 0.03  0.76  0.175 0.89  0.785]\n",
      "[0.885 0.915 0.945 0.815 0.15  0.07  0.72  0.06  0.195 0.735 0.095 0.815\n",
      " 0.975 0.065 0.96  0.855 0.09  0.065 0.085 0.83  0.045 0.165 0.27  0.75\n",
      " 0.8   0.085 0.14  0.945 0.975 0.055 0.905 0.22  0.035 0.765 0.195 0.055\n",
      " 0.95  0.205 0.85  0.945 0.845 0.205 0.03  0.96  0.945 0.06  0.935 0.805\n",
      " 0.855 0.025 0.005 0.16  0.765 0.21  0.15  0.135 0.725 0.04  0.955 0.28\n",
      " 0.065 0.165 0.115 0.255 0.215 0.28  0.895 0.915 0.93  0.1   0.03  0.065\n",
      " 0.735 0.045 0.16  0.795 0.08  0.765 0.89  0.115 0.175 0.755 0.775 0.23\n",
      " 0.14  0.91  0.84  0.085 0.155 0.235 0.065 0.79  0.835 0.04  0.16  0.11\n",
      " 0.275 0.065 0.265 0.1   0.155 0.82  0.75  0.075 0.235 0.845 0.08  0.035\n",
      " 0.895 0.085 0.08  0.17  0.205 0.965 0.83  0.905 0.04  0.905 0.205 0.83\n",
      " 0.145 0.825 0.155 0.845 0.12  0.885 0.1   0.105 0.925 0.045 0.16  0.045\n",
      " 0.9   0.025 0.08  0.95  0.745 0.165 0.06  0.13  0.945 0.82  0.1   0.055\n",
      " 0.945 0.095 0.95  0.79  0.125 0.92  0.875 0.13  0.76  0.03  0.91  0.91\n",
      " 0.965 0.785 0.19  0.97  0.91  0.935 0.105 0.05  0.04  0.95  0.94  0.83\n",
      " 0.095 0.885 0.13  0.93  0.24  0.03  0.045 0.07  0.955 0.145 0.885 0.82\n",
      " 0.075 0.075 0.07  0.095 0.085 0.12  0.925 0.92  0.88  0.91  0.775 0.845\n",
      " 0.765 0.825 0.94  0.95  0.125 0.22  0.685 0.945]\n",
      "[0.07  0.225 0.705 0.19  0.115 0.22  0.77  0.045 0.96  0.275 0.055 0.115\n",
      " 0.1   0.325 0.125 0.255 0.88  0.87  0.935 0.055 0.045 0.08  0.725 0.075\n",
      " 0.175 0.795 0.065 0.755 0.85  0.085 0.18  0.835 0.76  0.23  0.115 0.945\n",
      " 0.825 0.14  0.145 0.21  0.055 0.825 0.83  0.065 0.23  0.085 0.17  0.095\n",
      " 0.225 0.085 0.075 0.845 0.75  0.05  0.175 0.92  0.05  0.025 0.87  0.07\n",
      " 0.055 0.175 0.16  0.965 0.81  0.89  0.06  0.935 0.18  0.88  0.115 0.72\n",
      " 0.075 0.84  0.06  0.865 0.06  0.09  0.945 0.055 0.1   0.015 0.875 0.065\n",
      " 0.075 0.895 0.725 0.15  0.05  0.125 0.985 0.815 0.04  0.03  0.91  0.08\n",
      " 0.895 0.765 0.07  0.905 0.895 0.135 0.755 0.045 0.845 0.925 0.955 0.76\n",
      " 0.165 0.955 0.935 0.925 0.155 0.06  0.075 0.925 0.995 0.83  0.115 0.85\n",
      " 0.13  0.97  0.2   0.035 0.09  0.105 0.965 0.205 0.82  0.885 0.085 0.115\n",
      " 0.09  0.095 0.065 0.145 0.88  0.95  0.785 0.87  0.77  0.8   0.835 0.845\n",
      " 0.93  0.94  0.1   0.135 0.7   0.95  0.94  0.705 0.95  0.065 0.05  0.2\n",
      " 0.125 0.97  0.895 0.865 0.08  0.05  0.24  0.85  0.26  0.785 0.89  0.98\n",
      " 0.095 0.05  0.935 0.955 0.095 0.94  0.22  0.965 0.225 0.72  0.95  0.965\n",
      " 0.145 0.07  0.145 0.95  0.05  0.85  0.035 0.165 0.085 0.73  0.79  0.12\n",
      " 0.71  0.12  0.095 0.06  0.745 0.165 0.855 0.81 ]\n",
      "[0.91  0.915 0.955 0.875 0.16  0.085 0.755 0.07  0.16  0.78  0.055 0.81\n",
      " 0.96  0.095 0.89  0.865 0.09  0.045 0.06  0.875 0.055 0.155 0.305 0.74\n",
      " 0.86  0.17  0.12  0.935 0.96  0.06  0.85  0.165 0.045 0.835 0.19  0.085\n",
      " 0.985 0.27  0.795 0.965 0.87  0.255 0.015 0.97  0.97  0.095 0.97  0.79\n",
      " 0.94  0.045 0.075 0.855 0.82  0.08  0.16  0.94  0.045 0.06  0.875 0.15\n",
      " 0.055 0.215 0.23  0.995 0.845 0.94  0.015 0.97  0.18  0.82  0.16  0.775\n",
      " 0.16  0.905 0.075 0.895 0.105 0.085 0.96  0.045 0.075 0.05  0.93  0.015\n",
      " 0.09  0.975 0.765 0.18  0.05  0.21  0.95  0.78  0.06  0.055 0.96  0.06\n",
      " 0.95  0.775 0.09  0.93  0.935 0.26  0.815 0.04  0.91  0.955 0.995 0.72\n",
      " 0.15  0.96  0.94  0.92  0.17  0.04  0.045 0.965 0.985 0.885 0.11  0.91\n",
      " 0.095 0.985 0.21  0.04  0.045 0.1   0.94  0.2   0.865 0.89  0.075 0.075\n",
      " 0.07  0.08  0.105 0.13  0.975 0.955 0.795 0.81  0.83  0.81  0.79  0.895\n",
      " 0.945 0.95  0.065 0.21  0.635 0.97  0.955 0.63  0.93  0.085 0.055 0.195\n",
      " 0.125 0.975 0.885 0.89  0.095 0.025 0.22  0.85  0.26  0.89  0.945 0.97\n",
      " 0.065 0.015 0.96  0.92  0.085 0.965 0.255 0.985 0.2   0.75  0.955 0.95\n",
      " 0.21  0.135 0.175 0.935 0.04  0.945 0.055 0.195 0.09  0.79  0.83  0.1\n",
      " 0.74  0.145 0.22  0.025 0.89  0.165 0.95  0.815]\n",
      "[0.86  0.885 0.895 0.85  0.11  0.115 0.735 0.06  0.165 0.67  0.125 0.835\n",
      " 0.97  0.105 0.9   0.805 0.09  0.095 0.085 0.865 0.03  0.155 0.24  0.755\n",
      " 0.8   0.07  0.16  0.945 0.995 0.075 0.9   0.25  0.055 0.76  0.185 0.165\n",
      " 0.945 0.24  0.78  0.94  0.765 0.225 0.035 0.945 0.945 0.06  0.93  0.78\n",
      " 0.89  0.035 0.025 0.24  0.71  0.18  0.135 0.19  0.73  0.02  0.985 0.3\n",
      " 0.08  0.1   0.1   0.24  0.145 0.265 0.87  0.925 0.89  0.06  0.04  0.075\n",
      " 0.7   0.05  0.2   0.815 0.125 0.825 0.845 0.09  0.14  0.745 0.75  0.335\n",
      " 0.16  0.93  0.855 0.09  0.225 0.205 0.11  0.84  0.87  0.055 0.195 0.085\n",
      " 0.19  0.135 0.295 0.095 0.925 0.13  0.78  0.035 0.875 0.94  0.965 0.71\n",
      " 0.12  0.955 0.96  0.925 0.145 0.045 0.055 0.885 0.95  0.88  0.18  0.845\n",
      " 0.14  0.935 0.235 0.05  0.05  0.07  0.95  0.245 0.825 0.82  0.1   0.075\n",
      " 0.1   0.04  0.085 0.11  0.91  0.905 0.825 0.875 0.8   0.815 0.815 0.825\n",
      " 0.905 0.97  0.125 0.24  0.67  0.945 0.945 0.76  0.96  0.1   0.075 0.19\n",
      " 0.165 0.95  0.87  0.855 0.075 0.05  0.255 0.815 0.255 0.83  0.89  0.97\n",
      " 0.095 0.035 0.92  0.89  0.095 0.935 0.225 0.965 0.18  0.7   0.93  0.94\n",
      " 0.16  0.11  0.14  0.97  0.08  0.805 0.035 0.145 0.155 0.755 0.75  0.105\n",
      " 0.83  0.125 0.145 0.075 0.765 0.23  0.905 0.76 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.905 0.9   0.93  0.855 0.195 0.05  0.72  0.035 0.115 0.725 0.04  0.855\n",
      " 0.945 0.095 0.865 0.8   0.135 0.035 0.05  0.79  0.02  0.2   0.26  0.72\n",
      " 0.855 0.1   0.13  0.955 0.925 0.085 0.86  0.195 0.02  0.775 0.245 0.1\n",
      " 0.945 0.235 0.745 0.96  0.8   0.28  0.02  0.94  0.93  0.05  0.95  0.82\n",
      " 0.915 0.045 0.025 0.13  0.715 0.195 0.08  0.22  0.71  0.015 0.945 0.28\n",
      " 0.04  0.115 0.115 0.23  0.18  0.225 0.91  0.905 0.91  0.07  0.05  0.055\n",
      " 0.725 0.06  0.19  0.785 0.07  0.81  0.87  0.095 0.18  0.765 0.765 0.26\n",
      " 0.105 0.92  0.88  0.2   0.17  0.26  0.065 0.84  0.865 0.075 0.205 0.055\n",
      " 0.235 0.055 0.245 0.08  0.04  0.825 0.765 0.05  0.205 0.88  0.065 0.035\n",
      " 0.87  0.095 0.05  0.22  0.17  0.93  0.87  0.845 0.025 0.875 0.24  0.825\n",
      " 0.09  0.735 0.105 0.78  0.1   0.84  0.085 0.115 0.935 0.045 0.145 0.03\n",
      " 0.89  0.04  0.12  0.94  0.785 0.155 0.07  0.17  0.975 0.785 0.105 0.045\n",
      " 0.91  0.06  0.87  0.84  0.115 0.855 0.93  0.73  0.935 0.08  0.075 0.18\n",
      " 0.15  0.935 0.865 0.88  0.09  0.045 0.275 0.83  0.27  0.805 0.915 0.965\n",
      " 0.09  0.005 0.9   0.775 0.095 0.915 0.155 0.935 0.14  0.745 0.965 0.98\n",
      " 0.15  0.115 0.17  0.955 0.045 0.865 0.03  0.12  0.115 0.75  0.78  0.075\n",
      " 0.745 0.175 0.145 0.075 0.735 0.22  0.85  0.775]\n",
      "[0.895 0.93  0.94  0.835 0.155 0.115 0.78  0.05  0.195 0.75  0.045 0.81\n",
      " 0.955 0.08  0.91  0.81  0.08  0.055 0.135 0.82  0.015 0.195 0.265 0.785\n",
      " 0.755 0.07  0.115 0.935 0.96  0.095 0.885 0.235 0.025 0.76  0.16  0.05\n",
      " 0.965 0.27  0.86  0.93  0.885 0.28  0.04  0.975 0.955 0.055 0.95  0.805\n",
      " 0.915 0.045 0.025 0.17  0.745 0.285 0.1   0.185 0.705 0.035 0.95  0.275\n",
      " 0.065 0.105 0.11  0.195 0.185 0.195 0.945 0.9   0.96  0.06  0.035 0.12\n",
      " 0.72  0.04  0.205 0.75  0.1   0.815 0.895 0.055 0.175 0.71  0.745 0.28\n",
      " 0.15  0.95  0.88  0.135 0.2   0.26  0.065 0.835 0.875 0.035 0.195 0.08\n",
      " 0.24  0.095 0.315 0.065 0.07  0.88  0.79  0.07  0.245 0.88  0.065 0.03\n",
      " 0.845 0.085 0.085 0.23  0.225 0.98  0.755 0.875 0.03  0.93  0.16  0.835\n",
      " 0.1   0.81  0.085 0.905 0.085 0.895 0.08  0.09  0.93  0.035 0.13  0.025\n",
      " 0.915 0.035 0.115 0.955 0.785 0.175 0.125 0.155 0.94  0.795 0.06  0.04\n",
      " 0.9   0.1   0.94  0.8   0.15  0.875 0.915 0.14  0.815 0.04  0.85  0.965\n",
      " 0.98  0.81  0.165 0.94  0.9   0.925 0.145 0.02  0.04  0.93  0.955 0.89\n",
      " 0.11  0.895 0.105 0.935 0.205 0.04  0.065 0.135 0.915 0.115 0.9   0.85\n",
      " 0.08  0.095 0.09  0.09  0.105 0.13  0.92  0.925 0.83  0.9   0.805 0.815\n",
      " 0.775 0.84  0.93  0.945 0.085 0.17  0.685 0.885]\n",
      "[0.07  0.205 0.72  0.19  0.125 0.18  0.765 0.035 0.965 0.29  0.095 0.12\n",
      " 0.09  0.29  0.165 0.16  0.955 0.895 0.95  0.075 0.04  0.1   0.715 0.045\n",
      " 0.255 0.785 0.07  0.81  0.8   0.09  0.155 0.735 0.745 0.23  0.13  0.925\n",
      " 0.85  0.115 0.175 0.25  0.085 0.805 0.835 0.03  0.23  0.055 0.165 0.115\n",
      " 0.21  0.055 0.075 0.805 0.745 0.07  0.15  0.9   0.045 0.035 0.855 0.09\n",
      " 0.055 0.185 0.235 0.935 0.82  0.89  0.05  0.94  0.25  0.815 0.08  0.78\n",
      " 0.11  0.87  0.08  0.91  0.04  0.07  0.955 0.065 0.105 0.025 0.87  0.06\n",
      " 0.1   0.925 0.78  0.15  0.08  0.16  0.985 0.835 0.07  0.015 0.925 0.04\n",
      " 0.925 0.76  0.09  0.895 0.865 0.165 0.7   0.015 0.84  0.915 0.965 0.7\n",
      " 0.135 0.97  0.96  0.94  0.1   0.05  0.065 0.89  0.98  0.835 0.115 0.86\n",
      " 0.15  0.93  0.19  0.025 0.035 0.08  0.935 0.24  0.825 0.875 0.09  0.065\n",
      " 0.09  0.08  0.095 0.12  0.895 0.945 0.765 0.87  0.785 0.81  0.86  0.865\n",
      " 0.905 0.945 0.065 0.16  0.655 0.945 0.955 0.78  0.925 0.085 0.06  0.145\n",
      " 0.14  0.99  0.87  0.88  0.105 0.045 0.205 0.83  0.355 0.825 0.875 0.975\n",
      " 0.08  0.05  0.935 0.875 0.065 0.92  0.16  0.935 0.155 0.695 0.97  0.96\n",
      " 0.15  0.125 0.18  0.975 0.03  0.915 0.04  0.125 0.105 0.77  0.785 0.105\n",
      " 0.71  0.135 0.17  0.025 0.76  0.14  0.88  0.78 ]\n",
      "[0.88  0.945 0.965 0.88  0.165 0.125 0.8   0.02  0.245 0.77  0.06  0.815\n",
      " 0.975 0.09  0.925 0.895 0.085 0.04  0.065 0.86  0.045 0.18  0.25  0.82\n",
      " 0.85  0.13  0.115 0.97  0.98  0.085 0.925 0.24  0.04  0.775 0.28  0.045\n",
      " 0.99  0.25  0.855 0.97  0.87  0.22  0.015 0.98  0.965 0.055 0.99  0.78\n",
      " 0.935 0.04  0.075 0.83  0.74  0.07  0.215 0.915 0.025 0.065 0.855 0.13\n",
      " 0.06  0.175 0.21  0.985 0.81  0.95  0.015 0.95  0.185 0.86  0.11  0.8\n",
      " 0.18  0.88  0.1   0.895 0.135 0.1   0.96  0.05  0.11  0.025 0.875 0.015\n",
      " 0.095 0.95  0.78  0.23  0.045 0.17  0.965 0.81  0.055 0.055 0.93  0.035\n",
      " 0.945 0.77  0.1   0.885 0.9   0.205 0.825 0.02  0.945 0.96  0.985 0.74\n",
      " 0.125 0.965 0.97  0.95  0.17  0.055 0.05  0.97  0.985 0.865 0.165 0.915\n",
      " 0.16  0.98  0.255 0.07  0.055 0.035 0.965 0.2   0.865 0.92  0.07  0.055\n",
      " 0.11  0.105 0.085 0.105 0.95  0.955 0.715 0.885 0.885 0.815 0.835 0.865\n",
      " 0.95  0.965 0.1   0.245 0.64  0.98  0.945 0.75  0.965 0.095 0.03  0.155\n",
      " 0.155 0.99  0.895 0.805 0.095 0.01  0.17  0.86  0.255 0.79  0.94  0.98\n",
      " 0.09  0.02  0.935 0.925 0.1   0.96  0.275 0.965 0.21  0.785 0.985 0.98\n",
      " 0.16  0.11  0.205 0.98  0.035 0.93  0.045 0.19  0.135 0.805 0.78  0.175\n",
      " 0.72  0.175 0.165 0.05  0.89  0.17  0.935 0.855]\n",
      "[0.905 0.895 0.89  0.825 0.17  0.085 0.745 0.055 0.155 0.685 0.03  0.87\n",
      " 0.96  0.09  0.895 0.805 0.085 0.11  0.05  0.85  0.055 0.135 0.25  0.765\n",
      " 0.805 0.15  0.135 0.94  0.945 0.05  0.89  0.215 0.045 0.765 0.155 0.085\n",
      " 0.955 0.2   0.855 0.965 0.78  0.28  0.045 0.97  0.965 0.075 0.95  0.755\n",
      " 0.885 0.03  0.015 0.16  0.765 0.18  0.105 0.185 0.75  0.03  0.965 0.34\n",
      " 0.06  0.105 0.075 0.225 0.23  0.235 0.87  0.95  0.89  0.04  0.065 0.055\n",
      " 0.695 0.035 0.2   0.78  0.08  0.815 0.92  0.1   0.16  0.78  0.78  0.245\n",
      " 0.13  0.91  0.84  0.14  0.23  0.21  0.065 0.81  0.875 0.065 0.18  0.1\n",
      " 0.2   0.12  0.28  0.075 0.92  0.155 0.77  0.02  0.91  0.98  0.975 0.705\n",
      " 0.11  0.94  0.92  0.94  0.18  0.05  0.09  0.895 0.98  0.83  0.195 0.915\n",
      " 0.11  0.96  0.2   0.085 0.1   0.06  0.95  0.205 0.845 0.815 0.085 0.105\n",
      " 0.065 0.065 0.07  0.125 0.865 0.895 0.83  0.91  0.785 0.81  0.805 0.78\n",
      " 0.895 0.93  0.095 0.24  0.7   0.905 0.92  0.695 0.94  0.07  0.06  0.16\n",
      " 0.18  0.975 0.865 0.825 0.11  0.045 0.305 0.775 0.285 0.86  0.855 0.965\n",
      " 0.1   0.03  0.91  0.87  0.09  0.95  0.195 0.94  0.155 0.715 0.975 0.96\n",
      " 0.16  0.105 0.21  0.96  0.05  0.84  0.035 0.17  0.14  0.78  0.77  0.105\n",
      " 0.79  0.125 0.2   0.02  0.78  0.24  0.925 0.77 ]\n",
      "[0.855 0.905 0.96  0.815 0.235 0.06  0.745 0.06  0.195 0.75  0.085 0.795\n",
      " 0.965 0.105 0.89  0.84  0.125 0.07  0.08  0.91  0.045 0.155 0.275 0.715\n",
      " 0.8   0.04  0.12  0.92  0.95  0.08  0.85  0.155 0.045 0.83  0.22  0.11\n",
      " 0.95  0.28  0.79  0.94  0.84  0.215 0.06  0.955 0.96  0.07  0.95  0.74\n",
      " 0.855 0.025 0.02  0.155 0.715 0.205 0.1   0.155 0.73  0.06  0.955 0.245\n",
      " 0.065 0.115 0.075 0.215 0.18  0.185 0.925 0.9   0.895 0.07  0.05  0.075\n",
      " 0.72  0.075 0.175 0.755 0.095 0.765 0.86  0.07  0.13  0.735 0.785 0.25\n",
      " 0.155 0.9   0.82  0.115 0.17  0.205 0.085 0.81  0.84  0.065 0.165 0.035\n",
      " 0.22  0.075 0.245 0.045 0.095 0.81  0.68  0.06  0.17  0.89  0.04  0.045\n",
      " 0.875 0.13  0.04  0.115 0.21  0.96  0.835 0.885 0.03  0.895 0.19  0.84\n",
      " 0.08  0.78  0.165 0.84  0.135 0.9   0.11  0.095 0.965 0.02  0.105 0.065\n",
      " 0.895 0.025 0.14  0.925 0.795 0.2   0.115 0.195 0.97  0.8   0.105 0.055\n",
      " 0.935 0.085 0.925 0.795 0.095 0.895 0.94  0.715 0.93  0.06  0.045 0.16\n",
      " 0.095 0.93  0.885 0.84  0.11  0.02  0.265 0.805 0.24  0.795 0.89  0.97\n",
      " 0.08  0.015 0.91  0.84  0.08  0.935 0.195 0.95  0.145 0.72  0.96  0.98\n",
      " 0.16  0.1   0.16  0.935 0.065 0.85  0.08  0.12  0.12  0.765 0.73  0.095\n",
      " 0.77  0.165 0.18  0.05  0.8   0.18  0.91  0.765]\n",
      "[0.91  0.875 0.93  0.83  0.155 0.1   0.705 0.06  0.155 0.74  0.065 0.805\n",
      " 0.96  0.045 0.935 0.78  0.055 0.045 0.09  0.85  0.05  0.115 0.245 0.74\n",
      " 0.795 0.11  0.115 0.925 0.955 0.07  0.885 0.24  0.085 0.725 0.11  0.02\n",
      " 0.955 0.21  0.885 0.905 0.83  0.29  0.035 0.94  0.98  0.08  0.925 0.755\n",
      " 0.92  0.035 0.055 0.19  0.755 0.24  0.075 0.195 0.71  0.04  0.975 0.29\n",
      " 0.055 0.09  0.075 0.22  0.16  0.205 0.935 0.91  0.95  0.06  0.025 0.07\n",
      " 0.74  0.08  0.16  0.79  0.08  0.775 0.85  0.07  0.15  0.775 0.765 0.255\n",
      " 0.11  0.92  0.86  0.085 0.15  0.205 0.06  0.79  0.87  0.065 0.215 0.12\n",
      " 0.18  0.1   0.26  0.08  0.09  0.76  0.7   0.065 0.195 0.89  0.08  0.015\n",
      " 0.87  0.135 0.065 0.235 0.24  0.975 0.795 0.875 0.015 0.92  0.25  0.82\n",
      " 0.1   0.82  0.13  0.835 0.065 0.905 0.105 0.125 0.93  0.045 0.14  0.045\n",
      " 0.89  0.02  0.115 0.965 0.785 0.14  0.065 0.115 0.99  0.765 0.085 0.065\n",
      " 0.91  0.06  0.935 0.7   0.075 0.915 0.865 0.135 0.785 0.01  0.89  0.985\n",
      " 0.975 0.755 0.125 0.965 0.89  0.9   0.13  0.04  0.035 0.935 0.98  0.865\n",
      " 0.12  0.88  0.12  0.925 0.29  0.03  0.035 0.055 0.935 0.125 0.885 0.845\n",
      " 0.06  0.05  0.095 0.035 0.11  0.155 0.915 0.915 0.81  0.915 0.75  0.785\n",
      " 0.8   0.805 0.915 0.94  0.065 0.165 0.655 0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.035 0.19  0.76  0.2   0.07  0.185 0.755 0.045 0.955 0.28  0.05  0.135\n",
      " 0.075 0.295 0.115 0.22  0.95  0.86  0.94  0.07  0.065 0.125 0.79  0.045\n",
      " 0.175 0.76  0.055 0.78  0.845 0.05  0.125 0.72  0.82  0.25  0.135 0.955\n",
      " 0.87  0.115 0.135 0.22  0.07  0.845 0.82  0.06  0.25  0.09  0.19  0.085\n",
      " 0.2   0.055 0.1   0.86  0.75  0.04  0.215 0.89  0.065 0.02  0.86  0.09\n",
      " 0.07  0.205 0.21  0.96  0.85  0.9   0.03  0.93  0.215 0.81  0.075 0.77\n",
      " 0.12  0.89  0.08  0.89  0.12  0.13  0.98  0.04  0.15  0.04  0.92  0.07\n",
      " 0.085 0.945 0.765 0.16  0.065 0.095 0.975 0.87  0.06  0.085 0.95  0.04\n",
      " 0.935 0.73  0.09  0.86  0.925 0.095 0.745 0.01  0.855 0.95  0.965 0.745\n",
      " 0.115 0.965 0.95  0.895 0.145 0.025 0.055 0.955 0.98  0.845 0.065 0.88\n",
      " 0.15  0.98  0.22  0.01  0.055 0.12  0.96  0.23  0.86  0.805 0.105 0.105\n",
      " 0.05  0.075 0.075 0.1   0.86  0.945 0.795 0.89  0.805 0.835 0.795 0.86\n",
      " 0.9   0.92  0.095 0.205 0.66  0.925 0.96  0.735 0.915 0.11  0.06  0.175\n",
      " 0.105 0.98  0.835 0.845 0.08  0.065 0.29  0.84  0.295 0.73  0.89  0.97\n",
      " 0.09  0.03  0.905 0.915 0.1   0.905 0.21  0.965 0.13  0.78  0.92  0.955\n",
      " 0.15  0.095 0.225 0.93  0.06  0.805 0.025 0.18  0.11  0.815 0.755 0.085\n",
      " 0.73  0.075 0.105 0.045 0.765 0.145 0.85  0.815]\n",
      "[0.895 0.915 0.97  0.875 0.2   0.06  0.825 0.02  0.145 0.725 0.035 0.775\n",
      " 0.99  0.08  0.91  0.855 0.095 0.03  0.11  0.88  0.04  0.215 0.315 0.76\n",
      " 0.885 0.09  0.14  0.965 0.965 0.075 0.885 0.165 0.03  0.78  0.24  0.105\n",
      " 0.97  0.255 0.78  0.93  0.865 0.2   0.025 0.975 0.94  0.03  0.965 0.72\n",
      " 0.935 0.05  0.065 0.825 0.75  0.065 0.2   0.945 0.03  0.035 0.89  0.115\n",
      " 0.06  0.135 0.23  0.995 0.875 0.92  0.015 0.96  0.175 0.865 0.125 0.765\n",
      " 0.22  0.915 0.095 0.915 0.12  0.095 0.975 0.06  0.055 0.02  0.9   0.04\n",
      " 0.095 0.94  0.745 0.185 0.075 0.185 0.96  0.79  0.035 0.035 0.97  0.035\n",
      " 0.945 0.74  0.1   0.915 0.9   0.215 0.8   0.03  0.935 0.955 0.98  0.73\n",
      " 0.195 0.975 0.94  0.88  0.205 0.075 0.04  0.935 1.    0.87  0.12  0.88\n",
      " 0.1   0.975 0.245 0.03  0.055 0.115 0.96  0.195 0.87  0.925 0.055 0.065\n",
      " 0.075 0.115 0.105 0.135 0.955 0.96  0.735 0.825 0.835 0.84  0.825 0.86\n",
      " 0.94  0.955 0.045 0.19  0.63  0.985 0.95  0.745 0.955 0.07  0.05  0.145\n",
      " 0.19  0.97  0.9   0.84  0.115 0.04  0.185 0.9   0.275 0.82  0.975 0.995\n",
      " 0.085 0.03  0.97  0.935 0.1   0.97  0.25  0.995 0.145 0.66  0.945 0.96\n",
      " 0.155 0.12  0.185 0.975 0.04  0.95  0.045 0.135 0.095 0.755 0.815 0.175\n",
      " 0.79  0.085 0.19  0.03  0.87  0.145 0.94  0.775]\n",
      "[0.88  0.85  0.875 0.805 0.17  0.115 0.785 0.09  0.12  0.735 0.045 0.8\n",
      " 0.955 0.11  0.91  0.825 0.105 0.075 0.075 0.885 0.055 0.145 0.19  0.805\n",
      " 0.8   0.13  0.14  0.955 0.975 0.05  0.91  0.255 0.055 0.79  0.155 0.08\n",
      " 0.955 0.25  0.82  0.94  0.8   0.195 0.025 0.94  0.96  0.08  0.925 0.76\n",
      " 0.895 0.06  0.01  0.175 0.675 0.195 0.105 0.19  0.725 0.03  0.955 0.27\n",
      " 0.075 0.14  0.125 0.275 0.18  0.19  0.86  0.93  0.915 0.085 0.05  0.065\n",
      " 0.715 0.065 0.165 0.795 0.07  0.765 0.925 0.12  0.185 0.745 0.755 0.275\n",
      " 0.14  0.94  0.88  0.105 0.225 0.255 0.055 0.77  0.845 0.06  0.185 0.08\n",
      " 0.16  0.1   0.205 0.095 0.855 0.09  0.765 0.025 0.88  0.965 0.955 0.755\n",
      " 0.14  0.97  0.925 0.915 0.155 0.08  0.075 0.93  0.98  0.845 0.135 0.89\n",
      " 0.11  0.98  0.27  0.08  0.055 0.075 0.97  0.185 0.83  0.825 0.1   0.08\n",
      " 0.095 0.055 0.07  0.165 0.905 0.905 0.79  0.89  0.755 0.765 0.775 0.845\n",
      " 0.905 0.955 0.1   0.19  0.635 0.915 0.905 0.705 0.935 0.15  0.06  0.165\n",
      " 0.15  0.955 0.81  0.82  0.1   0.06  0.285 0.78  0.24  0.825 0.905 0.99\n",
      " 0.105 0.04  0.915 0.855 0.13  0.96  0.175 0.955 0.195 0.735 0.95  0.975\n",
      " 0.21  0.105 0.165 0.985 0.08  0.855 0.02  0.18  0.125 0.805 0.795 0.145\n",
      " 0.79  0.115 0.175 0.07  0.725 0.145 0.9   0.805]\n",
      "[0.89  0.945 0.95  0.82  0.135 0.065 0.73  0.055 0.14  0.76  0.035 0.83\n",
      " 0.965 0.085 0.89  0.865 0.12  0.055 0.06  0.85  0.03  0.185 0.21  0.725\n",
      " 0.805 0.07  0.145 0.945 0.925 0.06  0.88  0.185 0.03  0.835 0.17  0.105\n",
      " 0.95  0.27  0.77  0.945 0.815 0.235 0.03  0.95  0.955 0.09  0.935 0.71\n",
      " 0.88  0.05  0.035 0.17  0.715 0.18  0.065 0.15  0.77  0.06  0.94  0.235\n",
      " 0.045 0.135 0.105 0.32  0.195 0.25  0.935 0.87  0.93  0.065 0.045 0.09\n",
      " 0.73  0.06  0.195 0.755 0.09  0.76  0.875 0.09  0.15  0.785 0.74  0.22\n",
      " 0.135 0.905 0.85  0.135 0.22  0.25  0.09  0.77  0.855 0.07  0.14  0.065\n",
      " 0.16  0.05  0.28  0.06  0.04  0.745 0.72  0.05  0.17  0.855 0.045 0.045\n",
      " 0.87  0.145 0.055 0.185 0.185 0.98  0.865 0.88  0.01  0.875 0.205 0.84\n",
      " 0.065 0.72  0.14  0.82  0.095 0.91  0.11  0.08  0.985 0.02  0.09  0.055\n",
      " 0.87  0.025 0.085 0.95  0.795 0.115 0.065 0.155 0.935 0.79  0.095 0.04\n",
      " 0.91  0.1   0.88  0.74  0.09  0.885 0.9   0.72  0.94  0.04  0.035 0.16\n",
      " 0.135 0.945 0.85  0.86  0.1   0.055 0.24  0.85  0.2   0.81  0.94  0.955\n",
      " 0.045 0.035 0.96  0.905 0.12  0.925 0.155 0.945 0.125 0.695 0.945 0.965\n",
      " 0.155 0.07  0.16  0.965 0.05  0.87  0.055 0.11  0.115 0.745 0.78  0.1\n",
      " 0.79  0.165 0.165 0.04  0.77  0.22  0.895 0.745]\n",
      "[0.875 0.93  0.925 0.79  0.205 0.06  0.76  0.055 0.135 0.7   0.04  0.79\n",
      " 0.94  0.045 0.95  0.82  0.06  0.09  0.06  0.775 0.035 0.1   0.29  0.76\n",
      " 0.75  0.04  0.13  0.96  0.955 0.055 0.915 0.2   0.055 0.735 0.15  0.045\n",
      " 0.95  0.27  0.815 0.91  0.875 0.25  0.01  0.945 0.96  0.07  0.945 0.765\n",
      " 0.865 0.04  0.04  0.145 0.73  0.215 0.08  0.175 0.76  0.05  0.955 0.3\n",
      " 0.085 0.16  0.105 0.215 0.195 0.23  0.91  0.91  0.96  0.07  0.035 0.065\n",
      " 0.71  0.02  0.185 0.795 0.06  0.755 0.87  0.08  0.16  0.785 0.755 0.195\n",
      " 0.13  0.935 0.875 0.115 0.18  0.21  0.08  0.8   0.855 0.03  0.23  0.065\n",
      " 0.215 0.09  0.235 0.05  0.095 0.875 0.74  0.05  0.22  0.835 0.085 0.045\n",
      " 0.865 0.09  0.08  0.25  0.155 0.96  0.78  0.88  0.03  0.865 0.25  0.84\n",
      " 0.095 0.745 0.13  0.855 0.07  0.91  0.07  0.105 0.93  0.035 0.12  0.05\n",
      " 0.94  0.04  0.09  0.955 0.765 0.165 0.08  0.13  0.98  0.85  0.08  0.05\n",
      " 0.935 0.08  0.935 0.765 0.115 0.915 0.92  0.15  0.775 0.025 0.89  0.925\n",
      " 0.96  0.8   0.115 0.96  0.92  0.895 0.12  0.055 0.065 0.95  0.945 0.855\n",
      " 0.075 0.89  0.105 0.92  0.2   0.04  0.09  0.08  0.925 0.15  0.905 0.81\n",
      " 0.11  0.125 0.105 0.075 0.1   0.145 0.925 0.945 0.82  0.905 0.825 0.83\n",
      " 0.765 0.835 0.865 0.935 0.125 0.175 0.73  0.925]\n",
      "[0.05  0.225 0.72  0.18  0.105 0.23  0.785 0.035 0.985 0.28  0.095 0.1\n",
      " 0.1   0.295 0.175 0.17  0.94  0.89  0.9   0.08  0.04  0.095 0.77  0.025\n",
      " 0.175 0.75  0.025 0.78  0.845 0.095 0.145 0.725 0.74  0.25  0.115 0.905\n",
      " 0.86  0.105 0.11  0.22  0.11  0.825 0.875 0.03  0.205 0.075 0.21  0.06\n",
      " 0.2   0.1   0.07  0.85  0.74  0.07  0.155 0.88  0.085 0.045 0.82  0.105\n",
      " 0.09  0.205 0.225 0.975 0.84  0.885 0.045 0.9   0.16  0.815 0.13  0.765\n",
      " 0.11  0.925 0.075 0.9   0.07  0.095 0.95  0.06  0.11  0.045 0.875 0.045\n",
      " 0.12  0.89  0.735 0.165 0.095 0.15  0.99  0.86  0.07  0.05  0.915 0.06\n",
      " 0.915 0.795 0.105 0.845 0.91  0.12  0.745 0.045 0.855 0.95  0.955 0.76\n",
      " 0.17  0.95  0.93  0.945 0.08  0.045 0.06  0.93  0.985 0.895 0.13  0.86\n",
      " 0.1   0.955 0.165 0.02  0.07  0.07  0.955 0.195 0.86  0.815 0.105 0.125\n",
      " 0.07  0.1   0.06  0.13  0.875 0.95  0.785 0.855 0.755 0.85  0.855 0.85\n",
      " 0.97  0.94  0.1   0.155 0.645 0.975 0.945 0.73  0.95  0.085 0.06  0.2\n",
      " 0.15  0.965 0.81  0.87  0.105 0.065 0.22  0.86  0.23  0.745 0.855 0.97\n",
      " 0.105 0.025 0.925 0.925 0.065 0.975 0.23  0.97  0.165 0.695 0.92  0.97\n",
      " 0.155 0.085 0.2   0.945 0.04  0.875 0.035 0.145 0.115 0.74  0.765 0.135\n",
      " 0.76  0.095 0.145 0.045 0.805 0.195 0.92  0.79 ]\n",
      "[0.845 0.925 0.96  0.835 0.17  0.105 0.75  0.07  0.165 0.8   0.03  0.82\n",
      " 0.97  0.09  0.91  0.88  0.065 0.035 0.05  0.89  0.025 0.185 0.3   0.82\n",
      " 0.875 0.1   0.105 0.985 0.97  0.06  0.87  0.22  0.05  0.785 0.235 0.115\n",
      " 0.97  0.31  0.81  0.97  0.87  0.22  0.05  0.96  0.97  0.075 0.97  0.695\n",
      " 0.915 0.075 0.085 0.88  0.74  0.05  0.195 0.9   0.04  0.035 0.9   0.125\n",
      " 0.065 0.12  0.235 0.985 0.795 0.945 0.04  0.95  0.15  0.855 0.105 0.845\n",
      " 0.145 0.89  0.105 0.88  0.115 0.085 0.985 0.045 0.08  0.015 0.94  0.005\n",
      " 0.135 0.94  0.74  0.215 0.045 0.205 0.97  0.795 0.085 0.045 0.97  0.035\n",
      " 0.955 0.72  0.065 0.92  0.91  0.165 0.82  0.06  0.93  0.95  0.975 0.76\n",
      " 0.13  0.97  0.935 0.925 0.175 0.035 0.035 0.965 0.95  0.895 0.125 0.93\n",
      " 0.09  0.95  0.21  0.055 0.06  0.065 0.99  0.155 0.835 0.905 0.065 0.08\n",
      " 0.075 0.05  0.065 0.13  0.95  0.965 0.785 0.85  0.8   0.84  0.82  0.855\n",
      " 0.975 0.975 0.09  0.17  0.675 0.99  0.9   0.69  0.95  0.085 0.07  0.155\n",
      " 0.17  0.98  0.915 0.875 0.125 0.035 0.185 0.85  0.29  0.79  0.965 0.99\n",
      " 0.085 0.015 0.96  0.885 0.055 0.945 0.295 0.975 0.155 0.75  0.965 0.96\n",
      " 0.185 0.095 0.155 0.975 0.015 0.955 0.055 0.23  0.095 0.845 0.77  0.105\n",
      " 0.725 0.17  0.145 0.035 0.85  0.21  0.94  0.76 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.905 0.88  0.91  0.86  0.15  0.11  0.79  0.055 0.165 0.7   0.055 0.84\n",
      " 0.97  0.07  0.875 0.875 0.085 0.045 0.065 0.85  0.065 0.18  0.325 0.77\n",
      " 0.815 0.13  0.165 0.96  0.96  0.07  0.895 0.23  0.04  0.795 0.195 0.085\n",
      " 0.935 0.31  0.865 0.93  0.79  0.27  0.035 0.95  0.955 0.08  0.96  0.75\n",
      " 0.885 0.08  0.02  0.15  0.72  0.175 0.11  0.205 0.73  0.045 0.965 0.305\n",
      " 0.09  0.13  0.115 0.285 0.185 0.23  0.865 0.875 0.91  0.03  0.03  0.035\n",
      " 0.72  0.055 0.225 0.82  0.075 0.8   0.91  0.125 0.145 0.76  0.78  0.3\n",
      " 0.125 0.945 0.9   0.115 0.225 0.275 0.045 0.79  0.86  0.045 0.165 0.09\n",
      " 0.215 0.095 0.21  0.09  0.91  0.09  0.78  0.045 0.9   0.92  0.975 0.71\n",
      " 0.16  0.98  0.87  0.92  0.12  0.065 0.06  0.89  0.98  0.86  0.145 0.88\n",
      " 0.075 0.96  0.19  0.09  0.07  0.05  0.955 0.275 0.82  0.83  0.065 0.08\n",
      " 0.09  0.08  0.04  0.125 0.885 0.91  0.845 0.9   0.775 0.795 0.84  0.875\n",
      " 0.93  0.945 0.11  0.225 0.685 0.965 0.875 0.82  0.92  0.09  0.07  0.15\n",
      " 0.16  0.985 0.895 0.8   0.095 0.055 0.23  0.8   0.225 0.86  0.93  0.985\n",
      " 0.095 0.035 0.945 0.89  0.095 0.925 0.22  0.935 0.17  0.665 0.93  0.96\n",
      " 0.145 0.05  0.195 0.955 0.07  0.835 0.055 0.165 0.125 0.805 0.79  0.165\n",
      " 0.765 0.12  0.105 0.075 0.82  0.225 0.895 0.79 ]\n",
      "[0.925 0.925 0.935 0.88  0.21  0.055 0.715 0.055 0.18  0.695 0.03  0.82\n",
      " 0.975 0.095 0.92  0.78  0.105 0.055 0.08  0.83  0.02  0.215 0.24  0.78\n",
      " 0.835 0.07  0.12  0.935 0.965 0.04  0.865 0.175 0.045 0.8   0.21  0.115\n",
      " 0.935 0.275 0.83  0.945 0.83  0.235 0.02  0.935 0.945 0.08  0.915 0.69\n",
      " 0.87  0.05  0.04  0.135 0.68  0.17  0.095 0.22  0.795 0.07  0.965 0.3\n",
      " 0.04  0.13  0.1   0.215 0.175 0.145 0.92  0.885 0.905 0.015 0.03  0.08\n",
      " 0.725 0.06  0.195 0.73  0.105 0.795 0.91  0.06  0.16  0.705 0.77  0.23\n",
      " 0.1   0.905 0.825 0.14  0.2   0.24  0.09  0.785 0.87  0.055 0.195 0.04\n",
      " 0.175 0.055 0.23  0.08  0.055 0.785 0.765 0.065 0.185 0.875 0.055 0.05\n",
      " 0.85  0.105 0.015 0.185 0.17  0.94  0.83  0.855 0.03  0.915 0.195 0.82\n",
      " 0.105 0.78  0.125 0.865 0.1   0.89  0.13  0.08  0.915 0.045 0.12  0.06\n",
      " 0.88  0.04  0.17  0.93  0.825 0.135 0.08  0.12  0.96  0.78  0.1   0.03\n",
      " 0.94  0.07  0.915 0.795 0.07  0.87  0.905 0.73  0.935 0.1   0.05  0.14\n",
      " 0.095 0.925 0.9   0.9   0.08  0.055 0.185 0.815 0.3   0.785 0.93  0.93\n",
      " 0.045 0.025 0.94  0.855 0.09  0.92  0.155 0.915 0.175 0.775 0.96  0.965\n",
      " 0.145 0.11  0.135 0.945 0.07  0.805 0.06  0.115 0.1   0.78  0.77  0.11\n",
      " 0.775 0.19  0.175 0.035 0.785 0.13  0.87  0.75 ]\n",
      "[0.91  0.905 0.93  0.84  0.16  0.105 0.795 0.1   0.21  0.675 0.055 0.81\n",
      " 0.95  0.095 0.925 0.845 0.04  0.055 0.055 0.825 0.025 0.16  0.225 0.76\n",
      " 0.79  0.105 0.13  0.93  0.96  0.085 0.885 0.24  0.045 0.805 0.2   0.09\n",
      " 0.955 0.24  0.835 0.95  0.86  0.215 0.045 0.935 0.95  0.115 0.93  0.775\n",
      " 0.87  0.035 0.02  0.18  0.835 0.255 0.125 0.155 0.75  0.055 0.95  0.28\n",
      " 0.055 0.085 0.095 0.265 0.26  0.24  0.925 0.9   0.935 0.095 0.035 0.055\n",
      " 0.74  0.025 0.17  0.77  0.085 0.79  0.835 0.095 0.185 0.75  0.795 0.285\n",
      " 0.145 0.94  0.905 0.105 0.155 0.32  0.055 0.81  0.87  0.05  0.205 0.095\n",
      " 0.16  0.105 0.26  0.03  0.11  0.87  0.75  0.09  0.285 0.91  0.05  0.07\n",
      " 0.87  0.115 0.06  0.23  0.22  0.96  0.785 0.93  0.015 0.9   0.205 0.795\n",
      " 0.095 0.76  0.185 0.84  0.06  0.905 0.075 0.125 0.915 0.035 0.135 0.065\n",
      " 0.89  0.045 0.115 0.95  0.81  0.115 0.075 0.19  0.975 0.87  0.065 0.05\n",
      " 0.91  0.125 0.955 0.81  0.105 0.89  0.87  0.14  0.835 0.02  0.925 0.975\n",
      " 0.965 0.77  0.16  0.92  0.905 0.925 0.14  0.04  0.09  0.935 0.975 0.865\n",
      " 0.095 0.89  0.085 0.91  0.205 0.04  0.075 0.075 0.905 0.155 0.845 0.82\n",
      " 0.105 0.115 0.09  0.05  0.1   0.125 0.895 0.89  0.83  0.925 0.785 0.84\n",
      " 0.875 0.855 0.92  0.93  0.115 0.17  0.675 0.925]\n",
      "[0.03  0.165 0.72  0.205 0.095 0.195 0.76  0.035 0.985 0.325 0.09  0.105\n",
      " 0.1   0.305 0.13  0.19  0.945 0.86  0.94  0.075 0.055 0.085 0.815 0.035\n",
      " 0.225 0.785 0.055 0.795 0.855 0.095 0.12  0.695 0.775 0.235 0.105 0.94\n",
      " 0.89  0.11  0.185 0.28  0.065 0.815 0.85  0.055 0.205 0.12  0.2   0.07\n",
      " 0.17  0.085 0.08  0.855 0.72  0.06  0.235 0.91  0.05  0.05  0.855 0.045\n",
      " 0.04  0.18  0.28  0.975 0.85  0.93  0.035 0.925 0.22  0.82  0.075 0.765\n",
      " 0.085 0.835 0.05  0.895 0.07  0.125 0.975 0.07  0.1   0.055 0.89  0.08\n",
      " 0.07  0.91  0.8   0.12  0.06  0.115 0.985 0.8   0.035 0.035 0.935 0.07\n",
      " 0.935 0.73  0.11  0.9   0.925 0.135 0.795 0.045 0.83  0.955 0.985 0.745\n",
      " 0.16  0.935 0.95  0.895 0.12  0.05  0.08  0.945 0.97  0.88  0.095 0.855\n",
      " 0.11  0.975 0.215 0.035 0.015 0.08  0.97  0.195 0.83  0.84  0.08  0.08\n",
      " 0.09  0.11  0.055 0.135 0.86  0.95  0.79  0.915 0.81  0.85  0.865 0.87\n",
      " 0.94  0.975 0.13  0.205 0.635 0.925 0.955 0.76  0.935 0.105 0.065 0.155\n",
      " 0.095 0.97  0.86  0.87  0.055 0.055 0.255 0.85  0.32  0.79  0.9   0.97\n",
      " 0.09  0.055 0.9   0.925 0.11  0.935 0.17  0.955 0.135 0.675 0.94  0.965\n",
      " 0.125 0.07  0.165 0.965 0.075 0.855 0.045 0.18  0.13  0.745 0.855 0.1\n",
      " 0.745 0.085 0.165 0.095 0.83  0.17  0.89  0.77 ]\n",
      "[0.87  0.93  0.96  0.89  0.145 0.105 0.805 0.05  0.18  0.735 0.04  0.75\n",
      " 0.975 0.065 0.945 0.9   0.09  0.025 0.07  0.92  0.03  0.15  0.27  0.775\n",
      " 0.895 0.155 0.14  0.955 0.945 0.075 0.86  0.17  0.035 0.765 0.225 0.105\n",
      " 0.965 0.34  0.815 0.965 0.84  0.215 0.03  0.955 0.97  0.05  0.965 0.81\n",
      " 0.905 0.045 0.085 0.88  0.77  0.055 0.18  0.96  0.035 0.04  0.915 0.1\n",
      " 0.03  0.175 0.27  0.985 0.805 0.925 0.015 0.965 0.15  0.86  0.12  0.765\n",
      " 0.125 0.905 0.085 0.88  0.105 0.13  0.97  0.015 0.1   0.035 0.905 0.015\n",
      " 0.105 0.98  0.73  0.19  0.06  0.195 0.94  0.76  0.085 0.03  0.98  0.07\n",
      " 0.97  0.73  0.09  0.94  0.92  0.22  0.825 0.04  0.92  0.945 0.975 0.745\n",
      " 0.115 0.94  0.925 0.93  0.165 0.055 0.04  0.96  0.975 0.88  0.1   0.865\n",
      " 0.145 0.975 0.215 0.04  0.075 0.08  0.975 0.23  0.855 0.905 0.065 0.07\n",
      " 0.085 0.06  0.07  0.14  0.95  0.93  0.775 0.835 0.82  0.88  0.845 0.9\n",
      " 0.945 0.96  0.09  0.185 0.685 0.975 0.94  0.715 0.945 0.065 0.06  0.185\n",
      " 0.125 0.985 0.89  0.875 0.09  0.03  0.2   0.86  0.28  0.815 0.95  0.98\n",
      " 0.095 0.02  0.965 0.94  0.08  0.95  0.225 0.99  0.19  0.73  0.955 0.96\n",
      " 0.22  0.105 0.165 0.975 0.035 0.95  0.05  0.155 0.11  0.76  0.795 0.145\n",
      " 0.76  0.16  0.215 0.055 0.855 0.125 0.955 0.78 ]\n",
      "[0.905 0.9   0.93  0.845 0.19  0.12  0.755 0.035 0.125 0.73  0.08  0.805\n",
      " 0.94  0.08  0.925 0.86  0.13  0.08  0.06  0.865 0.06  0.145 0.225 0.78\n",
      " 0.765 0.095 0.155 0.935 0.975 0.06  0.905 0.235 0.035 0.76  0.185 0.06\n",
      " 0.95  0.215 0.84  0.945 0.81  0.265 0.03  0.94  0.96  0.075 0.97  0.805\n",
      " 0.89  0.04  0.03  0.205 0.76  0.175 0.09  0.24  0.72  0.04  0.97  0.26\n",
      " 0.05  0.175 0.135 0.24  0.2   0.23  0.915 0.915 0.89  0.055 0.03  0.065\n",
      " 0.775 0.065 0.15  0.74  0.07  0.82  0.885 0.09  0.21  0.76  0.765 0.245\n",
      " 0.165 0.91  0.855 0.125 0.27  0.265 0.055 0.795 0.86  0.025 0.18  0.12\n",
      " 0.16  0.145 0.245 0.055 0.875 0.105 0.755 0.025 0.885 0.945 0.975 0.765\n",
      " 0.105 0.97  0.93  0.895 0.15  0.055 0.065 0.955 0.97  0.815 0.165 0.85\n",
      " 0.115 0.935 0.21  0.09  0.06  0.055 0.95  0.22  0.84  0.81  0.11  0.11\n",
      " 0.06  0.05  0.085 0.105 0.875 0.9   0.81  0.895 0.83  0.785 0.795 0.815\n",
      " 0.91  0.96  0.095 0.235 0.7   0.94  0.88  0.735 0.94  0.1   0.08  0.13\n",
      " 0.175 0.95  0.85  0.86  0.08  0.06  0.285 0.795 0.23  0.815 0.935 0.985\n",
      " 0.09  0.055 0.95  0.865 0.07  0.935 0.215 0.95  0.19  0.66  0.915 0.96\n",
      " 0.16  0.115 0.215 0.945 0.09  0.855 0.065 0.155 0.19  0.785 0.755 0.155\n",
      " 0.69  0.115 0.145 0.04  0.75  0.16  0.895 0.74 ]\n",
      "[0.895 0.91  0.93  0.825 0.165 0.06  0.78  0.065 0.185 0.695 0.035 0.83\n",
      " 0.965 0.085 0.875 0.865 0.16  0.06  0.075 0.865 0.035 0.18  0.285 0.775\n",
      " 0.77  0.095 0.14  0.94  0.97  0.07  0.86  0.225 0.055 0.835 0.225 0.1\n",
      " 0.935 0.26  0.79  0.955 0.81  0.205 0.04  0.925 0.96  0.035 0.95  0.82\n",
      " 0.91  0.04  0.01  0.12  0.795 0.235 0.105 0.195 0.705 0.045 0.97  0.275\n",
      " 0.055 0.14  0.09  0.22  0.16  0.18  0.86  0.92  0.9   0.04  0.03  0.06\n",
      " 0.75  0.07  0.21  0.825 0.055 0.815 0.845 0.06  0.165 0.75  0.74  0.205\n",
      " 0.11  0.925 0.83  0.12  0.18  0.25  0.055 0.78  0.895 0.055 0.185 0.05\n",
      " 0.27  0.065 0.24  0.065 0.075 0.815 0.695 0.045 0.18  0.9   0.075 0.04\n",
      " 0.88  0.075 0.04  0.14  0.165 0.965 0.81  0.875 0.035 0.9   0.18  0.825\n",
      " 0.115 0.71  0.145 0.86  0.12  0.84  0.07  0.085 0.95  0.03  0.095 0.06\n",
      " 0.87  0.04  0.135 0.915 0.77  0.17  0.03  0.115 0.975 0.81  0.09  0.04\n",
      " 0.925 0.055 0.92  0.8   0.08  0.905 0.9   0.715 0.955 0.07  0.055 0.165\n",
      " 0.165 0.945 0.915 0.86  0.08  0.04  0.185 0.79  0.265 0.785 0.935 0.955\n",
      " 0.06  0.015 0.96  0.82  0.1   0.945 0.155 0.935 0.125 0.67  0.955 0.975\n",
      " 0.21  0.08  0.215 0.915 0.06  0.875 0.04  0.145 0.135 0.835 0.76  0.085\n",
      " 0.765 0.16  0.185 0.075 0.715 0.205 0.925 0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.895 0.905 0.95  0.885 0.13  0.1   0.72  0.06  0.18  0.755 0.06  0.77\n",
      " 0.955 0.09  0.93  0.78  0.045 0.06  0.09  0.845 0.055 0.145 0.375 0.72\n",
      " 0.77  0.07  0.16  0.94  0.975 0.045 0.895 0.24  0.03  0.78  0.15  0.05\n",
      " 0.95  0.285 0.79  0.865 0.845 0.245 0.03  0.97  0.955 0.095 0.94  0.805\n",
      " 0.865 0.08  0.055 0.19  0.805 0.21  0.12  0.22  0.7   0.045 0.955 0.33\n",
      " 0.085 0.11  0.115 0.3   0.235 0.225 0.93  0.94  0.935 0.105 0.025 0.055\n",
      " 0.655 0.03  0.12  0.795 0.07  0.81  0.865 0.07  0.225 0.765 0.815 0.265\n",
      " 0.095 0.975 0.87  0.135 0.18  0.215 0.055 0.77  0.885 0.085 0.175 0.04\n",
      " 0.175 0.085 0.255 0.075 0.115 0.855 0.78  0.095 0.21  0.895 0.055 0.035\n",
      " 0.875 0.095 0.08  0.255 0.205 0.96  0.81  0.915 0.02  0.91  0.175 0.825\n",
      " 0.055 0.82  0.135 0.895 0.12  0.93  0.085 0.13  0.935 0.045 0.135 0.085\n",
      " 0.895 0.035 0.1   0.97  0.83  0.13  0.09  0.135 0.955 0.835 0.065 0.05\n",
      " 0.89  0.09  0.94  0.75  0.125 0.95  0.89  0.125 0.75  0.025 0.89  0.945\n",
      " 0.975 0.76  0.155 0.96  0.94  0.89  0.085 0.025 0.06  0.97  0.97  0.88\n",
      " 0.03  0.875 0.12  0.94  0.195 0.04  0.07  0.095 0.955 0.17  0.845 0.84\n",
      " 0.075 0.12  0.075 0.05  0.06  0.115 0.95  0.93  0.795 0.935 0.775 0.82\n",
      " 0.78  0.82  0.9   0.92  0.115 0.185 0.645 0.945]\n",
      "[0.04  0.175 0.75  0.19  0.105 0.185 0.805 0.02  0.965 0.245 0.11  0.115\n",
      " 0.05  0.275 0.215 0.255 0.93  0.835 0.94  0.07  0.065 0.12  0.74  0.025\n",
      " 0.2   0.8   0.06  0.805 0.835 0.08  0.115 0.765 0.73  0.28  0.09  0.935\n",
      " 0.855 0.135 0.14  0.2   0.115 0.79  0.81  0.075 0.215 0.11  0.215 0.06\n",
      " 0.195 0.105 0.06  0.835 0.68  0.075 0.175 0.9   0.05  0.035 0.905 0.065\n",
      " 0.07  0.165 0.27  0.99  0.835 0.9   0.05  0.91  0.185 0.84  0.075 0.865\n",
      " 0.085 0.865 0.03  0.9   0.1   0.075 0.98  0.025 0.11  0.05  0.845 0.045\n",
      " 0.075 0.935 0.735 0.125 0.04  0.14  0.98  0.83  0.09  0.055 0.915 0.055\n",
      " 0.915 0.695 0.1   0.865 0.895 0.125 0.75  0.045 0.87  0.96  0.99  0.76\n",
      " 0.155 0.95  0.95  0.885 0.16  0.045 0.075 0.92  0.965 0.86  0.125 0.895\n",
      " 0.15  0.96  0.185 0.045 0.035 0.07  0.96  0.21  0.845 0.86  0.095 0.08\n",
      " 0.095 0.09  0.055 0.11  0.86  0.92  0.79  0.875 0.77  0.825 0.855 0.845\n",
      " 0.945 0.945 0.11  0.185 0.67  0.94  0.945 0.705 0.945 0.035 0.08  0.18\n",
      " 0.1   0.955 0.86  0.92  0.16  0.04  0.205 0.87  0.23  0.79  0.88  0.97\n",
      " 0.07  0.03  0.925 0.93  0.125 0.92  0.17  0.96  0.17  0.78  0.965 0.96\n",
      " 0.125 0.105 0.15  0.975 0.035 0.89  0.035 0.135 0.11  0.775 0.79  0.13\n",
      " 0.73  0.115 0.115 0.035 0.835 0.21  0.885 0.805]\n",
      "[0.855 0.905 0.985 0.86  0.195 0.12  0.79  0.035 0.175 0.775 0.045 0.825\n",
      " 0.985 0.085 0.9   0.89  0.08  0.04  0.08  0.905 0.025 0.18  0.3   0.8\n",
      " 0.855 0.125 0.08  0.965 0.93  0.055 0.85  0.25  0.035 0.755 0.23  0.1\n",
      " 0.965 0.25  0.84  0.955 0.835 0.215 0.03  0.93  0.955 0.08  0.98  0.735\n",
      " 0.9   0.02  0.095 0.86  0.72  0.055 0.155 0.945 0.03  0.015 0.905 0.095\n",
      " 0.04  0.16  0.215 0.985 0.865 0.9   0.025 0.96  0.15  0.84  0.14  0.855\n",
      " 0.195 0.905 0.085 0.935 0.085 0.1   0.99  0.045 0.075 0.035 0.9   0.03\n",
      " 0.115 0.94  0.79  0.17  0.06  0.225 0.965 0.8   0.06  0.035 0.965 0.05\n",
      " 0.94  0.73  0.085 0.915 0.95  0.165 0.805 0.02  0.92  0.94  0.985 0.785\n",
      " 0.105 0.945 0.895 0.905 0.205 0.08  0.04  0.96  0.97  0.905 0.15  0.875\n",
      " 0.085 0.975 0.27  0.045 0.03  0.095 0.955 0.21  0.83  0.905 0.055 0.06\n",
      " 0.085 0.06  0.085 0.125 0.95  0.95  0.79  0.865 0.85  0.88  0.825 0.87\n",
      " 0.945 0.945 0.08  0.13  0.69  0.985 0.95  0.72  0.97  0.075 0.05  0.19\n",
      " 0.175 0.98  0.93  0.835 0.055 0.03  0.195 0.885 0.25  0.845 0.92  0.975\n",
      " 0.075 0.025 0.965 0.955 0.11  0.955 0.225 0.99  0.21  0.69  0.985 0.95\n",
      " 0.19  0.1   0.16  0.96  0.055 0.955 0.08  0.21  0.055 0.755 0.78  0.12\n",
      " 0.755 0.075 0.205 0.02  0.88  0.15  0.965 0.78 ]\n",
      "[0.93  0.935 0.905 0.82  0.12  0.11  0.8   0.035 0.115 0.71  0.07  0.85\n",
      " 0.96  0.115 0.92  0.805 0.135 0.07  0.075 0.855 0.055 0.185 0.235 0.74\n",
      " 0.77  0.14  0.16  0.95  0.935 0.05  0.91  0.22  0.03  0.8   0.18  0.085\n",
      " 0.96  0.265 0.83  0.91  0.82  0.215 0.01  0.975 0.965 0.1   0.945 0.775\n",
      " 0.91  0.09  0.05  0.195 0.71  0.185 0.09  0.185 0.735 0.065 0.96  0.31\n",
      " 0.07  0.13  0.085 0.235 0.2   0.24  0.89  0.93  0.94  0.045 0.05  0.045\n",
      " 0.765 0.04  0.16  0.74  0.11  0.855 0.85  0.1   0.195 0.775 0.78  0.28\n",
      " 0.145 0.93  0.89  0.075 0.215 0.225 0.05  0.815 0.89  0.05  0.205 0.11\n",
      " 0.19  0.08  0.315 0.06  0.9   0.155 0.805 0.055 0.915 0.94  0.94  0.775\n",
      " 0.155 0.915 0.96  0.92  0.135 0.09  0.065 0.895 0.965 0.845 0.185 0.91\n",
      " 0.14  0.93  0.225 0.06  0.07  0.09  0.915 0.21  0.83  0.84  0.04  0.08\n",
      " 0.07  0.09  0.065 0.145 0.865 0.915 0.87  0.89  0.775 0.79  0.815 0.84\n",
      " 0.935 0.93  0.11  0.205 0.735 0.935 0.94  0.755 0.945 0.1   0.075 0.155\n",
      " 0.165 0.965 0.845 0.87  0.075 0.025 0.24  0.83  0.29  0.86  0.905 0.975\n",
      " 0.07  0.03  0.92  0.88  0.095 0.93  0.18  0.935 0.155 0.74  0.95  0.95\n",
      " 0.1   0.09  0.18  0.95  0.105 0.87  0.035 0.16  0.15  0.755 0.71  0.105\n",
      " 0.73  0.13  0.17  0.05  0.79  0.18  0.91  0.78 ]\n",
      "[0.88  0.89  0.955 0.83  0.14  0.085 0.71  0.075 0.19  0.705 0.05  0.785\n",
      " 0.945 0.095 0.92  0.875 0.12  0.055 0.08  0.85  0.03  0.245 0.28  0.705\n",
      " 0.835 0.065 0.135 0.915 0.97  0.07  0.89  0.185 0.035 0.835 0.145 0.085\n",
      " 0.93  0.255 0.825 0.94  0.85  0.295 0.045 0.95  0.92  0.085 0.925 0.735\n",
      " 0.865 0.04  0.025 0.135 0.745 0.21  0.06  0.27  0.73  0.075 0.955 0.285\n",
      " 0.055 0.135 0.08  0.17  0.21  0.18  0.935 0.88  0.89  0.03  0.035 0.095\n",
      " 0.7   0.07  0.25  0.75  0.08  0.755 0.89  0.075 0.12  0.775 0.715 0.26\n",
      " 0.115 0.91  0.86  0.12  0.22  0.23  0.145 0.785 0.815 0.05  0.17  0.06\n",
      " 0.26  0.045 0.19  0.07  0.07  0.765 0.69  0.08  0.18  0.925 0.055 0.03\n",
      " 0.865 0.1   0.035 0.175 0.175 0.965 0.8   0.865 0.01  0.9   0.22  0.835\n",
      " 0.095 0.75  0.16  0.86  0.065 0.87  0.09  0.115 0.94  0.04  0.09  0.04\n",
      " 0.885 0.065 0.115 0.91  0.81  0.145 0.065 0.145 0.955 0.82  0.125 0.05\n",
      " 0.93  0.08  0.925 0.805 0.11  0.92  0.935 0.725 0.965 0.1   0.06  0.135\n",
      " 0.17  0.945 0.865 0.865 0.1   0.05  0.2   0.79  0.275 0.8   0.945 0.96\n",
      " 0.075 0.04  0.955 0.86  0.075 0.95  0.165 0.9   0.125 0.755 0.955 0.975\n",
      " 0.175 0.06  0.185 0.965 0.055 0.825 0.065 0.12  0.13  0.74  0.79  0.07\n",
      " 0.83  0.155 0.215 0.045 0.77  0.16  0.85  0.795]\n",
      "[0.875 0.86  0.955 0.835 0.125 0.09  0.84  0.065 0.17  0.745 0.075 0.87\n",
      " 0.97  0.07  0.96  0.81  0.07  0.055 0.09  0.78  0.03  0.145 0.215 0.765\n",
      " 0.755 0.06  0.145 0.925 0.965 0.06  0.89  0.19  0.065 0.765 0.12  0.06\n",
      " 0.975 0.24  0.825 0.93  0.84  0.2   0.035 0.955 0.945 0.07  0.925 0.76\n",
      " 0.91  0.045 0.04  0.17  0.785 0.2   0.1   0.155 0.73  0.055 0.945 0.345\n",
      " 0.06  0.085 0.11  0.275 0.23  0.225 0.92  0.945 0.935 0.04  0.03  0.06\n",
      " 0.765 0.07  0.195 0.79  0.07  0.795 0.88  0.125 0.175 0.77  0.785 0.245\n",
      " 0.13  0.975 0.86  0.1   0.245 0.245 0.055 0.8   0.835 0.045 0.21  0.04\n",
      " 0.235 0.135 0.26  0.045 0.09  0.85  0.79  0.065 0.265 0.875 0.055 0.04\n",
      " 0.855 0.115 0.095 0.21  0.2   0.965 0.75  0.86  0.015 0.905 0.185 0.795\n",
      " 0.055 0.755 0.125 0.855 0.085 0.92  0.1   0.09  0.92  0.015 0.115 0.055\n",
      " 0.91  0.015 0.095 0.96  0.8   0.125 0.11  0.11  0.975 0.85  0.095 0.05\n",
      " 0.915 0.125 0.925 0.795 0.125 0.91  0.865 0.205 0.865 0.025 0.89  0.945\n",
      " 0.96  0.805 0.13  0.965 0.92  0.95  0.145 0.07  0.04  0.955 0.965 0.895\n",
      " 0.075 0.875 0.085 0.935 0.26  0.045 0.065 0.095 0.905 0.145 0.88  0.81\n",
      " 0.085 0.1   0.045 0.07  0.115 0.125 0.905 0.89  0.815 0.925 0.8   0.825\n",
      " 0.77  0.815 0.925 0.95  0.14  0.175 0.7   0.92 ]\n",
      "[0.03  0.215 0.775 0.14  0.07  0.18  0.76  0.045 0.985 0.22  0.08  0.13\n",
      " 0.11  0.235 0.165 0.12  0.945 0.845 0.925 0.1   0.075 0.08  0.745 0.045\n",
      " 0.18  0.74  0.08  0.83  0.85  0.11  0.11  0.77  0.765 0.26  0.12  0.92\n",
      " 0.86  0.11  0.15  0.24  0.075 0.805 0.87  0.05  0.25  0.075 0.185 0.055\n",
      " 0.225 0.065 0.065 0.815 0.75  0.065 0.13  0.88  0.04  0.05  0.855 0.095\n",
      " 0.06  0.175 0.225 0.975 0.8   0.93  0.07  0.885 0.26  0.8   0.09  0.745\n",
      " 0.085 0.85  0.1   0.85  0.115 0.07  0.955 0.045 0.165 0.065 0.89  0.04\n",
      " 0.1   0.87  0.76  0.14  0.065 0.15  0.98  0.835 0.04  0.035 0.935 0.065\n",
      " 0.94  0.74  0.065 0.88  0.93  0.17  0.76  0.015 0.875 0.945 0.99  0.74\n",
      " 0.16  0.94  0.96  0.915 0.12  0.035 0.07  0.925 0.98  0.86  0.07  0.865\n",
      " 0.15  0.97  0.2   0.025 0.03  0.105 0.935 0.17  0.865 0.85  0.095 0.045\n",
      " 0.08  0.08  0.05  0.12  0.825 0.905 0.775 0.905 0.77  0.83  0.82  0.82\n",
      " 0.95  0.955 0.07  0.155 0.645 0.92  0.97  0.68  0.935 0.085 0.04  0.195\n",
      " 0.135 0.93  0.865 0.855 0.09  0.05  0.23  0.835 0.29  0.825 0.9   0.97\n",
      " 0.05  0.035 0.905 0.895 0.065 0.93  0.165 0.96  0.15  0.715 0.96  0.96\n",
      " 0.13  0.075 0.165 0.96  0.03  0.88  0.035 0.175 0.13  0.74  0.81  0.13\n",
      " 0.73  0.13  0.11  0.07  0.78  0.14  0.86  0.77 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86  0.915 0.93  0.9   0.2   0.1   0.74  0.08  0.2   0.77  0.055 0.76\n",
      " 0.98  0.07  0.92  0.875 0.095 0.04  0.08  0.875 0.04  0.19  0.33  0.795\n",
      " 0.865 0.13  0.12  0.975 0.965 0.065 0.84  0.145 0.025 0.825 0.24  0.095\n",
      " 0.96  0.36  0.81  0.965 0.89  0.2   0.03  0.97  0.98  0.075 0.97  0.755\n",
      " 0.94  0.03  0.09  0.86  0.755 0.08  0.18  0.885 0.04  0.055 0.865 0.13\n",
      " 0.04  0.16  0.14  0.985 0.87  0.91  0.03  0.975 0.18  0.785 0.12  0.78\n",
      " 0.185 0.875 0.075 0.895 0.105 0.125 0.955 0.085 0.105 0.035 0.89  0.02\n",
      " 0.125 0.95  0.83  0.195 0.07  0.16  0.95  0.745 0.085 0.04  0.96  0.045\n",
      " 0.955 0.75  0.115 0.9   0.94  0.165 0.815 0.035 0.925 0.935 0.99  0.785\n",
      " 0.13  0.96  0.935 0.95  0.195 0.04  0.04  0.975 0.99  0.91  0.155 0.895\n",
      " 0.115 0.955 0.2   0.04  0.065 0.085 0.98  0.285 0.865 0.905 0.06  0.105\n",
      " 0.095 0.085 0.075 0.125 0.955 0.94  0.78  0.825 0.85  0.835 0.83  0.88\n",
      " 0.965 0.95  0.105 0.175 0.66  0.97  0.955 0.725 0.955 0.09  0.025 0.195\n",
      " 0.18  0.985 0.915 0.895 0.095 0.015 0.16  0.84  0.235 0.865 0.96  1.\n",
      " 0.09  0.005 0.94  0.91  0.085 0.945 0.23  0.99  0.15  0.78  0.955 0.965\n",
      " 0.185 0.105 0.225 0.975 0.045 0.925 0.055 0.18  0.07  0.825 0.795 0.165\n",
      " 0.82  0.115 0.19  0.03  0.885 0.21  0.98  0.71 ]\n",
      "[0.865 0.9   0.88  0.84  0.145 0.11  0.745 0.03  0.125 0.72  0.045 0.835\n",
      " 0.965 0.125 0.895 0.87  0.075 0.1   0.075 0.83  0.035 0.13  0.35  0.755\n",
      " 0.775 0.085 0.125 0.94  0.975 0.08  0.865 0.175 0.09  0.8   0.165 0.09\n",
      " 0.94  0.29  0.835 0.91  0.795 0.185 0.045 0.925 0.935 0.045 0.925 0.77\n",
      " 0.905 0.045 0.045 0.13  0.675 0.18  0.1   0.19  0.775 0.025 0.98  0.285\n",
      " 0.075 0.145 0.11  0.22  0.195 0.2   0.88  0.925 0.935 0.04  0.035 0.065\n",
      " 0.715 0.065 0.125 0.765 0.105 0.82  0.875 0.075 0.175 0.77  0.82  0.295\n",
      " 0.2   0.87  0.845 0.09  0.245 0.245 0.09  0.81  0.885 0.045 0.165 0.09\n",
      " 0.2   0.085 0.255 0.06  0.895 0.1   0.805 0.045 0.885 0.97  0.97  0.77\n",
      " 0.105 0.965 0.93  0.915 0.145 0.06  0.05  0.91  0.945 0.84  0.145 0.87\n",
      " 0.075 0.955 0.225 0.07  0.065 0.09  0.95  0.235 0.855 0.815 0.1   0.08\n",
      " 0.09  0.075 0.09  0.11  0.925 0.905 0.795 0.88  0.77  0.78  0.855 0.85\n",
      " 0.95  0.93  0.075 0.17  0.645 0.925 0.89  0.725 0.94  0.075 0.11  0.135\n",
      " 0.135 0.94  0.88  0.84  0.075 0.06  0.22  0.89  0.275 0.865 0.93  0.945\n",
      " 0.095 0.045 0.93  0.87  0.105 0.935 0.155 0.94  0.185 0.675 0.94  0.955\n",
      " 0.14  0.1   0.19  0.945 0.06  0.855 0.07  0.205 0.065 0.78  0.73  0.07\n",
      " 0.705 0.125 0.185 0.045 0.72  0.215 0.905 0.8  ]\n",
      "[0.92  0.875 0.945 0.835 0.17  0.09  0.75  0.06  0.08  0.655 0.055 0.79\n",
      " 0.965 0.09  0.885 0.865 0.125 0.055 0.065 0.805 0.03  0.125 0.245 0.755\n",
      " 0.745 0.065 0.12  0.915 0.965 0.095 0.825 0.23  0.03  0.795 0.225 0.09\n",
      " 0.925 0.205 0.765 0.93  0.795 0.28  0.02  0.94  0.915 0.06  0.915 0.72\n",
      " 0.89  0.035 0.035 0.125 0.68  0.185 0.06  0.205 0.775 0.035 0.955 0.245\n",
      " 0.09  0.13  0.07  0.295 0.205 0.155 0.91  0.89  0.895 0.045 0.03  0.075\n",
      " 0.69  0.055 0.16  0.785 0.08  0.785 0.88  0.11  0.165 0.74  0.73  0.305\n",
      " 0.09  0.9   0.83  0.09  0.205 0.205 0.08  0.79  0.86  0.08  0.21  0.075\n",
      " 0.21  0.065 0.26  0.055 0.04  0.75  0.74  0.045 0.195 0.885 0.045 0.04\n",
      " 0.835 0.095 0.045 0.165 0.175 0.955 0.84  0.88  0.02  0.94  0.175 0.855\n",
      " 0.065 0.775 0.105 0.84  0.1   0.89  0.085 0.09  0.94  0.035 0.11  0.065\n",
      " 0.885 0.055 0.155 0.925 0.775 0.185 0.075 0.135 0.955 0.825 0.08  0.085\n",
      " 0.925 0.09  0.935 0.72  0.14  0.895 0.9   0.72  0.9   0.06  0.075 0.155\n",
      " 0.115 0.965 0.925 0.865 0.1   0.08  0.265 0.765 0.23  0.835 0.915 0.99\n",
      " 0.08  0.02  0.925 0.845 0.1   0.95  0.215 0.95  0.145 0.71  0.955 0.98\n",
      " 0.185 0.06  0.23  0.925 0.025 0.835 0.08  0.095 0.14  0.765 0.67  0.11\n",
      " 0.77  0.125 0.17  0.05  0.805 0.185 0.91  0.755]\n",
      "[0.89  0.885 0.94  0.8   0.14  0.08  0.8   0.05  0.16  0.74  0.03  0.86\n",
      " 0.955 0.09  0.925 0.795 0.095 0.055 0.085 0.815 0.055 0.115 0.28  0.78\n",
      " 0.74  0.08  0.14  0.945 0.985 0.03  0.89  0.155 0.055 0.78  0.135 0.03\n",
      " 0.93  0.19  0.825 0.975 0.87  0.22  0.035 0.935 0.965 0.085 0.955 0.745\n",
      " 0.92  0.06  0.045 0.085 0.75  0.215 0.11  0.195 0.7   0.04  0.95  0.275\n",
      " 0.05  0.16  0.1   0.215 0.21  0.27  0.93  0.895 0.925 0.065 0.035 0.095\n",
      " 0.685 0.015 0.165 0.745 0.065 0.775 0.86  0.055 0.17  0.73  0.77  0.29\n",
      " 0.125 0.935 0.845 0.13  0.14  0.215 0.05  0.86  0.86  0.055 0.175 0.09\n",
      " 0.18  0.1   0.305 0.06  0.09  0.81  0.765 0.055 0.19  0.885 0.06  0.02\n",
      " 0.86  0.09  0.1   0.155 0.215 0.945 0.76  0.9   0.015 0.905 0.215 0.805\n",
      " 0.065 0.81  0.135 0.845 0.11  0.885 0.085 0.105 0.95  0.025 0.13  0.055\n",
      " 0.895 0.04  0.085 0.95  0.73  0.135 0.13  0.115 0.975 0.845 0.045 0.06\n",
      " 0.95  0.07  0.925 0.69  0.135 0.915 0.87  0.16  0.815 0.005 0.855 0.965\n",
      " 0.98  0.72  0.125 0.935 0.89  0.89  0.115 0.045 0.055 0.935 0.97  0.875\n",
      " 0.1   0.885 0.06  0.94  0.24  0.07  0.1   0.065 0.945 0.155 0.84  0.785\n",
      " 0.085 0.08  0.115 0.06  0.07  0.15  0.915 0.925 0.83  0.895 0.78  0.87\n",
      " 0.8   0.8   0.905 0.945 0.105 0.2   0.65  0.885]\n",
      "[0.065 0.195 0.705 0.18  0.115 0.14  0.795 0.02  0.975 0.29  0.06  0.09\n",
      " 0.11  0.225 0.165 0.2   0.94  0.825 0.935 0.09  0.055 0.11  0.75  0.045\n",
      " 0.28  0.685 0.085 0.77  0.845 0.08  0.16  0.785 0.75  0.265 0.12  0.95\n",
      " 0.86  0.13  0.145 0.22  0.12  0.82  0.845 0.06  0.215 0.065 0.195 0.09\n",
      " 0.185 0.095 0.065 0.825 0.74  0.085 0.195 0.87  0.025 0.04  0.865 0.08\n",
      " 0.04  0.23  0.245 0.96  0.825 0.89  0.025 0.935 0.245 0.84  0.07  0.77\n",
      " 0.09  0.825 0.055 0.915 0.08  0.105 0.98  0.045 0.13  0.075 0.915 0.055\n",
      " 0.075 0.87  0.78  0.215 0.055 0.135 0.98  0.85  0.06  0.055 0.935 0.065\n",
      " 0.95  0.685 0.07  0.885 0.905 0.115 0.745 0.03  0.835 0.925 0.98  0.735\n",
      " 0.185 0.975 0.94  0.88  0.135 0.02  0.035 0.965 0.975 0.835 0.125 0.85\n",
      " 0.12  0.95  0.265 0.02  0.03  0.14  0.985 0.24  0.85  0.85  0.07  0.085\n",
      " 0.06  0.08  0.07  0.12  0.905 0.96  0.84  0.905 0.785 0.825 0.845 0.84\n",
      " 0.945 0.94  0.075 0.18  0.675 0.935 0.98  0.7   0.94  0.055 0.055 0.17\n",
      " 0.12  0.98  0.91  0.885 0.07  0.03  0.3   0.895 0.34  0.815 0.885 0.965\n",
      " 0.065 0.03  0.88  0.91  0.12  0.95  0.18  0.98  0.17  0.72  0.935 0.96\n",
      " 0.14  0.095 0.185 0.97  0.07  0.87  0.06  0.185 0.11  0.815 0.78  0.125\n",
      " 0.705 0.1   0.12  0.065 0.81  0.15  0.86  0.795]\n",
      "[0.865 0.86  0.955 0.895 0.16  0.12  0.81  0.07  0.2   0.73  0.025 0.805\n",
      " 0.97  0.075 0.95  0.88  0.075 0.045 0.06  0.865 0.025 0.215 0.27  0.775\n",
      " 0.905 0.145 0.09  0.99  0.95  0.06  0.895 0.175 0.045 0.8   0.16  0.105\n",
      " 0.97  0.275 0.79  0.97  0.835 0.23  0.03  0.98  0.945 0.055 0.98  0.675\n",
      " 0.89  0.055 0.085 0.9   0.76  0.05  0.19  0.95  0.035 0.04  0.885 0.1\n",
      " 0.075 0.19  0.24  0.99  0.82  0.92  0.01  0.97  0.14  0.855 0.125 0.78\n",
      " 0.165 0.895 0.07  0.91  0.11  0.115 0.995 0.045 0.105 0.055 0.885 0.03\n",
      " 0.085 0.965 0.76  0.215 0.06  0.155 0.96  0.75  0.055 0.04  0.94  0.065\n",
      " 0.98  0.755 0.075 0.935 0.93  0.175 0.82  0.045 0.955 0.93  0.995 0.795\n",
      " 0.135 0.965 0.925 0.9   0.195 0.03  0.04  0.965 1.    0.88  0.135 0.925\n",
      " 0.1   0.97  0.255 0.04  0.04  0.06  0.96  0.205 0.87  0.92  0.03  0.05\n",
      " 0.08  0.07  0.12  0.125 0.975 0.96  0.725 0.775 0.895 0.82  0.84  0.885\n",
      " 0.95  0.97  0.105 0.205 0.64  0.97  0.955 0.66  0.95  0.105 0.03  0.175\n",
      " 0.165 0.99  0.885 0.89  0.09  0.03  0.18  0.84  0.22  0.84  0.94  0.985\n",
      " 0.055 0.025 0.93  0.97  0.07  0.95  0.28  0.97  0.185 0.775 0.96  0.975\n",
      " 0.215 0.1   0.14  0.97  0.03  0.94  0.06  0.215 0.1   0.795 0.8   0.12\n",
      " 0.69  0.09  0.14  0.05  0.885 0.23  0.94  0.77 ]\n",
      "[0.87  0.9   0.89  0.81  0.175 0.1   0.79  0.075 0.18  0.715 0.045 0.825\n",
      " 0.965 0.1   0.875 0.81  0.06  0.065 0.075 0.83  0.075 0.14  0.215 0.69\n",
      " 0.745 0.11  0.16  0.94  0.955 0.075 0.9   0.225 0.055 0.77  0.185 0.075\n",
      " 0.93  0.29  0.78  0.94  0.755 0.255 0.005 0.94  0.955 0.085 0.92  0.765\n",
      " 0.89  0.035 0.045 0.145 0.67  0.24  0.115 0.165 0.735 0.03  0.975 0.29\n",
      " 0.09  0.12  0.12  0.28  0.19  0.195 0.925 0.945 0.94  0.055 0.035 0.1\n",
      " 0.775 0.07  0.205 0.79  0.135 0.815 0.83  0.11  0.24  0.755 0.775 0.23\n",
      " 0.095 0.88  0.81  0.08  0.2   0.19  0.055 0.74  0.875 0.035 0.21  0.095\n",
      " 0.245 0.105 0.225 0.085 0.89  0.125 0.775 0.06  0.905 0.93  0.925 0.695\n",
      " 0.1   0.955 0.93  0.96  0.14  0.05  0.065 0.955 0.955 0.855 0.13  0.915\n",
      " 0.12  0.96  0.19  0.075 0.06  0.07  0.94  0.21  0.875 0.865 0.065 0.08\n",
      " 0.05  0.05  0.075 0.135 0.885 0.895 0.8   0.9   0.795 0.85  0.77  0.815\n",
      " 0.925 0.965 0.14  0.21  0.71  0.895 0.905 0.755 0.945 0.1   0.09  0.18\n",
      " 0.135 0.95  0.855 0.845 0.105 0.015 0.295 0.805 0.265 0.855 0.9   0.97\n",
      " 0.065 0.03  0.905 0.87  0.075 0.945 0.19  0.95  0.185 0.74  0.905 0.965\n",
      " 0.19  0.115 0.165 0.97  0.08  0.86  0.065 0.155 0.15  0.74  0.75  0.125\n",
      " 0.735 0.14  0.145 0.04  0.735 0.17  0.91  0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.865 0.885 0.95  0.785 0.14  0.065 0.745 0.05  0.15  0.765 0.065 0.785\n",
      " 0.955 0.09  0.885 0.855 0.11  0.085 0.08  0.865 0.05  0.17  0.3   0.745\n",
      " 0.85  0.075 0.15  0.91  0.94  0.075 0.885 0.18  0.025 0.785 0.175 0.06\n",
      " 0.935 0.3   0.795 0.945 0.815 0.205 0.025 0.945 0.945 0.07  0.92  0.785\n",
      " 0.88  0.035 0.035 0.165 0.675 0.155 0.08  0.215 0.745 0.035 0.94  0.24\n",
      " 0.05  0.175 0.07  0.23  0.165 0.25  0.855 0.915 0.905 0.045 0.04  0.105\n",
      " 0.73  0.055 0.21  0.725 0.065 0.78  0.875 0.095 0.135 0.755 0.8   0.225\n",
      " 0.135 0.87  0.845 0.125 0.18  0.225 0.09  0.82  0.81  0.055 0.22  0.06\n",
      " 0.275 0.045 0.235 0.04  0.065 0.775 0.69  0.04  0.15  0.915 0.07  0.035\n",
      " 0.905 0.09  0.065 0.16  0.2   0.96  0.825 0.9   0.015 0.885 0.195 0.82\n",
      " 0.12  0.735 0.145 0.82  0.125 0.895 0.12  0.14  0.98  0.065 0.11  0.045\n",
      " 0.885 0.03  0.135 0.935 0.78  0.11  0.035 0.145 0.945 0.815 0.135 0.075\n",
      " 0.93  0.08  0.945 0.79  0.085 0.95  0.925 0.765 0.965 0.07  0.045 0.17\n",
      " 0.125 0.93  0.875 0.88  0.075 0.045 0.245 0.77  0.21  0.785 0.93  0.965\n",
      " 0.085 0.02  0.915 0.85  0.06  0.935 0.15  0.925 0.175 0.71  0.91  0.965\n",
      " 0.17  0.07  0.195 0.94  0.06  0.89  0.055 0.175 0.12  0.73  0.795 0.08\n",
      " 0.74  0.105 0.145 0.045 0.81  0.195 0.9   0.765]\n",
      "[0.88  0.9   0.945 0.85  0.17  0.1   0.75  0.045 0.175 0.765 0.025 0.815\n",
      " 0.945 0.075 0.965 0.795 0.055 0.055 0.085 0.76  0.025 0.125 0.24  0.78\n",
      " 0.74  0.095 0.09  0.94  0.96  0.105 0.905 0.215 0.065 0.77  0.14  0.05\n",
      " 0.945 0.2   0.865 0.905 0.805 0.275 0.03  0.945 0.95  0.055 0.93  0.725\n",
      " 0.9   0.025 0.03  0.2   0.795 0.235 0.075 0.15  0.715 0.035 0.945 0.29\n",
      " 0.06  0.105 0.11  0.21  0.195 0.195 0.92  0.91  0.945 0.05  0.02  0.08\n",
      " 0.655 0.045 0.185 0.765 0.11  0.78  0.895 0.08  0.195 0.73  0.785 0.245\n",
      " 0.135 0.95  0.845 0.11  0.16  0.205 0.06  0.8   0.88  0.065 0.2   0.08\n",
      " 0.195 0.125 0.27  0.085 0.1   0.825 0.715 0.05  0.225 0.86  0.08  0.05\n",
      " 0.905 0.07  0.09  0.225 0.2   0.955 0.785 0.885 0.025 0.93  0.205 0.77\n",
      " 0.105 0.805 0.13  0.93  0.08  0.895 0.095 0.08  0.955 0.035 0.115 0.04\n",
      " 0.905 0.04  0.095 0.95  0.81  0.165 0.07  0.15  0.98  0.8   0.05  0.04\n",
      " 0.965 0.095 0.91  0.76  0.135 0.895 0.87  0.145 0.84  0.01  0.885 0.93\n",
      " 0.94  0.805 0.105 0.93  0.895 0.905 0.155 0.05  0.055 0.91  0.955 0.875\n",
      " 0.11  0.94  0.12  0.91  0.22  0.03  0.055 0.08  0.935 0.105 0.855 0.815\n",
      " 0.075 0.085 0.085 0.055 0.105 0.12  0.915 0.935 0.75  0.905 0.78  0.865\n",
      " 0.775 0.84  0.9   0.945 0.095 0.18  0.69  0.925]\n",
      "[0.045 0.165 0.69  0.16  0.095 0.155 0.77  0.04  0.97  0.275 0.06  0.155\n",
      " 0.09  0.25  0.155 0.195 0.92  0.84  0.935 0.075 0.05  0.055 0.79  0.035\n",
      " 0.235 0.75  0.065 0.725 0.84  0.085 0.19  0.71  0.785 0.245 0.14  0.925\n",
      " 0.865 0.11  0.12  0.23  0.095 0.85  0.865 0.035 0.225 0.07  0.22  0.075\n",
      " 0.23  0.095 0.09  0.85  0.72  0.065 0.16  0.88  0.07  0.04  0.855 0.045\n",
      " 0.06  0.24  0.165 0.96  0.805 0.875 0.055 0.9   0.185 0.815 0.085 0.805\n",
      " 0.095 0.855 0.065 0.925 0.135 0.085 0.97  0.045 0.155 0.065 0.875 0.05\n",
      " 0.155 0.875 0.775 0.1   0.05  0.14  0.98  0.875 0.065 0.035 0.92  0.075\n",
      " 0.92  0.725 0.105 0.855 0.885 0.165 0.735 0.025 0.825 0.94  0.97  0.715\n",
      " 0.205 0.96  0.95  0.895 0.11  0.05  0.07  0.935 0.965 0.845 0.14  0.855\n",
      " 0.115 0.965 0.23  0.04  0.045 0.135 0.935 0.245 0.87  0.84  0.115 0.09\n",
      " 0.08  0.095 0.07  0.115 0.895 0.97  0.805 0.9   0.755 0.835 0.87  0.84\n",
      " 0.91  0.94  0.105 0.155 0.64  0.925 0.97  0.75  0.935 0.07  0.045 0.12\n",
      " 0.115 0.975 0.83  0.87  0.085 0.05  0.25  0.875 0.28  0.81  0.895 0.98\n",
      " 0.08  0.065 0.93  0.91  0.095 0.925 0.22  0.965 0.12  0.705 0.935 0.94\n",
      " 0.11  0.105 0.205 0.96  0.06  0.87  0.055 0.145 0.06  0.755 0.805 0.165\n",
      " 0.71  0.095 0.09  0.06  0.795 0.17  0.875 0.74 ]\n",
      "[0.885 0.885 0.98  0.86  0.16  0.145 0.815 0.05  0.18  0.78  0.08  0.81\n",
      " 0.97  0.05  0.925 0.85  0.105 0.025 0.08  0.88  0.045 0.21  0.35  0.705\n",
      " 0.91  0.09  0.13  0.97  0.95  0.095 0.88  0.225 0.025 0.855 0.24  0.07\n",
      " 0.965 0.305 0.845 0.96  0.86  0.22  0.04  0.975 0.96  0.04  0.975 0.74\n",
      " 0.915 0.045 0.06  0.905 0.745 0.09  0.22  0.925 0.05  0.065 0.91  0.105\n",
      " 0.085 0.19  0.215 0.98  0.85  0.91  0.03  0.96  0.18  0.835 0.125 0.815\n",
      " 0.18  0.87  0.1   0.9   0.125 0.135 0.95  0.055 0.125 0.035 0.89  0.03\n",
      " 0.08  0.965 0.83  0.18  0.06  0.135 0.965 0.81  0.1   0.075 0.94  0.105\n",
      " 0.955 0.74  0.1   0.915 0.96  0.16  0.71  0.055 0.93  0.945 0.97  0.79\n",
      " 0.135 0.955 0.915 0.9   0.175 0.055 0.045 0.985 0.99  0.885 0.14  0.9\n",
      " 0.12  0.965 0.225 0.05  0.02  0.065 0.985 0.265 0.86  0.92  0.055 0.055\n",
      " 0.05  0.095 0.075 0.115 0.945 0.96  0.775 0.84  0.88  0.845 0.785 0.865\n",
      " 0.96  0.965 0.06  0.24  0.63  0.985 0.96  0.725 0.935 0.1   0.05  0.16\n",
      " 0.21  0.995 0.92  0.875 0.105 0.035 0.24  0.82  0.3   0.82  0.915 0.99\n",
      " 0.07  0.025 0.965 0.955 0.08  0.945 0.275 0.975 0.185 0.77  0.95  0.975\n",
      " 0.16  0.095 0.255 0.97  0.02  0.945 0.06  0.17  0.065 0.78  0.77  0.175\n",
      " 0.795 0.14  0.22  0.02  0.895 0.21  0.915 0.785]\n",
      "[0.88  0.95  0.92  0.835 0.155 0.1   0.795 0.055 0.135 0.67  0.075 0.805\n",
      " 0.955 0.11  0.935 0.875 0.08  0.095 0.1   0.88  0.035 0.135 0.255 0.69\n",
      " 0.825 0.125 0.13  0.925 0.95  0.07  0.905 0.23  0.085 0.755 0.185 0.07\n",
      " 0.95  0.27  0.82  0.92  0.79  0.21  0.01  0.94  0.945 0.075 0.93  0.8\n",
      " 0.89  0.06  0.045 0.19  0.695 0.14  0.11  0.245 0.705 0.035 0.975 0.245\n",
      " 0.095 0.155 0.11  0.275 0.18  0.175 0.855 0.91  0.875 0.04  0.05  0.08\n",
      " 0.725 0.055 0.15  0.795 0.105 0.84  0.87  0.09  0.205 0.705 0.73  0.29\n",
      " 0.16  0.9   0.895 0.12  0.225 0.185 0.035 0.76  0.91  0.045 0.23  0.1\n",
      " 0.215 0.07  0.27  0.095 0.895 0.12  0.79  0.035 0.845 0.985 0.95  0.75\n",
      " 0.125 0.935 0.945 0.94  0.15  0.04  0.075 0.91  0.98  0.83  0.19  0.9\n",
      " 0.13  0.935 0.21  0.105 0.075 0.1   0.96  0.21  0.87  0.835 0.1   0.1\n",
      " 0.085 0.045 0.055 0.125 0.9   0.93  0.835 0.9   0.765 0.735 0.805 0.835\n",
      " 0.925 0.965 0.11  0.23  0.66  0.935 0.94  0.78  0.955 0.115 0.045 0.18\n",
      " 0.13  0.95  0.88  0.79  0.09  0.045 0.295 0.75  0.265 0.815 0.905 0.975\n",
      " 0.11  0.06  0.94  0.865 0.11  0.945 0.18  0.975 0.22  0.67  0.95  0.96\n",
      " 0.19  0.09  0.16  0.965 0.045 0.855 0.07  0.18  0.16  0.775 0.785 0.105\n",
      " 0.735 0.12  0.13  0.06  0.795 0.13  0.9   0.765]\n",
      "[0.845 0.925 0.955 0.81  0.13  0.03  0.79  0.075 0.14  0.675 0.04  0.77\n",
      " 0.965 0.095 0.925 0.865 0.09  0.07  0.055 0.85  0.025 0.135 0.25  0.715\n",
      " 0.835 0.06  0.15  0.94  0.925 0.07  0.875 0.195 0.04  0.85  0.195 0.07\n",
      " 0.905 0.265 0.745 0.935 0.82  0.275 0.03  0.935 0.95  0.04  0.92  0.71\n",
      " 0.915 0.035 0.03  0.165 0.74  0.23  0.075 0.22  0.76  0.07  0.96  0.255\n",
      " 0.045 0.13  0.1   0.245 0.215 0.165 0.91  0.865 0.935 0.05  0.03  0.075\n",
      " 0.725 0.06  0.225 0.76  0.085 0.79  0.875 0.07  0.13  0.68  0.795 0.265\n",
      " 0.085 0.935 0.83  0.1   0.22  0.225 0.085 0.805 0.825 0.05  0.205 0.055\n",
      " 0.205 0.055 0.29  0.075 0.06  0.73  0.715 0.05  0.235 0.93  0.06  0.03\n",
      " 0.885 0.12  0.045 0.22  0.21  0.975 0.8   0.87  0.025 0.9   0.185 0.845\n",
      " 0.105 0.7   0.105 0.81  0.105 0.875 0.07  0.095 0.96  0.045 0.12  0.05\n",
      " 0.865 0.02  0.15  0.95  0.795 0.135 0.06  0.215 0.95  0.78  0.1   0.06\n",
      " 0.925 0.065 0.925 0.695 0.105 0.88  0.9   0.74  0.93  0.055 0.06  0.17\n",
      " 0.1   0.94  0.91  0.925 0.065 0.065 0.235 0.845 0.235 0.82  0.92  0.95\n",
      " 0.095 0.025 0.955 0.825 0.105 0.93  0.155 0.935 0.165 0.67  0.91  0.955\n",
      " 0.135 0.08  0.25  0.955 0.09  0.865 0.055 0.1   0.095 0.755 0.775 0.115\n",
      " 0.775 0.11  0.19  0.035 0.795 0.165 0.895 0.79 ]\n",
      "[0.935 0.91  0.96  0.78  0.125 0.12  0.77  0.045 0.155 0.685 0.025 0.855\n",
      " 0.965 0.075 0.95  0.8   0.095 0.08  0.12  0.805 0.025 0.105 0.31  0.805\n",
      " 0.715 0.06  0.075 0.935 0.945 0.07  0.89  0.21  0.025 0.82  0.155 0.065\n",
      " 0.97  0.265 0.86  0.92  0.835 0.28  0.03  0.95  0.955 0.035 0.925 0.78\n",
      " 0.885 0.05  0.04  0.2   0.755 0.26  0.09  0.145 0.725 0.06  0.96  0.3\n",
      " 0.045 0.125 0.06  0.23  0.21  0.31  0.925 0.91  0.955 0.105 0.03  0.065\n",
      " 0.715 0.05  0.195 0.73  0.08  0.8   0.85  0.07  0.125 0.75  0.71  0.285\n",
      " 0.115 0.985 0.825 0.085 0.215 0.25  0.07  0.825 0.885 0.065 0.18  0.08\n",
      " 0.22  0.15  0.215 0.09  0.045 0.795 0.73  0.07  0.225 0.89  0.065 0.035\n",
      " 0.89  0.1   0.11  0.23  0.2   0.995 0.795 0.895 0.035 0.95  0.235 0.79\n",
      " 0.1   0.785 0.12  0.835 0.07  0.93  0.08  0.085 0.92  0.045 0.11  0.03\n",
      " 0.925 0.025 0.11  0.97  0.77  0.13  0.09  0.14  0.98  0.845 0.08  0.025\n",
      " 0.92  0.09  0.955 0.745 0.175 0.885 0.9   0.145 0.785 0.01  0.9   0.96\n",
      " 0.98  0.75  0.175 0.95  0.915 0.895 0.115 0.055 0.06  0.96  0.96  0.83\n",
      " 0.09  0.925 0.12  0.905 0.195 0.01  0.055 0.13  0.975 0.13  0.885 0.79\n",
      " 0.095 0.11  0.085 0.045 0.095 0.16  0.945 0.94  0.865 0.935 0.79  0.86\n",
      " 0.76  0.815 0.94  0.935 0.13  0.175 0.69  0.94 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.045 0.23  0.69  0.17  0.1   0.215 0.76  0.025 0.97  0.25  0.09  0.13\n",
      " 0.09  0.31  0.145 0.175 0.92  0.82  0.93  0.075 0.055 0.1   0.705 0.05\n",
      " 0.205 0.78  0.09  0.84  0.81  0.08  0.15  0.78  0.81  0.29  0.12  0.955\n",
      " 0.875 0.115 0.15  0.26  0.115 0.87  0.88  0.035 0.22  0.1   0.195 0.105\n",
      " 0.175 0.095 0.125 0.79  0.73  0.02  0.19  0.88  0.04  0.045 0.855 0.065\n",
      " 0.075 0.245 0.205 0.97  0.775 0.935 0.025 0.915 0.23  0.79  0.14  0.745\n",
      " 0.08  0.895 0.065 0.9   0.065 0.09  0.955 0.025 0.125 0.04  0.845 0.04\n",
      " 0.115 0.905 0.755 0.14  0.085 0.175 0.985 0.86  0.075 0.04  0.97  0.075\n",
      " 0.91  0.74  0.11  0.875 0.9   0.13  0.77  0.045 0.87  0.92  0.96  0.785\n",
      " 0.195 0.965 0.93  0.895 0.14  0.045 0.045 0.935 0.975 0.85  0.085 0.905\n",
      " 0.115 0.965 0.19  0.02  0.06  0.125 0.91  0.205 0.82  0.855 0.1   0.04\n",
      " 0.075 0.08  0.07  0.12  0.875 0.96  0.855 0.905 0.795 0.84  0.825 0.83\n",
      " 0.965 0.98  0.11  0.135 0.675 0.965 0.935 0.69  0.925 0.065 0.055 0.22\n",
      " 0.15  0.97  0.895 0.845 0.115 0.075 0.24  0.885 0.265 0.8   0.93  0.975\n",
      " 0.085 0.015 0.91  0.94  0.08  0.91  0.22  0.935 0.19  0.73  0.95  0.98\n",
      " 0.185 0.085 0.185 0.985 0.065 0.865 0.065 0.155 0.06  0.71  0.815 0.11\n",
      " 0.755 0.12  0.105 0.045 0.735 0.155 0.865 0.785]\n",
      "[0.855 0.92  0.97  0.88  0.18  0.09  0.765 0.045 0.215 0.775 0.015 0.81\n",
      " 0.985 0.07  0.905 0.845 0.07  0.07  0.035 0.86  0.035 0.155 0.32  0.77\n",
      " 0.84  0.11  0.1   0.975 0.96  0.075 0.875 0.21  0.02  0.82  0.22  0.055\n",
      " 0.965 0.32  0.81  0.93  0.845 0.21  0.04  0.97  0.98  0.04  0.975 0.725\n",
      " 0.945 0.05  0.09  0.85  0.735 0.075 0.21  0.94  0.06  0.035 0.905 0.15\n",
      " 0.06  0.19  0.235 0.98  0.86  0.945 0.015 0.975 0.25  0.835 0.15  0.75\n",
      " 0.15  0.87  0.095 0.86  0.155 0.135 0.995 0.03  0.06  0.04  0.92  0.005\n",
      " 0.09  0.95  0.8   0.165 0.075 0.16  0.955 0.78  0.13  0.06  0.96  0.06\n",
      " 0.96  0.725 0.075 0.91  0.935 0.265 0.795 0.045 0.935 0.965 0.995 0.79\n",
      " 0.11  0.94  0.925 0.915 0.175 0.065 0.045 0.96  0.98  0.895 0.14  0.855\n",
      " 0.06  0.965 0.265 0.03  0.04  0.075 0.965 0.22  0.875 0.885 0.07  0.05\n",
      " 0.07  0.055 0.08  0.1   0.97  0.955 0.73  0.815 0.86  0.835 0.83  0.845\n",
      " 0.95  0.95  0.09  0.19  0.68  0.975 0.965 0.735 0.925 0.07  0.03  0.165\n",
      " 0.19  1.    0.885 0.895 0.08  0.04  0.155 0.855 0.235 0.82  0.975 0.975\n",
      " 0.075 0.025 0.97  0.885 0.11  0.935 0.255 0.98  0.155 0.815 0.97  0.94\n",
      " 0.195 0.13  0.19  0.97  0.05  0.945 0.05  0.19  0.08  0.785 0.785 0.155\n",
      " 0.73  0.13  0.17  0.04  0.885 0.24  0.945 0.755]\n",
      "[0.89  0.905 0.89  0.785 0.12  0.09  0.72  0.06  0.105 0.71  0.045 0.84\n",
      " 0.94  0.09  0.87  0.815 0.085 0.1   0.09  0.78  0.03  0.185 0.265 0.76\n",
      " 0.8   0.1   0.195 0.965 0.97  0.055 0.89  0.22  0.045 0.805 0.19  0.095\n",
      " 0.935 0.26  0.76  0.955 0.76  0.19  0.03  0.965 0.96  0.065 0.955 0.77\n",
      " 0.89  0.075 0.065 0.15  0.73  0.175 0.08  0.175 0.73  0.055 0.96  0.3\n",
      " 0.075 0.115 0.085 0.225 0.225 0.215 0.885 0.875 0.92  0.04  0.06  0.085\n",
      " 0.725 0.08  0.175 0.715 0.095 0.85  0.865 0.15  0.195 0.76  0.79  0.325\n",
      " 0.15  0.92  0.855 0.12  0.22  0.275 0.045 0.775 0.855 0.02  0.22  0.16\n",
      " 0.16  0.11  0.255 0.08  0.86  0.13  0.745 0.055 0.85  0.945 0.965 0.71\n",
      " 0.14  0.965 0.94  0.92  0.1   0.035 0.08  0.9   0.96  0.86  0.14  0.905\n",
      " 0.14  0.96  0.255 0.08  0.07  0.085 0.94  0.195 0.875 0.805 0.07  0.13\n",
      " 0.085 0.06  0.085 0.13  0.905 0.895 0.835 0.86  0.815 0.78  0.83  0.815\n",
      " 0.93  0.965 0.065 0.23  0.73  0.93  0.94  0.67  0.89  0.09  0.085 0.175\n",
      " 0.215 0.985 0.84  0.86  0.09  0.055 0.27  0.82  0.285 0.76  0.9   0.945\n",
      " 0.085 0.045 0.93  0.875 0.09  0.96  0.225 0.94  0.19  0.715 0.95  0.95\n",
      " 0.155 0.07  0.205 0.935 0.045 0.855 0.065 0.2   0.18  0.755 0.795 0.085\n",
      " 0.785 0.085 0.19  0.1   0.8   0.175 0.915 0.775]\n",
      "[0.81  0.865 0.95  0.865 0.15  0.04  0.745 0.03  0.14  0.73  0.03  0.82\n",
      " 0.98  0.06  0.915 0.84  0.13  0.085 0.055 0.855 0.05  0.215 0.24  0.69\n",
      " 0.82  0.08  0.155 0.9   0.965 0.08  0.87  0.185 0.045 0.825 0.155 0.09\n",
      " 0.9   0.265 0.785 0.95  0.82  0.21  0.04  0.95  0.93  0.05  0.89  0.74\n",
      " 0.925 0.015 0.02  0.15  0.735 0.21  0.085 0.21  0.825 0.03  0.945 0.275\n",
      " 0.105 0.135 0.08  0.255 0.195 0.15  0.885 0.9   0.935 0.06  0.03  0.065\n",
      " 0.695 0.06  0.16  0.75  0.095 0.715 0.865 0.035 0.15  0.75  0.745 0.28\n",
      " 0.17  0.965 0.855 0.17  0.21  0.19  0.085 0.835 0.815 0.035 0.205 0.095\n",
      " 0.23  0.06  0.265 0.07  0.05  0.75  0.73  0.045 0.135 0.875 0.08  0.045\n",
      " 0.875 0.14  0.035 0.13  0.215 0.975 0.815 0.885 0.035 0.86  0.195 0.795\n",
      " 0.11  0.785 0.155 0.81  0.11  0.855 0.09  0.075 0.93  0.025 0.13  0.05\n",
      " 0.895 0.035 0.125 0.935 0.805 0.16  0.09  0.13  0.965 0.84  0.09  0.07\n",
      " 0.955 0.05  0.945 0.74  0.055 0.92  0.925 0.765 0.94  0.06  0.1   0.14\n",
      " 0.105 0.93  0.875 0.87  0.07  0.02  0.255 0.8   0.23  0.79  0.915 0.965\n",
      " 0.04  0.    0.955 0.89  0.075 0.925 0.12  0.93  0.18  0.755 0.915 0.96\n",
      " 0.19  0.07  0.195 0.935 0.035 0.83  0.06  0.15  0.11  0.78  0.815 0.085\n",
      " 0.79  0.145 0.215 0.05  0.835 0.165 0.9   0.755]\n",
      "[0.895 0.92  0.94  0.85  0.155 0.075 0.785 0.045 0.215 0.74  0.035 0.815\n",
      " 0.945 0.1   0.95  0.79  0.04  0.04  0.07  0.85  0.05  0.165 0.305 0.795\n",
      " 0.735 0.095 0.145 0.94  0.97  0.08  0.865 0.225 0.03  0.79  0.13  0.08\n",
      " 0.95  0.24  0.82  0.9   0.8   0.255 0.035 0.93  0.96  0.05  0.93  0.73\n",
      " 0.86  0.04  0.02  0.16  0.745 0.225 0.06  0.145 0.71  0.05  0.95  0.26\n",
      " 0.06  0.1   0.13  0.25  0.205 0.195 0.91  0.91  0.955 0.12  0.025 0.075\n",
      " 0.73  0.07  0.14  0.745 0.055 0.8   0.825 0.07  0.205 0.775 0.765 0.215\n",
      " 0.115 0.95  0.835 0.105 0.19  0.19  0.03  0.815 0.83  0.035 0.2   0.075\n",
      " 0.18  0.105 0.3   0.1   0.1   0.795 0.725 0.055 0.175 0.885 0.085 0.055\n",
      " 0.88  0.07  0.085 0.23  0.17  0.98  0.805 0.94  0.055 0.945 0.205 0.77\n",
      " 0.06  0.78  0.145 0.85  0.07  0.855 0.065 0.1   0.955 0.01  0.1   0.045\n",
      " 0.92  0.07  0.105 0.95  0.765 0.165 0.05  0.15  0.94  0.835 0.045 0.06\n",
      " 0.885 0.115 0.92  0.73  0.09  0.9   0.885 0.16  0.835 0.025 0.895 0.95\n",
      " 0.98  0.76  0.165 0.935 0.915 0.885 0.16  0.055 0.065 0.91  0.96  0.885\n",
      " 0.08  0.865 0.095 0.92  0.24  0.06  0.085 0.055 0.955 0.14  0.86  0.825\n",
      " 0.06  0.095 0.08  0.075 0.085 0.11  0.915 0.925 0.795 0.9   0.755 0.855\n",
      " 0.81  0.8   0.905 0.92  0.15  0.2   0.67  0.925]\n",
      "[0.035 0.16  0.75  0.2   0.075 0.22  0.74  0.035 0.96  0.255 0.08  0.115\n",
      " 0.125 0.2   0.185 0.175 0.945 0.835 0.93  0.08  0.045 0.085 0.75  0.015\n",
      " 0.225 0.74  0.105 0.815 0.8   0.065 0.125 0.775 0.76  0.26  0.14  0.965\n",
      " 0.885 0.105 0.125 0.275 0.125 0.82  0.9   0.05  0.215 0.11  0.205 0.085\n",
      " 0.225 0.08  0.105 0.83  0.77  0.06  0.235 0.9   0.045 0.045 0.88  0.08\n",
      " 0.08  0.215 0.245 0.96  0.8   0.88  0.055 0.925 0.205 0.78  0.105 0.75\n",
      " 0.105 0.895 0.05  0.93  0.085 0.09  0.95  0.03  0.115 0.07  0.885 0.05\n",
      " 0.085 0.865 0.775 0.14  0.055 0.145 0.995 0.82  0.045 0.03  0.92  0.085\n",
      " 0.94  0.78  0.075 0.905 0.925 0.115 0.72  0.    0.845 0.92  0.955 0.765\n",
      " 0.155 0.94  0.905 0.895 0.105 0.02  0.08  0.95  0.965 0.88  0.09  0.84\n",
      " 0.105 0.955 0.225 0.025 0.04  0.055 0.945 0.205 0.875 0.865 0.045 0.06\n",
      " 0.075 0.09  0.035 0.105 0.8   0.95  0.8   0.905 0.74  0.84  0.86  0.76\n",
      " 0.935 0.92  0.155 0.2   0.64  0.895 0.935 0.74  0.955 0.12  0.055 0.17\n",
      " 0.135 0.965 0.88  0.875 0.075 0.04  0.305 0.845 0.23  0.78  0.875 0.945\n",
      " 0.085 0.03  0.915 0.905 0.09  0.93  0.205 0.945 0.135 0.715 0.965 0.985\n",
      " 0.175 0.085 0.165 0.97  0.08  0.86  0.05  0.145 0.125 0.79  0.845 0.155\n",
      " 0.74  0.115 0.12  0.045 0.77  0.14  0.9   0.76 ]\n",
      "[0.845 0.91  0.975 0.905 0.2   0.045 0.835 0.045 0.23  0.765 0.035 0.76\n",
      " 0.975 0.05  0.91  0.835 0.07  0.04  0.065 0.855 0.035 0.185 0.34  0.78\n",
      " 0.89  0.075 0.125 0.965 0.95  0.085 0.89  0.24  0.025 0.75  0.205 0.065\n",
      " 0.98  0.285 0.8   0.99  0.865 0.24  0.04  0.955 0.945 0.095 0.975 0.71\n",
      " 0.94  0.025 0.065 0.88  0.72  0.085 0.14  0.935 0.04  0.03  0.905 0.135\n",
      " 0.045 0.15  0.265 0.985 0.83  0.94  0.025 0.98  0.18  0.835 0.11  0.825\n",
      " 0.185 0.87  0.09  0.93  0.095 0.07  0.985 0.04  0.095 0.035 0.915 0.015\n",
      " 0.125 0.975 0.745 0.185 0.06  0.165 0.97  0.785 0.09  0.05  0.97  0.045\n",
      " 0.945 0.785 0.095 0.94  0.935 0.18  0.76  0.03  0.935 0.955 0.985 0.77\n",
      " 0.15  0.95  0.905 0.935 0.175 0.03  0.025 0.98  0.985 0.905 0.16  0.855\n",
      " 0.06  0.965 0.21  0.02  0.06  0.1   0.97  0.19  0.89  0.9   0.045 0.055\n",
      " 0.06  0.1   0.085 0.09  0.955 0.96  0.76  0.845 0.845 0.855 0.81  0.88\n",
      " 0.965 0.925 0.085 0.155 0.705 0.97  0.93  0.705 0.955 0.105 0.045 0.17\n",
      " 0.15  0.975 0.9   0.865 0.1   0.055 0.18  0.85  0.26  0.88  0.935 0.97\n",
      " 0.08  0.025 0.97  0.935 0.07  0.945 0.285 0.98  0.175 0.815 0.975 0.95\n",
      " 0.155 0.09  0.22  0.98  0.035 0.945 0.055 0.185 0.12  0.8   0.81  0.14\n",
      " 0.8   0.175 0.21  0.03  0.87  0.19  0.925 0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88  0.89  0.9   0.8   0.16  0.105 0.75  0.045 0.15  0.725 0.06  0.88\n",
      " 0.92  0.115 0.935 0.815 0.09  0.05  0.085 0.865 0.045 0.18  0.33  0.785\n",
      " 0.78  0.135 0.165 0.945 0.96  0.05  0.895 0.245 0.035 0.85  0.2   0.095\n",
      " 0.955 0.235 0.835 0.95  0.83  0.195 0.02  0.905 0.97  0.05  0.92  0.725\n",
      " 0.885 0.055 0.025 0.195 0.675 0.195 0.105 0.195 0.715 0.045 0.955 0.305\n",
      " 0.08  0.16  0.075 0.195 0.19  0.255 0.93  0.89  0.915 0.05  0.07  0.065\n",
      " 0.735 0.055 0.185 0.76  0.095 0.8   0.87  0.11  0.185 0.75  0.845 0.255\n",
      " 0.115 0.905 0.875 0.135 0.255 0.27  0.075 0.8   0.86  0.05  0.185 0.13\n",
      " 0.21  0.065 0.255 0.1   0.91  0.14  0.73  0.025 0.885 0.96  0.97  0.81\n",
      " 0.155 0.96  0.95  0.915 0.12  0.09  0.055 0.93  0.97  0.89  0.15  0.905\n",
      " 0.165 0.955 0.245 0.08  0.045 0.05  0.965 0.17  0.815 0.825 0.055 0.135\n",
      " 0.12  0.075 0.075 0.135 0.88  0.93  0.825 0.89  0.76  0.865 0.75  0.82\n",
      " 0.95  0.94  0.13  0.205 0.725 0.89  0.945 0.735 0.955 0.115 0.105 0.145\n",
      " 0.185 0.965 0.865 0.835 0.095 0.065 0.285 0.84  0.31  0.835 0.935 0.94\n",
      " 0.1   0.055 0.895 0.835 0.075 0.975 0.21  0.955 0.135 0.725 0.915 0.975\n",
      " 0.16  0.08  0.195 0.965 0.11  0.87  0.06  0.16  0.14  0.78  0.78  0.14\n",
      " 0.755 0.145 0.15  0.03  0.77  0.17  0.95  0.8  ]\n",
      "[0.875 0.935 0.965 0.82  0.175 0.05  0.765 0.05  0.1   0.69  0.05  0.795\n",
      " 0.97  0.1   0.885 0.87  0.125 0.035 0.06  0.845 0.025 0.17  0.265 0.71\n",
      " 0.82  0.04  0.125 0.91  0.96  0.085 0.86  0.22  0.025 0.825 0.21  0.085\n",
      " 0.94  0.25  0.795 0.905 0.865 0.265 0.025 0.93  0.94  0.07  0.92  0.745\n",
      " 0.9   0.03  0.055 0.12  0.78  0.155 0.095 0.23  0.795 0.045 0.95  0.265\n",
      " 0.085 0.16  0.09  0.215 0.235 0.195 0.915 0.89  0.865 0.07  0.035 0.105\n",
      " 0.735 0.04  0.19  0.805 0.075 0.785 0.85  0.07  0.175 0.715 0.76  0.21\n",
      " 0.14  0.94  0.82  0.12  0.175 0.255 0.04  0.765 0.835 0.085 0.23  0.055\n",
      " 0.225 0.055 0.25  0.07  0.05  0.71  0.69  0.06  0.185 0.895 0.08  0.045\n",
      " 0.865 0.11  0.04  0.15  0.2   0.95  0.845 0.87  0.01  0.905 0.26  0.845\n",
      " 0.105 0.745 0.17  0.82  0.11  0.905 0.08  0.11  0.92  0.015 0.08  0.04\n",
      " 0.885 0.05  0.2   0.925 0.795 0.16  0.085 0.165 0.96  0.835 0.07  0.07\n",
      " 0.91  0.075 0.925 0.7   0.145 0.865 0.925 0.75  0.935 0.095 0.055 0.19\n",
      " 0.15  0.925 0.875 0.87  0.06  0.055 0.22  0.86  0.28  0.775 0.955 0.94\n",
      " 0.095 0.03  0.95  0.83  0.06  0.9   0.13  0.92  0.16  0.73  0.925 0.94\n",
      " 0.15  0.115 0.19  0.955 0.055 0.85  0.075 0.08  0.105 0.795 0.77  0.075\n",
      " 0.81  0.15  0.135 0.04  0.795 0.155 0.905 0.78 ]\n",
      "[0.915 0.9   0.94  0.885 0.145 0.115 0.75  0.06  0.21  0.73  0.065 0.835\n",
      " 0.985 0.075 0.945 0.83  0.06  0.04  0.07  0.805 0.035 0.15  0.245 0.77\n",
      " 0.79  0.105 0.175 0.9   0.975 0.06  0.875 0.255 0.045 0.82  0.17  0.105\n",
      " 0.96  0.24  0.8   0.945 0.865 0.26  0.035 0.97  0.965 0.065 0.95  0.805\n",
      " 0.88  0.055 0.03  0.22  0.78  0.215 0.095 0.13  0.685 0.055 0.955 0.285\n",
      " 0.04  0.125 0.12  0.255 0.225 0.28  0.93  0.89  0.935 0.09  0.025 0.045\n",
      " 0.75  0.035 0.145 0.765 0.06  0.8   0.88  0.09  0.215 0.73  0.805 0.29\n",
      " 0.13  0.94  0.9   0.105 0.2   0.245 0.055 0.85  0.835 0.04  0.21  0.11\n",
      " 0.22  0.1   0.19  0.1   0.105 0.845 0.755 0.085 0.25  0.905 0.055 0.04\n",
      " 0.905 0.08  0.07  0.2   0.255 0.975 0.79  0.88  0.03  0.935 0.2   0.785\n",
      " 0.095 0.805 0.155 0.885 0.035 0.915 0.105 0.18  0.945 0.05  0.11  0.06\n",
      " 0.94  0.025 0.065 0.965 0.785 0.14  0.075 0.15  0.965 0.825 0.045 0.03\n",
      " 0.915 0.125 0.94  0.735 0.135 0.905 0.91  0.105 0.825 0.04  0.92  0.925\n",
      " 0.97  0.735 0.14  0.975 0.93  0.925 0.075 0.055 0.035 0.94  0.96  0.9\n",
      " 0.07  0.93  0.1   0.94  0.2   0.05  0.04  0.06  0.94  0.145 0.91  0.835\n",
      " 0.07  0.085 0.09  0.07  0.095 0.13  0.92  0.93  0.795 0.895 0.795 0.885\n",
      " 0.785 0.82  0.935 0.915 0.165 0.155 0.685 0.9  ]\n",
      "[0.03  0.225 0.71  0.2   0.115 0.24  0.835 0.05  0.985 0.3   0.055 0.14\n",
      " 0.07  0.245 0.185 0.16  0.93  0.805 0.91  0.1   0.065 0.085 0.77  0.035\n",
      " 0.23  0.755 0.095 0.73  0.855 0.08  0.135 0.8   0.71  0.255 0.105 0.91\n",
      " 0.865 0.115 0.145 0.23  0.095 0.85  0.855 0.065 0.255 0.075 0.175 0.07\n",
      " 0.145 0.07  0.09  0.83  0.745 0.065 0.19  0.86  0.075 0.035 0.845 0.065\n",
      " 0.065 0.17  0.21  0.955 0.875 0.92  0.035 0.93  0.22  0.785 0.075 0.81\n",
      " 0.12  0.885 0.065 0.915 0.085 0.085 0.95  0.04  0.13  0.045 0.88  0.035\n",
      " 0.09  0.925 0.79  0.12  0.095 0.14  0.96  0.855 0.07  0.045 0.9   0.055\n",
      " 0.89  0.675 0.095 0.84  0.915 0.13  0.72  0.01  0.875 0.935 0.99  0.695\n",
      " 0.165 0.93  0.94  0.895 0.135 0.025 0.035 0.94  0.975 0.88  0.105 0.89\n",
      " 0.12  0.96  0.21  0.03  0.055 0.155 0.945 0.24  0.84  0.835 0.095 0.06\n",
      " 0.075 0.09  0.04  0.105 0.825 0.925 0.82  0.875 0.785 0.855 0.825 0.83\n",
      " 0.945 0.96  0.075 0.13  0.665 0.945 0.96  0.735 0.935 0.05  0.05  0.16\n",
      " 0.12  0.975 0.915 0.87  0.095 0.065 0.27  0.815 0.3   0.79  0.86  0.955\n",
      " 0.075 0.025 0.885 0.94  0.085 0.88  0.12  0.965 0.13  0.685 0.95  0.975\n",
      " 0.115 0.065 0.175 0.96  0.07  0.89  0.055 0.165 0.12  0.8   0.805 0.175\n",
      " 0.71  0.08  0.115 0.03  0.78  0.14  0.855 0.835]\n",
      "[0.875 0.92  0.965 0.87  0.155 0.07  0.775 0.03  0.24  0.745 0.02  0.78\n",
      " 0.97  0.06  0.94  0.905 0.09  0.04  0.035 0.905 0.035 0.195 0.345 0.835\n",
      " 0.91  0.115 0.135 0.98  0.965 0.035 0.86  0.23  0.04  0.76  0.24  0.035\n",
      " 0.965 0.225 0.805 0.96  0.87  0.245 0.025 0.97  0.97  0.07  0.985 0.75\n",
      " 0.89  0.025 0.08  0.835 0.8   0.065 0.17  0.95  0.045 0.04  0.9   0.12\n",
      " 0.03  0.14  0.255 0.98  0.865 0.965 0.01  0.99  0.15  0.805 0.12  0.805\n",
      " 0.185 0.9   0.095 0.9   0.115 0.085 0.965 0.045 0.09  0.03  0.905 0.02\n",
      " 0.135 0.985 0.795 0.175 0.07  0.12  0.965 0.785 0.06  0.03  0.955 0.045\n",
      " 0.975 0.755 0.085 0.915 0.945 0.175 0.795 0.04  0.925 0.96  0.97  0.745\n",
      " 0.155 0.98  0.925 0.915 0.185 0.035 0.06  0.97  0.985 0.84  0.07  0.87\n",
      " 0.05  0.975 0.28  0.04  0.015 0.07  0.955 0.245 0.85  0.905 0.055 0.06\n",
      " 0.065 0.07  0.08  0.12  0.955 0.965 0.765 0.825 0.84  0.815 0.84  0.875\n",
      " 0.95  0.95  0.075 0.185 0.65  0.965 0.94  0.715 0.93  0.085 0.045 0.18\n",
      " 0.15  0.97  0.885 0.885 0.11  0.025 0.22  0.845 0.325 0.855 0.905 0.985\n",
      " 0.05  0.01  0.95  0.94  0.075 0.975 0.245 0.955 0.18  0.765 0.98  0.975\n",
      " 0.255 0.11  0.14  0.965 0.045 0.95  0.06  0.195 0.095 0.78  0.785 0.215\n",
      " 0.805 0.105 0.225 0.005 0.835 0.195 0.96  0.81 ]\n",
      "[0.865 0.935 0.89  0.8   0.165 0.1   0.78  0.045 0.13  0.69  0.075 0.82\n",
      " 0.97  0.09  0.93  0.845 0.11  0.07  0.09  0.865 0.04  0.145 0.235 0.795\n",
      " 0.735 0.07  0.1   0.925 0.965 0.075 0.9   0.25  0.06  0.82  0.185 0.06\n",
      " 0.95  0.235 0.845 0.94  0.78  0.195 0.02  0.96  0.98  0.07  0.895 0.775\n",
      " 0.95  0.055 0.035 0.215 0.715 0.155 0.1   0.16  0.79  0.06  0.96  0.27\n",
      " 0.075 0.14  0.11  0.195 0.17  0.19  0.89  0.905 0.91  0.08  0.015 0.035\n",
      " 0.77  0.07  0.225 0.72  0.1   0.82  0.85  0.14  0.165 0.78  0.765 0.255\n",
      " 0.155 0.935 0.845 0.105 0.245 0.26  0.04  0.775 0.855 0.06  0.24  0.115\n",
      " 0.2   0.135 0.3   0.075 0.85  0.145 0.84  0.015 0.885 0.955 0.975 0.68\n",
      " 0.16  0.975 0.95  0.94  0.15  0.07  0.09  0.9   0.945 0.845 0.13  0.88\n",
      " 0.115 0.975 0.2   0.055 0.07  0.07  0.95  0.19  0.84  0.815 0.05  0.1\n",
      " 0.095 0.12  0.09  0.125 0.85  0.925 0.805 0.885 0.79  0.78  0.79  0.835\n",
      " 0.92  0.935 0.105 0.165 0.645 0.89  0.89  0.785 0.95  0.115 0.06  0.21\n",
      " 0.155 0.93  0.87  0.845 0.105 0.055 0.26  0.835 0.32  0.855 0.93  0.955\n",
      " 0.095 0.04  0.905 0.875 0.115 0.95  0.195 0.955 0.19  0.73  0.95  0.96\n",
      " 0.205 0.09  0.24  0.945 0.1   0.845 0.05  0.195 0.15  0.815 0.78  0.11\n",
      " 0.75  0.19  0.16  0.045 0.77  0.145 0.91  0.795]\n",
      "[0.845 0.91  0.95  0.865 0.21  0.075 0.735 0.095 0.125 0.73  0.055 0.78\n",
      " 0.965 0.07  0.91  0.86  0.155 0.045 0.075 0.855 0.035 0.175 0.28  0.69\n",
      " 0.845 0.085 0.135 0.97  0.955 0.08  0.9   0.155 0.03  0.77  0.2   0.08\n",
      " 0.94  0.235 0.755 0.96  0.845 0.23  0.025 0.925 0.955 0.08  0.945 0.745\n",
      " 0.88  0.045 0.035 0.135 0.73  0.16  0.055 0.2   0.77  0.045 0.98  0.295\n",
      " 0.1   0.16  0.065 0.295 0.185 0.185 0.955 0.87  0.92  0.03  0.035 0.075\n",
      " 0.73  0.03  0.23  0.765 0.095 0.805 0.83  0.105 0.1   0.76  0.735 0.26\n",
      " 0.13  0.915 0.845 0.15  0.2   0.315 0.08  0.745 0.815 0.04  0.155 0.045\n",
      " 0.215 0.05  0.25  0.075 0.06  0.75  0.74  0.06  0.17  0.93  0.055 0.05\n",
      " 0.86  0.095 0.045 0.16  0.225 0.965 0.785 0.875 0.035 0.925 0.245 0.77\n",
      " 0.155 0.75  0.125 0.85  0.125 0.845 0.115 0.1   0.955 0.025 0.1   0.05\n",
      " 0.905 0.02  0.165 0.94  0.755 0.21  0.065 0.145 0.975 0.835 0.105 0.06\n",
      " 0.93  0.03  0.965 0.71  0.135 0.885 0.93  0.725 0.905 0.08  0.08  0.165\n",
      " 0.105 0.95  0.875 0.85  0.11  0.03  0.275 0.825 0.285 0.83  0.89  0.965\n",
      " 0.075 0.02  0.955 0.83  0.11  0.89  0.17  0.91  0.14  0.7   0.955 0.97\n",
      " 0.165 0.105 0.205 0.945 0.05  0.79  0.06  0.135 0.13  0.77  0.81  0.065\n",
      " 0.775 0.135 0.16  0.055 0.75  0.14  0.875 0.755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.915 0.875 0.95  0.785 0.185 0.075 0.78  0.05  0.195 0.735 0.03  0.82\n",
      " 0.95  0.065 0.95  0.825 0.035 0.085 0.07  0.825 0.03  0.175 0.25  0.77\n",
      " 0.785 0.07  0.11  0.935 0.965 0.06  0.905 0.31  0.09  0.745 0.15  0.085\n",
      " 0.925 0.27  0.82  0.925 0.86  0.28  0.03  0.965 0.95  0.05  0.89  0.76\n",
      " 0.89  0.06  0.025 0.155 0.77  0.255 0.12  0.165 0.7   0.04  0.96  0.265\n",
      " 0.075 0.115 0.09  0.28  0.23  0.205 0.93  0.945 0.945 0.085 0.04  0.055\n",
      " 0.76  0.065 0.195 0.815 0.07  0.79  0.85  0.07  0.175 0.8   0.795 0.285\n",
      " 0.155 0.925 0.855 0.125 0.18  0.25  0.065 0.81  0.935 0.03  0.205 0.105\n",
      " 0.215 0.105 0.29  0.045 0.05  0.82  0.765 0.05  0.225 0.925 0.07  0.015\n",
      " 0.845 0.09  0.09  0.225 0.215 0.97  0.8   0.905 0.035 0.93  0.165 0.775\n",
      " 0.075 0.775 0.095 0.88  0.085 0.89  0.085 0.15  0.955 0.    0.11  0.015\n",
      " 0.915 0.04  0.08  0.965 0.76  0.15  0.07  0.135 0.955 0.805 0.085 0.065\n",
      " 0.915 0.095 0.945 0.765 0.155 0.915 0.88  0.18  0.755 0.015 0.91  0.945\n",
      " 0.94  0.81  0.155 0.955 0.925 0.93  0.085 0.045 0.085 0.96  0.97  0.865\n",
      " 0.105 0.905 0.075 0.93  0.235 0.035 0.06  0.05  0.955 0.16  0.835 0.815\n",
      " 0.085 0.135 0.125 0.04  0.095 0.135 0.935 0.895 0.845 0.915 0.755 0.855\n",
      " 0.77  0.835 0.965 0.95  0.105 0.175 0.655 0.875]\n",
      "[0.075 0.215 0.76  0.22  0.075 0.19  0.805 0.01  0.96  0.25  0.055 0.125\n",
      " 0.085 0.29  0.175 0.135 0.95  0.855 0.945 0.1   0.065 0.075 0.755 0.045\n",
      " 0.225 0.74  0.09  0.805 0.83  0.095 0.155 0.73  0.745 0.275 0.13  0.93\n",
      " 0.83  0.105 0.17  0.235 0.065 0.855 0.855 0.07  0.245 0.08  0.255 0.065\n",
      " 0.195 0.12  0.125 0.8   0.76  0.09  0.175 0.915 0.085 0.04  0.87  0.08\n",
      " 0.04  0.21  0.235 0.965 0.86  0.9   0.05  0.905 0.16  0.785 0.095 0.755\n",
      " 0.135 0.895 0.045 0.905 0.075 0.09  0.97  0.085 0.11  0.08  0.905 0.07\n",
      " 0.145 0.89  0.8   0.12  0.04  0.18  0.985 0.825 0.08  0.06  0.945 0.065\n",
      " 0.93  0.715 0.055 0.905 0.915 0.125 0.755 0.02  0.825 0.925 0.99  0.745\n",
      " 0.13  0.965 0.915 0.9   0.125 0.06  0.05  0.965 0.99  0.865 0.095 0.88\n",
      " 0.115 0.99  0.225 0.035 0.035 0.125 0.955 0.245 0.88  0.86  0.1   0.115\n",
      " 0.065 0.08  0.045 0.125 0.84  0.94  0.825 0.87  0.8   0.865 0.875 0.83\n",
      " 0.945 0.92  0.085 0.21  0.67  0.95  0.94  0.685 0.935 0.065 0.05  0.165\n",
      " 0.12  0.96  0.865 0.85  0.11  0.07  0.31  0.875 0.305 0.775 0.86  0.97\n",
      " 0.075 0.045 0.915 0.915 0.105 0.955 0.175 0.96  0.105 0.69  0.92  0.96\n",
      " 0.135 0.08  0.165 0.97  0.065 0.875 0.075 0.14  0.085 0.775 0.81  0.1\n",
      " 0.75  0.11  0.155 0.05  0.765 0.185 0.87  0.77 ]\n",
      "[0.875 0.895 0.965 0.895 0.175 0.125 0.805 0.07  0.245 0.77  0.035 0.77\n",
      " 0.995 0.055 0.87  0.84  0.05  0.055 0.075 0.89  0.03  0.195 0.32  0.79\n",
      " 0.87  0.095 0.14  0.985 0.935 0.06  0.83  0.205 0.035 0.845 0.235 0.065\n",
      " 0.93  0.295 0.76  0.965 0.835 0.255 0.02  0.99  0.955 0.09  0.985 0.72\n",
      " 0.91  0.02  0.08  0.825 0.81  0.08  0.14  0.93  0.025 0.035 0.875 0.11\n",
      " 0.04  0.195 0.205 0.985 0.855 0.945 0.015 0.97  0.17  0.81  0.135 0.765\n",
      " 0.165 0.885 0.09  0.905 0.1   0.04  0.98  0.035 0.09  0.04  0.915 0.035\n",
      " 0.13  0.95  0.73  0.22  0.04  0.18  0.97  0.77  0.07  0.05  0.965 0.04\n",
      " 0.945 0.76  0.095 0.935 0.945 0.215 0.79  0.01  0.945 0.95  0.995 0.705\n",
      " 0.15  0.965 0.895 0.885 0.205 0.015 0.04  0.98  0.995 0.87  0.115 0.91\n",
      " 0.085 0.98  0.215 0.03  0.025 0.11  0.97  0.245 0.86  0.925 0.04  0.055\n",
      " 0.07  0.065 0.085 0.08  0.94  0.955 0.745 0.845 0.815 0.87  0.845 0.81\n",
      " 0.96  0.96  0.055 0.19  0.665 0.99  0.955 0.705 0.945 0.08  0.045 0.17\n",
      " 0.15  0.99  0.915 0.91  0.075 0.015 0.175 0.82  0.32  0.87  0.95  0.985\n",
      " 0.07  0.01  0.955 0.915 0.075 0.94  0.27  0.975 0.205 0.735 0.97  0.98\n",
      " 0.21  0.08  0.205 0.975 0.045 0.945 0.07  0.195 0.09  0.785 0.81  0.105\n",
      " 0.795 0.16  0.19  0.05  0.875 0.185 0.95  0.73 ]\n",
      "[0.89  0.885 0.93  0.855 0.135 0.14  0.745 0.015 0.155 0.71  0.085 0.825\n",
      " 0.95  0.11  0.895 0.775 0.06  0.05  0.1   0.805 0.085 0.19  0.245 0.74\n",
      " 0.82  0.135 0.145 0.915 0.97  0.085 0.91  0.2   0.055 0.795 0.165 0.12\n",
      " 0.95  0.24  0.82  0.925 0.825 0.225 0.02  0.93  0.955 0.07  0.93  0.775\n",
      " 0.89  0.045 0.025 0.245 0.71  0.21  0.13  0.16  0.73  0.035 0.96  0.275\n",
      " 0.055 0.13  0.07  0.25  0.155 0.255 0.88  0.925 0.895 0.045 0.055 0.08\n",
      " 0.715 0.075 0.215 0.73  0.045 0.74  0.88  0.085 0.195 0.76  0.79  0.315\n",
      " 0.145 0.92  0.855 0.06  0.24  0.215 0.065 0.8   0.88  0.075 0.235 0.09\n",
      " 0.19  0.105 0.23  0.08  0.84  0.075 0.715 0.04  0.885 0.97  0.98  0.775\n",
      " 0.13  0.985 0.935 0.885 0.125 0.1   0.07  0.97  0.98  0.855 0.115 0.85\n",
      " 0.12  0.96  0.215 0.065 0.04  0.06  0.96  0.16  0.865 0.845 0.085 0.095\n",
      " 0.075 0.08  0.055 0.165 0.865 0.92  0.81  0.885 0.765 0.81  0.8   0.84\n",
      " 0.935 0.96  0.1   0.185 0.685 0.93  0.96  0.7   0.925 0.1   0.095 0.13\n",
      " 0.18  0.94  0.85  0.85  0.08  0.075 0.25  0.795 0.265 0.765 0.9   0.94\n",
      " 0.115 0.015 0.88  0.88  0.115 0.94  0.21  0.975 0.13  0.69  0.905 0.945\n",
      " 0.16  0.07  0.27  0.95  0.075 0.85  0.05  0.13  0.12  0.74  0.755 0.125\n",
      " 0.71  0.14  0.135 0.065 0.78  0.185 0.905 0.79 ]\n",
      "[0.9   0.885 0.985 0.86  0.16  0.09  0.745 0.05  0.16  0.67  0.08  0.85\n",
      " 0.955 0.065 0.915 0.81  0.075 0.085 0.055 0.875 0.03  0.17  0.255 0.71\n",
      " 0.785 0.055 0.14  0.9   0.96  0.065 0.855 0.225 0.025 0.765 0.15  0.065\n",
      " 0.925 0.235 0.815 0.925 0.8   0.215 0.045 0.945 0.96  0.07  0.925 0.755\n",
      " 0.89  0.03  0.045 0.14  0.715 0.215 0.095 0.19  0.7   0.055 0.93  0.24\n",
      " 0.09  0.135 0.095 0.165 0.17  0.155 0.93  0.855 0.915 0.05  0.015 0.085\n",
      " 0.74  0.07  0.215 0.755 0.09  0.79  0.855 0.09  0.15  0.72  0.735 0.26\n",
      " 0.17  0.96  0.815 0.125 0.22  0.28  0.075 0.87  0.845 0.055 0.205 0.06\n",
      " 0.2   0.045 0.245 0.045 0.055 0.775 0.69  0.05  0.195 0.895 0.04  0.025\n",
      " 0.84  0.12  0.035 0.2   0.175 0.955 0.765 0.85  0.03  0.86  0.17  0.85\n",
      " 0.125 0.735 0.125 0.815 0.105 0.905 0.06  0.12  0.955 0.02  0.045 0.045\n",
      " 0.915 0.055 0.095 0.935 0.795 0.13  0.07  0.17  0.95  0.795 0.1   0.12\n",
      " 0.94  0.095 0.93  0.785 0.105 0.89  0.89  0.655 0.955 0.065 0.05  0.175\n",
      " 0.155 0.945 0.93  0.84  0.065 0.055 0.235 0.785 0.24  0.795 0.92  0.955\n",
      " 0.085 0.025 0.925 0.835 0.065 0.89  0.15  0.935 0.145 0.75  0.9   0.965\n",
      " 0.185 0.06  0.18  0.95  0.075 0.87  0.075 0.115 0.07  0.78  0.755 0.13\n",
      " 0.76  0.17  0.175 0.04  0.735 0.21  0.915 0.745]\n",
      "[0.885 0.935 0.93  0.81  0.14  0.11  0.75  0.07  0.195 0.705 0.06  0.86\n",
      " 0.935 0.095 0.965 0.785 0.045 0.075 0.08  0.83  0.04  0.185 0.25  0.785\n",
      " 0.74  0.09  0.11  0.97  0.975 0.065 0.91  0.265 0.035 0.755 0.185 0.065\n",
      " 0.945 0.24  0.84  0.905 0.84  0.225 0.045 0.925 0.975 0.09  0.935 0.75\n",
      " 0.86  0.06  0.03  0.18  0.785 0.24  0.1   0.19  0.725 0.07  0.97  0.27\n",
      " 0.065 0.16  0.125 0.245 0.245 0.225 0.94  0.935 0.94  0.09  0.02  0.095\n",
      " 0.735 0.06  0.2   0.725 0.08  0.745 0.92  0.08  0.22  0.76  0.795 0.335\n",
      " 0.14  0.95  0.855 0.1   0.165 0.19  0.05  0.85  0.88  0.065 0.195 0.08\n",
      " 0.245 0.095 0.25  0.06  0.1   0.825 0.775 0.06  0.25  0.88  0.065 0.035\n",
      " 0.89  0.095 0.08  0.225 0.215 0.945 0.765 0.89  0.03  0.89  0.21  0.78\n",
      " 0.09  0.83  0.12  0.865 0.105 0.865 0.125 0.11  0.94  0.02  0.155 0.035\n",
      " 0.88  0.045 0.09  0.945 0.755 0.15  0.08  0.135 0.965 0.845 0.085 0.075\n",
      " 0.94  0.105 0.92  0.73  0.095 0.925 0.925 0.14  0.81  0.04  0.875 0.97\n",
      " 0.96  0.745 0.145 0.955 0.93  0.905 0.11  0.055 0.05  0.915 0.97  0.845\n",
      " 0.06  0.885 0.085 0.935 0.225 0.02  0.075 0.08  0.96  0.165 0.88  0.82\n",
      " 0.105 0.09  0.09  0.075 0.115 0.135 0.93  0.95  0.81  0.96  0.775 0.785\n",
      " 0.795 0.835 0.915 0.95  0.125 0.19  0.66  0.92 ]\n",
      "[0.03  0.26  0.74  0.175 0.075 0.195 0.775 0.015 0.975 0.245 0.125 0.12\n",
      " 0.14  0.3   0.17  0.175 0.92  0.82  0.945 0.07  0.05  0.09  0.72  0.07\n",
      " 0.285 0.745 0.07  0.755 0.795 0.08  0.16  0.715 0.73  0.275 0.14  0.97\n",
      " 0.87  0.085 0.17  0.275 0.085 0.8   0.89  0.055 0.2   0.095 0.195 0.055\n",
      " 0.165 0.105 0.105 0.84  0.725 0.065 0.16  0.905 0.025 0.04  0.885 0.08\n",
      " 0.065 0.2   0.235 0.97  0.855 0.89  0.05  0.95  0.245 0.805 0.11  0.81\n",
      " 0.07  0.895 0.095 0.9   0.095 0.105 0.985 0.04  0.12  0.035 0.875 0.07\n",
      " 0.065 0.895 0.77  0.1   0.065 0.16  0.995 0.875 0.05  0.065 0.915 0.07\n",
      " 0.955 0.74  0.1   0.885 0.94  0.13  0.785 0.03  0.87  0.94  0.965 0.765\n",
      " 0.19  0.96  0.95  0.92  0.115 0.035 0.05  0.94  0.99  0.855 0.07  0.88\n",
      " 0.14  0.965 0.175 0.065 0.035 0.12  0.95  0.15  0.845 0.82  0.095 0.06\n",
      " 0.065 0.085 0.04  0.105 0.88  0.965 0.815 0.905 0.745 0.79  0.865 0.835\n",
      " 0.945 0.94  0.085 0.185 0.695 0.93  0.965 0.74  0.94  0.075 0.055 0.135\n",
      " 0.11  0.975 0.815 0.9   0.125 0.06  0.18  0.835 0.3   0.745 0.93  0.97\n",
      " 0.09  0.015 0.93  0.935 0.085 0.93  0.2   0.975 0.175 0.715 0.97  0.975\n",
      " 0.14  0.03  0.125 0.955 0.085 0.87  0.06  0.16  0.125 0.805 0.795 0.105\n",
      " 0.755 0.105 0.15  0.065 0.78  0.16  0.91  0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9   0.91  0.96  0.83  0.155 0.125 0.81  0.045 0.19  0.785 0.03  0.855\n",
      " 0.965 0.09  0.895 0.875 0.065 0.05  0.025 0.89  0.04  0.18  0.305 0.785\n",
      " 0.865 0.125 0.1   0.975 0.955 0.055 0.84  0.195 0.045 0.77  0.275 0.07\n",
      " 0.96  0.27  0.805 0.95  0.85  0.245 0.03  0.95  0.96  0.095 0.945 0.765\n",
      " 0.88  0.045 0.075 0.845 0.745 0.065 0.17  0.895 0.035 0.035 0.885 0.075\n",
      " 0.06  0.16  0.22  0.98  0.78  0.94  0.02  0.97  0.13  0.855 0.14  0.795\n",
      " 0.16  0.88  0.065 0.92  0.09  0.1   0.965 0.035 0.08  0.035 0.895 0.02\n",
      " 0.125 0.96  0.815 0.14  0.06  0.185 0.97  0.785 0.065 0.055 0.965 0.05\n",
      " 0.955 0.83  0.095 0.91  0.935 0.22  0.805 0.035 0.93  0.955 0.985 0.745\n",
      " 0.155 0.97  0.935 0.92  0.17  0.065 0.045 0.985 0.98  0.865 0.075 0.905\n",
      " 0.12  0.965 0.235 0.035 0.03  0.105 0.98  0.195 0.835 0.9   0.04  0.06\n",
      " 0.065 0.11  0.065 0.14  0.985 0.96  0.715 0.82  0.845 0.84  0.89  0.895\n",
      " 0.915 0.955 0.065 0.21  0.6   0.985 0.955 0.705 0.955 0.06  0.04  0.225\n",
      " 0.175 0.98  0.905 0.865 0.105 0.03  0.2   0.87  0.23  0.855 0.94  0.97\n",
      " 0.075 0.025 0.96  0.92  0.105 0.945 0.26  0.975 0.19  0.775 0.96  0.98\n",
      " 0.125 0.095 0.195 0.97  0.03  0.935 0.075 0.195 0.075 0.84  0.74  0.135\n",
      " 0.74  0.115 0.125 0.045 0.905 0.22  0.955 0.82 ]\n",
      "[0.87  0.9   0.895 0.845 0.17  0.09  0.735 0.045 0.125 0.725 0.055 0.845\n",
      " 0.96  0.075 0.895 0.81  0.085 0.09  0.07  0.835 0.065 0.16  0.26  0.725\n",
      " 0.835 0.12  0.125 0.965 0.975 0.09  0.905 0.195 0.035 0.775 0.205 0.07\n",
      " 0.95  0.28  0.875 0.925 0.78  0.235 0.03  0.935 0.915 0.065 0.95  0.8\n",
      " 0.885 0.06  0.05  0.18  0.72  0.17  0.085 0.2   0.75  0.03  0.975 0.265\n",
      " 0.06  0.15  0.12  0.23  0.175 0.18  0.9   0.885 0.9   0.04  0.055 0.055\n",
      " 0.705 0.06  0.145 0.74  0.105 0.82  0.87  0.105 0.16  0.76  0.76  0.26\n",
      " 0.13  0.955 0.81  0.12  0.215 0.25  0.09  0.85  0.87  0.08  0.22  0.12\n",
      " 0.195 0.07  0.23  0.085 0.92  0.095 0.765 0.03  0.825 0.94  0.965 0.765\n",
      " 0.135 0.94  0.945 0.895 0.155 0.05  0.05  0.945 0.955 0.825 0.155 0.9\n",
      " 0.095 0.945 0.225 0.045 0.07  0.08  0.925 0.225 0.825 0.835 0.075 0.065\n",
      " 0.11  0.085 0.065 0.155 0.915 0.925 0.82  0.89  0.78  0.79  0.795 0.84\n",
      " 0.925 0.945 0.105 0.22  0.69  0.94  0.92  0.715 0.955 0.115 0.045 0.135\n",
      " 0.19  0.96  0.86  0.85  0.115 0.04  0.33  0.805 0.255 0.83  0.91  0.95\n",
      " 0.055 0.045 0.935 0.835 0.085 0.96  0.195 0.965 0.17  0.73  0.94  0.965\n",
      " 0.125 0.1   0.175 0.965 0.04  0.845 0.055 0.175 0.215 0.74  0.73  0.15\n",
      " 0.745 0.11  0.135 0.03  0.745 0.185 0.87  0.785]\n",
      "[0.895 0.89  0.95  0.755 0.17  0.06  0.77  0.04  0.09  0.7   0.06  0.825\n",
      " 0.94  0.12  0.895 0.835 0.14  0.055 0.075 0.86  0.03  0.185 0.325 0.705\n",
      " 0.795 0.075 0.115 0.925 0.955 0.1   0.845 0.23  0.03  0.815 0.17  0.075\n",
      " 0.94  0.215 0.785 0.935 0.785 0.265 0.05  0.905 0.97  0.075 0.915 0.765\n",
      " 0.935 0.03  0.01  0.145 0.77  0.2   0.09  0.21  0.705 0.05  0.975 0.24\n",
      " 0.09  0.125 0.08  0.265 0.185 0.16  0.89  0.845 0.895 0.025 0.03  0.09\n",
      " 0.685 0.06  0.165 0.8   0.065 0.775 0.815 0.06  0.12  0.69  0.725 0.195\n",
      " 0.15  0.935 0.9   0.1   0.165 0.285 0.09  0.815 0.83  0.05  0.18  0.055\n",
      " 0.205 0.03  0.275 0.055 0.055 0.8   0.745 0.045 0.27  0.83  0.065 0.035\n",
      " 0.855 0.095 0.065 0.185 0.17  0.965 0.79  0.845 0.025 0.91  0.19  0.82\n",
      " 0.125 0.76  0.14  0.825 0.125 0.885 0.085 0.09  0.93  0.02  0.125 0.04\n",
      " 0.92  0.035 0.15  0.94  0.76  0.165 0.065 0.125 0.96  0.84  0.13  0.03\n",
      " 0.915 0.085 0.925 0.715 0.125 0.84  0.9   0.735 0.935 0.07  0.04  0.205\n",
      " 0.155 0.93  0.855 0.835 0.075 0.02  0.285 0.795 0.215 0.805 0.925 0.94\n",
      " 0.075 0.02  0.955 0.875 0.08  0.88  0.19  0.905 0.13  0.705 0.93  0.955\n",
      " 0.135 0.09  0.165 0.93  0.075 0.835 0.03  0.12  0.13  0.795 0.73  0.115\n",
      " 0.755 0.135 0.155 0.04  0.78  0.185 0.875 0.785]\n",
      "[0.87  0.88  0.97  0.84  0.145 0.14  0.78  0.035 0.185 0.73  0.07  0.815\n",
      " 0.95  0.115 0.95  0.845 0.085 0.105 0.065 0.79  0.02  0.135 0.26  0.775\n",
      " 0.77  0.09  0.14  0.925 0.955 0.05  0.885 0.265 0.025 0.75  0.15  0.055\n",
      " 0.92  0.265 0.845 0.93  0.91  0.245 0.02  0.97  0.93  0.09  0.925 0.83\n",
      " 0.815 0.095 0.04  0.16  0.785 0.265 0.11  0.225 0.69  0.05  0.945 0.23\n",
      " 0.07  0.125 0.125 0.22  0.295 0.235 0.915 0.905 0.955 0.085 0.04  0.05\n",
      " 0.715 0.04  0.235 0.745 0.095 0.835 0.895 0.065 0.16  0.695 0.815 0.295\n",
      " 0.13  0.955 0.88  0.115 0.195 0.18  0.075 0.795 0.905 0.06  0.205 0.08\n",
      " 0.195 0.135 0.305 0.08  0.115 0.825 0.815 0.085 0.265 0.83  0.05  0.025\n",
      " 0.885 0.075 0.09  0.23  0.22  0.925 0.79  0.855 0.03  0.895 0.2   0.83\n",
      " 0.075 0.81  0.17  0.88  0.07  0.915 0.12  0.105 0.945 0.03  0.175 0.06\n",
      " 0.905 0.075 0.075 0.955 0.84  0.13  0.07  0.14  0.975 0.845 0.12  0.055\n",
      " 0.94  0.07  0.945 0.83  0.125 0.865 0.915 0.14  0.82  0.04  0.915 0.93\n",
      " 0.97  0.81  0.175 0.945 0.93  0.93  0.135 0.035 0.05  0.92  0.965 0.865\n",
      " 0.1   0.925 0.105 0.92  0.22  0.02  0.06  0.065 0.92  0.12  0.86  0.79\n",
      " 0.08  0.095 0.105 0.08  0.11  0.135 0.925 0.93  0.81  0.88  0.785 0.855\n",
      " 0.78  0.8   0.95  0.945 0.115 0.22  0.68  0.905]\n",
      "[0.03  0.205 0.745 0.155 0.105 0.195 0.74  0.035 0.96  0.29  0.075 0.1\n",
      " 0.08  0.24  0.115 0.165 0.885 0.825 0.94  0.105 0.04  0.1   0.7   0.035\n",
      " 0.175 0.795 0.085 0.76  0.805 0.07  0.165 0.715 0.78  0.22  0.125 0.925\n",
      " 0.845 0.105 0.14  0.28  0.115 0.865 0.85  0.05  0.25  0.05  0.165 0.11\n",
      " 0.22  0.11  0.055 0.85  0.7   0.045 0.165 0.865 0.02  0.035 0.87  0.03\n",
      " 0.08  0.18  0.24  0.975 0.85  0.9   0.015 0.96  0.21  0.745 0.055 0.775\n",
      " 0.095 0.865 0.075 0.925 0.095 0.07  0.945 0.025 0.11  0.055 0.88  0.065\n",
      " 0.1   0.88  0.775 0.135 0.035 0.15  0.985 0.845 0.065 0.045 0.915 0.105\n",
      " 0.915 0.71  0.07  0.875 0.905 0.13  0.765 0.045 0.85  0.93  0.985 0.68\n",
      " 0.15  0.975 0.955 0.895 0.14  0.025 0.055 0.955 0.975 0.865 0.065 0.855\n",
      " 0.085 0.94  0.25  0.02  0.01  0.12  0.975 0.265 0.915 0.85  0.09  0.065\n",
      " 0.07  0.08  0.04  0.12  0.85  0.985 0.84  0.89  0.72  0.795 0.805 0.8\n",
      " 0.965 0.96  0.06  0.155 0.685 0.955 0.935 0.71  0.915 0.065 0.055 0.11\n",
      " 0.14  0.965 0.84  0.88  0.105 0.05  0.255 0.835 0.25  0.79  0.91  0.96\n",
      " 0.09  0.03  0.91  0.925 0.05  0.91  0.185 0.965 0.115 0.765 0.965 0.965\n",
      " 0.14  0.055 0.205 0.96  0.085 0.86  0.065 0.165 0.09  0.73  0.77  0.095\n",
      " 0.7   0.09  0.115 0.02  0.76  0.205 0.865 0.81 ]\n",
      "[0.905 0.88  0.955 0.845 0.16  0.095 0.81  0.045 0.295 0.72  0.025 0.805\n",
      " 0.975 0.09  0.94  0.87  0.09  0.035 0.08  0.89  0.03  0.135 0.285 0.765\n",
      " 0.87  0.1   0.125 0.975 0.95  0.06  0.88  0.185 0.025 0.865 0.21  0.035\n",
      " 0.94  0.265 0.805 0.975 0.85  0.23  0.03  0.945 0.955 0.05  0.965 0.7\n",
      " 0.91  0.04  0.07  0.905 0.78  0.06  0.185 0.91  0.02  0.015 0.855 0.095\n",
      " 0.05  0.17  0.225 0.985 0.845 0.925 0.005 0.955 0.13  0.85  0.135 0.815\n",
      " 0.24  0.88  0.105 0.915 0.115 0.095 0.975 0.03  0.125 0.03  0.9   0.02\n",
      " 0.08  0.96  0.795 0.155 0.05  0.14  0.965 0.77  0.085 0.06  0.955 0.02\n",
      " 0.965 0.795 0.075 0.935 0.94  0.225 0.765 0.035 0.92  0.95  0.985 0.82\n",
      " 0.14  0.96  0.9   0.885 0.215 0.035 0.05  0.975 0.985 0.875 0.11  0.885\n",
      " 0.115 0.97  0.265 0.02  0.04  0.08  0.945 0.19  0.89  0.92  0.05  0.05\n",
      " 0.07  0.03  0.08  0.125 0.975 0.96  0.78  0.83  0.85  0.855 0.805 0.85\n",
      " 0.945 0.935 0.06  0.18  0.63  0.96  0.95  0.685 0.945 0.08  0.06  0.18\n",
      " 0.145 0.99  0.905 0.885 0.085 0.03  0.195 0.885 0.275 0.86  0.95  0.985\n",
      " 0.065 0.02  0.935 0.935 0.07  0.96  0.23  0.965 0.175 0.765 0.98  0.96\n",
      " 0.215 0.09  0.185 0.955 0.075 0.935 0.04  0.21  0.07  0.795 0.785 0.14\n",
      " 0.815 0.135 0.15  0.025 0.875 0.205 0.925 0.805]\n",
      "[0.895 0.905 0.88  0.83  0.17  0.09  0.795 0.085 0.115 0.68  0.05  0.88\n",
      " 0.99  0.13  0.91  0.875 0.12  0.055 0.08  0.83  0.07  0.205 0.34  0.74\n",
      " 0.78  0.13  0.155 0.95  0.96  0.075 0.91  0.195 0.055 0.75  0.17  0.1\n",
      " 0.95  0.225 0.825 0.91  0.78  0.16  0.04  0.94  0.945 0.085 0.975 0.79\n",
      " 0.875 0.03  0.055 0.16  0.71  0.185 0.1   0.185 0.76  0.055 0.98  0.3\n",
      " 0.05  0.125 0.09  0.255 0.195 0.225 0.91  0.915 0.92  0.06  0.04  0.055\n",
      " 0.695 0.045 0.2   0.78  0.055 0.84  0.875 0.095 0.145 0.8   0.795 0.31\n",
      " 0.175 0.91  0.92  0.095 0.24  0.24  0.095 0.75  0.865 0.045 0.205 0.09\n",
      " 0.195 0.115 0.205 0.08  0.865 0.115 0.835 0.06  0.88  0.955 0.955 0.745\n",
      " 0.12  0.955 0.955 0.935 0.115 0.065 0.07  0.94  0.955 0.885 0.145 0.9\n",
      " 0.155 0.96  0.265 0.08  0.05  0.08  0.955 0.215 0.815 0.88  0.06  0.085\n",
      " 0.075 0.095 0.075 0.17  0.89  0.91  0.82  0.9   0.79  0.765 0.81  0.84\n",
      " 0.915 0.96  0.12  0.175 0.69  0.905 0.925 0.76  0.905 0.12  0.04  0.15\n",
      " 0.12  0.945 0.855 0.855 0.155 0.035 0.285 0.81  0.295 0.815 0.935 0.985\n",
      " 0.075 0.04  0.96  0.865 0.105 0.95  0.205 0.955 0.15  0.725 0.94  0.96\n",
      " 0.15  0.105 0.225 0.955 0.085 0.89  0.055 0.195 0.14  0.765 0.73  0.12\n",
      " 0.79  0.13  0.15  0.055 0.745 0.16  0.91  0.805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85  0.89  0.94  0.865 0.2   0.055 0.73  0.095 0.14  0.73  0.055 0.805\n",
      " 0.96  0.12  0.915 0.805 0.11  0.085 0.08  0.855 0.04  0.195 0.245 0.74\n",
      " 0.865 0.055 0.09  0.925 0.955 0.11  0.875 0.205 0.01  0.82  0.195 0.125\n",
      " 0.945 0.24  0.765 0.95  0.865 0.215 0.02  0.925 0.955 0.065 0.93  0.815\n",
      " 0.89  0.075 0.025 0.145 0.745 0.19  0.075 0.205 0.74  0.03  0.955 0.32\n",
      " 0.03  0.15  0.085 0.235 0.22  0.14  0.88  0.9   0.91  0.055 0.04  0.08\n",
      " 0.755 0.035 0.18  0.78  0.09  0.73  0.86  0.065 0.165 0.71  0.69  0.26\n",
      " 0.125 0.95  0.79  0.12  0.18  0.255 0.095 0.805 0.84  0.08  0.16  0.065\n",
      " 0.185 0.06  0.265 0.06  0.065 0.755 0.735 0.085 0.17  0.91  0.065 0.045\n",
      " 0.875 0.105 0.06  0.175 0.27  0.95  0.815 0.84  0.04  0.935 0.215 0.865\n",
      " 0.1   0.73  0.155 0.84  0.12  0.89  0.115 0.1   0.96  0.02  0.07  0.045\n",
      " 0.88  0.02  0.11  0.94  0.81  0.1   0.085 0.165 0.965 0.87  0.09  0.045\n",
      " 0.935 0.07  0.9   0.725 0.11  0.885 0.925 0.73  0.935 0.11  0.085 0.17\n",
      " 0.14  0.92  0.87  0.905 0.11  0.035 0.265 0.8   0.24  0.845 0.91  0.97\n",
      " 0.045 0.035 0.945 0.835 0.13  0.935 0.19  0.915 0.14  0.72  0.97  0.975\n",
      " 0.175 0.1   0.18  0.97  0.035 0.855 0.05  0.155 0.12  0.77  0.785 0.1\n",
      " 0.805 0.13  0.17  0.045 0.775 0.165 0.88  0.76 ]\n",
      "[0.88  0.895 0.975 0.84  0.13  0.1   0.69  0.055 0.17  0.71  0.045 0.835\n",
      " 0.96  0.09  0.92  0.755 0.09  0.045 0.06  0.82  0.04  0.115 0.28  0.745\n",
      " 0.775 0.095 0.125 0.905 0.97  0.09  0.91  0.275 0.045 0.81  0.18  0.075\n",
      " 0.935 0.26  0.84  0.93  0.86  0.265 0.035 0.935 0.99  0.065 0.915 0.75\n",
      " 0.845 0.055 0.04  0.215 0.835 0.24  0.09  0.19  0.79  0.07  0.955 0.35\n",
      " 0.035 0.115 0.115 0.17  0.22  0.24  0.935 0.895 0.93  0.05  0.045 0.1\n",
      " 0.71  0.05  0.155 0.805 0.085 0.745 0.855 0.11  0.15  0.78  0.78  0.275\n",
      " 0.15  0.96  0.9   0.11  0.135 0.235 0.035 0.78  0.87  0.06  0.165 0.08\n",
      " 0.205 0.105 0.3   0.085 0.055 0.795 0.73  0.05  0.215 0.925 0.075 0.05\n",
      " 0.91  0.1   0.1   0.19  0.185 0.925 0.795 0.9   0.025 0.88  0.185 0.8\n",
      " 0.07  0.79  0.165 0.845 0.085 0.89  0.095 0.085 0.935 0.04  0.07  0.025\n",
      " 0.895 0.025 0.105 0.98  0.785 0.145 0.055 0.125 0.96  0.85  0.095 0.03\n",
      " 0.935 0.095 0.935 0.725 0.135 0.895 0.89  0.155 0.77  0.03  0.895 0.93\n",
      " 0.92  0.74  0.16  0.96  0.945 0.91  0.16  0.075 0.06  0.93  0.96  0.855\n",
      " 0.065 0.89  0.1   0.92  0.225 0.045 0.05  0.08  0.94  0.15  0.88  0.805\n",
      " 0.065 0.15  0.075 0.045 0.115 0.14  0.955 0.92  0.805 0.865 0.745 0.855\n",
      " 0.76  0.82  0.935 0.93  0.13  0.145 0.69  0.915]\n",
      "[0.025 0.215 0.78  0.195 0.09  0.185 0.76  0.025 0.965 0.25  0.08  0.105\n",
      " 0.1   0.285 0.16  0.155 0.95  0.815 0.935 0.095 0.05  0.155 0.76  0.06\n",
      " 0.21  0.76  0.05  0.77  0.835 0.095 0.14  0.77  0.76  0.205 0.155 0.935\n",
      " 0.87  0.105 0.155 0.195 0.06  0.805 0.87  0.055 0.23  0.11  0.21  0.085\n",
      " 0.175 0.075 0.105 0.855 0.75  0.06  0.18  0.91  0.07  0.065 0.89  0.08\n",
      " 0.065 0.195 0.28  0.95  0.845 0.92  0.045 0.925 0.165 0.805 0.07  0.74\n",
      " 0.105 0.915 0.07  0.89  0.1   0.06  0.96  0.035 0.15  0.07  0.92  0.035\n",
      " 0.06  0.91  0.78  0.185 0.025 0.09  0.995 0.835 0.07  0.045 0.97  0.065\n",
      " 0.89  0.72  0.04  0.865 0.91  0.115 0.72  0.045 0.835 0.95  0.98  0.76\n",
      " 0.16  0.91  0.925 0.905 0.14  0.05  0.045 0.93  0.985 0.825 0.115 0.88\n",
      " 0.095 0.985 0.185 0.055 0.06  0.14  0.955 0.19  0.875 0.83  0.05  0.07\n",
      " 0.095 0.105 0.08  0.125 0.845 0.94  0.785 0.875 0.755 0.82  0.8   0.805\n",
      " 0.91  0.965 0.095 0.17  0.715 0.925 0.935 0.685 0.92  0.065 0.045 0.13\n",
      " 0.135 0.96  0.845 0.9   0.085 0.035 0.225 0.87  0.345 0.74  0.89  0.98\n",
      " 0.07  0.045 0.935 0.915 0.105 0.91  0.205 0.915 0.14  0.725 0.955 0.965\n",
      " 0.085 0.09  0.185 0.965 0.07  0.865 0.035 0.165 0.105 0.765 0.815 0.125\n",
      " 0.695 0.085 0.135 0.045 0.735 0.09  0.885 0.765]\n",
      "[0.83  0.915 0.965 0.885 0.19  0.095 0.73  0.08  0.22  0.785 0.03  0.82\n",
      " 0.98  0.095 0.905 0.885 0.13  0.05  0.07  0.9   0.04  0.19  0.295 0.81\n",
      " 0.91  0.11  0.095 0.97  0.985 0.06  0.87  0.23  0.025 0.83  0.185 0.095\n",
      " 0.985 0.28  0.83  0.975 0.865 0.285 0.01  0.955 0.96  0.065 0.955 0.8\n",
      " 0.915 0.04  0.06  0.82  0.8   0.045 0.205 0.93  0.035 0.02  0.925 0.13\n",
      " 0.05  0.22  0.3   0.98  0.82  0.915 0.01  0.98  0.165 0.835 0.125 0.855\n",
      " 0.205 0.89  0.085 0.885 0.1   0.095 0.97  0.06  0.09  0.035 0.9   0.02\n",
      " 0.07  0.985 0.83  0.17  0.05  0.195 0.985 0.865 0.055 0.035 0.96  0.035\n",
      " 0.97  0.78  0.095 0.945 0.94  0.23  0.76  0.06  0.95  0.97  0.99  0.71\n",
      " 0.165 0.97  0.945 0.905 0.17  0.03  0.035 0.96  0.985 0.9   0.14  0.885\n",
      " 0.06  0.99  0.28  0.015 0.025 0.09  0.99  0.22  0.865 0.91  0.03  0.055\n",
      " 0.065 0.095 0.1   0.095 0.955 0.975 0.755 0.87  0.865 0.84  0.855 0.815\n",
      " 0.95  0.97  0.045 0.195 0.62  0.99  0.945 0.785 0.93  0.095 0.065 0.14\n",
      " 0.115 0.985 0.875 0.9   0.085 0.035 0.205 0.84  0.23  0.87  0.95  0.975\n",
      " 0.05  0.01  0.955 0.95  0.06  0.955 0.25  0.975 0.155 0.785 0.97  0.945\n",
      " 0.23  0.145 0.215 0.98  0.035 0.955 0.075 0.225 0.06  0.795 0.76  0.145\n",
      " 0.815 0.105 0.25  0.045 0.9   0.175 0.95  0.805]\n",
      "[0.895 0.925 0.9   0.815 0.15  0.115 0.77  0.05  0.19  0.74  0.09  0.83\n",
      " 0.94  0.1   0.885 0.83  0.11  0.07  0.04  0.865 0.075 0.2   0.255 0.73\n",
      " 0.755 0.12  0.13  0.95  0.965 0.07  0.915 0.205 0.04  0.8   0.17  0.085\n",
      " 0.96  0.27  0.8   0.905 0.82  0.23  0.04  0.945 0.965 0.06  0.94  0.78\n",
      " 0.915 0.075 0.05  0.195 0.78  0.175 0.115 0.175 0.77  0.03  0.985 0.26\n",
      " 0.12  0.14  0.1   0.23  0.175 0.21  0.895 0.92  0.93  0.045 0.04  0.07\n",
      " 0.79  0.06  0.16  0.76  0.055 0.865 0.885 0.09  0.24  0.775 0.795 0.285\n",
      " 0.21  0.905 0.88  0.085 0.24  0.195 0.11  0.805 0.87  0.065 0.24  0.11\n",
      " 0.14  0.085 0.24  0.09  0.93  0.09  0.755 0.06  0.865 0.94  0.94  0.785\n",
      " 0.115 0.975 0.9   0.93  0.155 0.08  0.04  0.92  0.975 0.86  0.15  0.92\n",
      " 0.105 0.965 0.255 0.1   0.04  0.04  0.93  0.24  0.825 0.835 0.09  0.105\n",
      " 0.085 0.105 0.095 0.11  0.875 0.945 0.815 0.9   0.81  0.86  0.79  0.82\n",
      " 0.93  0.94  0.075 0.25  0.755 0.925 0.935 0.78  0.92  0.06  0.12  0.18\n",
      " 0.17  0.98  0.855 0.815 0.095 0.035 0.25  0.82  0.225 0.82  0.915 0.96\n",
      " 0.075 0.065 0.91  0.83  0.125 0.96  0.265 0.925 0.17  0.69  0.93  0.945\n",
      " 0.16  0.135 0.15  0.98  0.07  0.81  0.095 0.19  0.125 0.805 0.76  0.16\n",
      " 0.785 0.085 0.14  0.035 0.72  0.225 0.915 0.8  ]\n",
      "[0.87  0.895 0.925 0.88  0.145 0.04  0.68  0.075 0.125 0.735 0.06  0.82\n",
      " 0.945 0.12  0.885 0.78  0.135 0.04  0.075 0.825 0.035 0.17  0.265 0.73\n",
      " 0.775 0.09  0.115 0.935 0.94  0.1   0.78  0.215 0.03  0.805 0.215 0.055\n",
      " 0.9   0.23  0.72  0.945 0.795 0.27  0.025 0.915 0.905 0.065 0.905 0.74\n",
      " 0.86  0.03  0.035 0.17  0.72  0.17  0.065 0.165 0.74  0.045 0.96  0.305\n",
      " 0.07  0.14  0.085 0.205 0.22  0.18  0.915 0.87  0.95  0.07  0.04  0.115\n",
      " 0.735 0.085 0.2   0.725 0.085 0.78  0.855 0.065 0.195 0.735 0.745 0.245\n",
      " 0.135 0.93  0.875 0.145 0.185 0.245 0.085 0.835 0.81  0.06  0.155 0.055\n",
      " 0.18  0.03  0.245 0.09  0.075 0.745 0.74  0.07  0.165 0.885 0.06  0.02\n",
      " 0.86  0.13  0.055 0.175 0.215 0.95  0.805 0.885 0.03  0.86  0.22  0.825\n",
      " 0.085 0.795 0.135 0.86  0.105 0.87  0.065 0.08  0.93  0.03  0.11  0.04\n",
      " 0.92  0.03  0.16  0.93  0.76  0.165 0.055 0.175 0.975 0.84  0.11  0.03\n",
      " 0.945 0.06  0.935 0.745 0.11  0.895 0.905 0.73  0.955 0.06  0.05  0.165\n",
      " 0.13  0.955 0.86  0.9   0.045 0.025 0.275 0.805 0.285 0.8   0.875 0.95\n",
      " 0.065 0.03  0.945 0.815 0.08  0.955 0.17  0.92  0.155 0.71  0.945 0.965\n",
      " 0.185 0.09  0.22  0.96  0.05  0.865 0.05  0.18  0.17  0.8   0.785 0.1\n",
      " 0.76  0.12  0.15  0.06  0.795 0.135 0.87  0.775]\n",
      "[0.91  0.91  0.93  0.825 0.11  0.105 0.775 0.095 0.185 0.75  0.03  0.84\n",
      " 0.965 0.06  0.95  0.85  0.08  0.04  0.105 0.795 0.04  0.115 0.26  0.765\n",
      " 0.765 0.075 0.175 0.95  0.99  0.06  0.87  0.23  0.05  0.73  0.155 0.045\n",
      " 0.935 0.19  0.84  0.92  0.82  0.275 0.035 0.95  0.96  0.065 0.94  0.745\n",
      " 0.88  0.05  0.04  0.19  0.725 0.22  0.14  0.145 0.695 0.04  0.955 0.32\n",
      " 0.05  0.12  0.115 0.245 0.215 0.215 0.925 0.935 0.94  0.06  0.045 0.08\n",
      " 0.645 0.035 0.225 0.785 0.065 0.77  0.805 0.09  0.19  0.755 0.795 0.325\n",
      " 0.105 0.95  0.825 0.09  0.24  0.225 0.045 0.85  0.855 0.025 0.155 0.075\n",
      " 0.145 0.14  0.27  0.085 0.055 0.815 0.765 0.04  0.28  0.87  0.04  0.035\n",
      " 0.885 0.06  0.095 0.23  0.21  0.97  0.8   0.9   0.015 0.905 0.225 0.815\n",
      " 0.065 0.8   0.175 0.865 0.09  0.915 0.14  0.115 0.945 0.035 0.09  0.03\n",
      " 0.92  0.01  0.09  0.95  0.79  0.17  0.07  0.105 0.97  0.855 0.07  0.05\n",
      " 0.915 0.08  0.94  0.74  0.105 0.895 0.89  0.125 0.815 0.025 0.87  0.97\n",
      " 0.955 0.77  0.125 0.96  0.915 0.875 0.1   0.04  0.04  0.965 0.975 0.89\n",
      " 0.11  0.885 0.085 0.925 0.155 0.055 0.055 0.11  0.945 0.215 0.885 0.79\n",
      " 0.065 0.065 0.11  0.085 0.1   0.12  0.92  0.925 0.85  0.905 0.785 0.835\n",
      " 0.755 0.82  0.925 0.94  0.095 0.245 0.69  0.885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06  0.185 0.715 0.145 0.11  0.145 0.77  0.02  0.96  0.255 0.095 0.09\n",
      " 0.065 0.305 0.155 0.165 0.935 0.805 0.925 0.095 0.075 0.085 0.73  0.025\n",
      " 0.205 0.77  0.11  0.81  0.78  0.06  0.125 0.745 0.745 0.29  0.12  0.95\n",
      " 0.87  0.11  0.18  0.265 0.045 0.76  0.865 0.045 0.205 0.075 0.17  0.075\n",
      " 0.175 0.085 0.09  0.875 0.725 0.07  0.165 0.895 0.06  0.02  0.875 0.105\n",
      " 0.06  0.235 0.235 0.975 0.87  0.925 0.035 0.91  0.195 0.805 0.085 0.795\n",
      " 0.1   0.85  0.065 0.92  0.04  0.08  0.96  0.05  0.17  0.08  0.87  0.03\n",
      " 0.09  0.9   0.81  0.135 0.065 0.12  0.96  0.82  0.045 0.08  0.925 0.085\n",
      " 0.915 0.715 0.055 0.875 0.925 0.14  0.77  0.03  0.87  0.925 0.99  0.715\n",
      " 0.175 0.96  0.955 0.92  0.115 0.045 0.04  0.95  0.98  0.925 0.095 0.87\n",
      " 0.12  0.975 0.25  0.02  0.05  0.055 0.94  0.17  0.825 0.85  0.12  0.07\n",
      " 0.06  0.085 0.045 0.125 0.855 0.94  0.83  0.87  0.82  0.8   0.86  0.805\n",
      " 0.955 0.96  0.1   0.16  0.63  0.91  0.95  0.72  0.92  0.08  0.065 0.165\n",
      " 0.09  0.98  0.875 0.89  0.14  0.04  0.21  0.85  0.27  0.805 0.925 0.98\n",
      " 0.06  0.04  0.925 0.905 0.1   0.915 0.185 0.97  0.17  0.725 0.955 0.99\n",
      " 0.085 0.07  0.195 0.96  0.1   0.88  0.04  0.19  0.09  0.78  0.815 0.085\n",
      " 0.73  0.13  0.155 0.04  0.825 0.155 0.885 0.775]\n",
      "[0.895 0.885 0.985 0.835 0.195 0.07  0.77  0.07  0.195 0.74  0.065 0.79\n",
      " 0.98  0.07  0.94  0.88  0.085 0.035 0.1   0.89  0.05  0.145 0.34  0.8\n",
      " 0.88  0.115 0.135 0.975 0.98  0.025 0.895 0.23  0.025 0.79  0.235 0.06\n",
      " 0.96  0.315 0.82  0.975 0.86  0.285 0.03  0.95  0.975 0.08  0.985 0.785\n",
      " 0.935 0.015 0.06  0.89  0.725 0.075 0.17  0.905 0.035 0.04  0.87  0.12\n",
      " 0.03  0.165 0.19  0.985 0.84  0.895 0.025 0.965 0.165 0.815 0.135 0.805\n",
      " 0.155 0.88  0.055 0.92  0.085 0.14  0.975 0.04  0.09  0.035 0.91  0.01\n",
      " 0.105 0.97  0.79  0.155 0.06  0.205 0.97  0.78  0.055 0.065 0.98  0.07\n",
      " 0.96  0.74  0.11  0.915 0.925 0.17  0.81  0.04  0.915 0.935 0.98  0.8\n",
      " 0.14  0.945 0.945 0.925 0.165 0.03  0.04  0.995 0.98  0.89  0.105 0.885\n",
      " 0.08  0.96  0.27  0.015 0.045 0.055 0.97  0.28  0.87  0.9   0.07  0.09\n",
      " 0.1   0.06  0.085 0.075 0.965 0.945 0.78  0.82  0.845 0.855 0.845 0.86\n",
      " 0.965 0.965 0.08  0.21  0.695 0.975 0.97  0.755 0.94  0.085 0.035 0.185\n",
      " 0.125 0.98  0.88  0.925 0.12  0.035 0.165 0.85  0.28  0.82  0.94  0.99\n",
      " 0.08  0.015 0.95  0.945 0.065 0.955 0.26  0.97  0.205 0.705 0.955 0.95\n",
      " 0.2   0.12  0.195 0.95  0.045 0.92  0.065 0.225 0.12  0.835 0.79  0.15\n",
      " 0.785 0.165 0.19  0.02  0.9   0.185 0.96  0.8  ]\n",
      "[0.875 0.9   0.915 0.79  0.19  0.135 0.78  0.05  0.155 0.715 0.08  0.82\n",
      " 0.965 0.105 0.93  0.815 0.125 0.045 0.08  0.81  0.075 0.17  0.285 0.77\n",
      " 0.79  0.105 0.165 0.945 0.97  0.065 0.91  0.285 0.055 0.745 0.215 0.11\n",
      " 0.945 0.23  0.795 0.94  0.765 0.24  0.06  0.945 0.955 0.085 0.93  0.765\n",
      " 0.935 0.045 0.06  0.205 0.74  0.235 0.105 0.215 0.73  0.03  0.96  0.335\n",
      " 0.09  0.15  0.09  0.18  0.16  0.225 0.885 0.91  0.905 0.055 0.025 0.07\n",
      " 0.72  0.06  0.19  0.83  0.11  0.82  0.87  0.1   0.2   0.705 0.73  0.25\n",
      " 0.16  0.945 0.835 0.115 0.25  0.195 0.075 0.775 0.91  0.03  0.19  0.095\n",
      " 0.165 0.105 0.245 0.095 0.87  0.125 0.755 0.055 0.905 0.955 0.965 0.755\n",
      " 0.135 0.98  0.925 0.9   0.145 0.045 0.085 0.935 0.955 0.87  0.155 0.87\n",
      " 0.125 0.97  0.15  0.05  0.085 0.085 0.935 0.2   0.815 0.805 0.08  0.065\n",
      " 0.08  0.07  0.09  0.11  0.85  0.92  0.75  0.9   0.81  0.81  0.785 0.81\n",
      " 0.93  0.975 0.085 0.195 0.71  0.9   0.92  0.725 0.94  0.145 0.065 0.175\n",
      " 0.155 0.95  0.855 0.855 0.105 0.06  0.24  0.775 0.285 0.78  0.915 0.96\n",
      " 0.08  0.05  0.93  0.85  0.075 0.955 0.27  0.93  0.14  0.73  0.925 0.97\n",
      " 0.2   0.085 0.22  0.965 0.075 0.825 0.055 0.175 0.14  0.77  0.775 0.125\n",
      " 0.79  0.115 0.1   0.035 0.76  0.17  0.91  0.775]\n",
      "[0.805 0.87  0.96  0.78  0.16  0.06  0.755 0.08  0.205 0.72  0.035 0.815\n",
      " 0.98  0.1   0.91  0.84  0.09  0.065 0.07  0.8   0.02  0.15  0.26  0.705\n",
      " 0.815 0.055 0.105 0.94  0.98  0.075 0.865 0.24  0.01  0.815 0.135 0.085\n",
      " 0.945 0.265 0.8   0.935 0.83  0.195 0.045 0.92  0.975 0.06  0.92  0.745\n",
      " 0.895 0.035 0.055 0.145 0.72  0.15  0.07  0.24  0.78  0.03  0.93  0.28\n",
      " 0.04  0.135 0.095 0.21  0.175 0.175 0.925 0.905 0.945 0.055 0.035 0.06\n",
      " 0.72  0.085 0.22  0.83  0.06  0.845 0.815 0.055 0.185 0.72  0.735 0.225\n",
      " 0.135 0.94  0.825 0.115 0.215 0.25  0.09  0.835 0.84  0.075 0.185 0.035\n",
      " 0.235 0.06  0.2   0.055 0.075 0.755 0.765 0.06  0.205 0.915 0.07  0.06\n",
      " 0.85  0.15  0.04  0.19  0.235 0.95  0.795 0.865 0.015 0.875 0.26  0.82\n",
      " 0.12  0.775 0.14  0.815 0.11  0.875 0.075 0.095 0.94  0.005 0.11  0.045\n",
      " 0.91  0.065 0.11  0.935 0.8   0.18  0.095 0.145 0.985 0.82  0.075 0.045\n",
      " 0.94  0.09  0.95  0.745 0.13  0.885 0.93  0.725 0.95  0.075 0.05  0.19\n",
      " 0.135 0.935 0.895 0.875 0.09  0.055 0.225 0.765 0.24  0.82  0.92  0.975\n",
      " 0.035 0.015 0.96  0.83  0.08  0.91  0.14  0.92  0.14  0.745 0.96  0.97\n",
      " 0.175 0.055 0.17  0.95  0.06  0.84  0.075 0.155 0.14  0.735 0.785 0.12\n",
      " 0.755 0.145 0.15  0.055 0.805 0.185 0.9   0.725]\n",
      "[0.91  0.85  0.97  0.795 0.145 0.07  0.775 0.045 0.23  0.74  0.04  0.885\n",
      " 0.96  0.11  0.945 0.785 0.04  0.04  0.11  0.815 0.04  0.15  0.285 0.735\n",
      " 0.735 0.065 0.15  0.94  0.955 0.045 0.91  0.27  0.035 0.75  0.155 0.06\n",
      " 0.965 0.27  0.83  0.93  0.835 0.27  0.03  0.96  0.955 0.11  0.93  0.73\n",
      " 0.875 0.055 0.04  0.18  0.76  0.19  0.085 0.16  0.725 0.03  0.95  0.31\n",
      " 0.055 0.1   0.115 0.21  0.25  0.235 0.95  0.915 0.955 0.065 0.03  0.065\n",
      " 0.73  0.02  0.155 0.745 0.095 0.78  0.845 0.085 0.145 0.765 0.78  0.305\n",
      " 0.16  0.965 0.84  0.12  0.225 0.215 0.04  0.8   0.925 0.05  0.275 0.095\n",
      " 0.21  0.13  0.25  0.08  0.085 0.86  0.74  0.065 0.3   0.89  0.03  0.045\n",
      " 0.875 0.105 0.07  0.285 0.23  0.995 0.815 0.885 0.015 0.89  0.21  0.77\n",
      " 0.07  0.805 0.125 0.86  0.065 0.87  0.09  0.1   0.915 0.025 0.11  0.055\n",
      " 0.905 0.015 0.085 0.98  0.78  0.1   0.07  0.14  0.955 0.82  0.05  0.06\n",
      " 0.955 0.085 0.935 0.74  0.125 0.905 0.92  0.125 0.815 0.01  0.865 0.915\n",
      " 0.97  0.75  0.13  0.955 0.91  0.9   0.12  0.06  0.065 0.955 0.985 0.835\n",
      " 0.055 0.93  0.105 0.95  0.3   0.07  0.03  0.145 0.895 0.18  0.9   0.77\n",
      " 0.06  0.075 0.08  0.06  0.105 0.095 0.92  0.93  0.845 0.93  0.81  0.805\n",
      " 0.78  0.76  0.965 0.97  0.105 0.26  0.735 0.88 ]\n",
      "[0.06  0.215 0.75  0.16  0.09  0.19  0.8   0.02  0.975 0.275 0.065 0.09\n",
      " 0.08  0.245 0.155 0.135 0.91  0.86  0.955 0.08  0.055 0.065 0.71  0.04\n",
      " 0.25  0.8   0.06  0.8   0.77  0.08  0.155 0.75  0.74  0.255 0.17  0.94\n",
      " 0.86  0.09  0.15  0.225 0.115 0.765 0.885 0.045 0.26  0.1   0.18  0.045\n",
      " 0.15  0.105 0.115 0.87  0.71  0.035 0.195 0.9   0.055 0.025 0.88  0.08\n",
      " 0.055 0.23  0.23  0.94  0.835 0.895 0.02  0.9   0.2   0.785 0.085 0.755\n",
      " 0.095 0.875 0.06  0.875 0.08  0.075 0.975 0.05  0.09  0.055 0.875 0.05\n",
      " 0.065 0.9   0.8   0.135 0.07  0.095 0.985 0.825 0.05  0.035 0.93  0.075\n",
      " 0.895 0.695 0.06  0.87  0.92  0.1   0.735 0.045 0.875 0.935 0.975 0.755\n",
      " 0.17  0.97  0.925 0.93  0.105 0.045 0.09  0.955 0.97  0.9   0.1   0.885\n",
      " 0.135 0.97  0.215 0.03  0.045 0.105 0.955 0.215 0.85  0.84  0.09  0.075\n",
      " 0.08  0.095 0.035 0.155 0.86  0.92  0.845 0.885 0.77  0.78  0.79  0.83\n",
      " 0.965 0.955 0.1   0.165 0.695 0.925 0.95  0.74  0.93  0.05  0.07  0.135\n",
      " 0.11  0.975 0.82  0.89  0.12  0.05  0.22  0.87  0.4   0.765 0.82  0.96\n",
      " 0.085 0.03  0.87  0.94  0.085 0.94  0.195 0.935 0.185 0.71  0.955 0.97\n",
      " 0.12  0.035 0.21  0.97  0.095 0.83  0.03  0.16  0.085 0.745 0.79  0.115\n",
      " 0.715 0.09  0.115 0.075 0.77  0.16  0.895 0.845]\n",
      "[0.855 0.9   0.97  0.835 0.165 0.095 0.78  0.045 0.205 0.76  0.035 0.82\n",
      " 0.98  0.05  0.925 0.88  0.045 0.025 0.06  0.91  0.045 0.16  0.3   0.765\n",
      " 0.9   0.085 0.11  0.98  0.95  0.03  0.835 0.23  0.025 0.775 0.2   0.045\n",
      " 0.965 0.33  0.82  0.95  0.865 0.27  0.025 0.96  0.96  0.065 0.98  0.76\n",
      " 0.885 0.02  0.06  0.895 0.765 0.065 0.185 0.965 0.035 0.025 0.915 0.11\n",
      " 0.07  0.19  0.24  0.985 0.835 0.89  0.01  0.965 0.17  0.81  0.075 0.77\n",
      " 0.19  0.9   0.105 0.86  0.105 0.075 0.99  0.04  0.085 0.015 0.91  0.015\n",
      " 0.075 0.93  0.775 0.165 0.06  0.105 0.97  0.825 0.085 0.035 0.985 0.065\n",
      " 0.97  0.785 0.1   0.92  0.915 0.18  0.81  0.025 0.89  0.915 0.98  0.78\n",
      " 0.125 0.955 0.92  0.92  0.18  0.045 0.03  0.975 0.99  0.89  0.075 0.84\n",
      " 0.035 0.985 0.32  0.03  0.03  0.095 0.99  0.33  0.895 0.88  0.06  0.085\n",
      " 0.085 0.07  0.095 0.115 0.95  0.965 0.75  0.81  0.795 0.83  0.89  0.8\n",
      " 0.96  0.94  0.07  0.145 0.685 0.985 0.935 0.735 0.97  0.08  0.05  0.13\n",
      " 0.165 0.975 0.85  0.86  0.06  0.015 0.215 0.84  0.26  0.815 0.945 0.955\n",
      " 0.065 0.01  0.97  0.94  0.07  0.95  0.25  0.97  0.18  0.82  0.96  0.925\n",
      " 0.205 0.08  0.205 0.97  0.03  0.935 0.06  0.205 0.105 0.785 0.765 0.14\n",
      " 0.77  0.15  0.21  0.015 0.91  0.2   0.94  0.75 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.905 0.885 0.915 0.85  0.17  0.105 0.785 0.04  0.16  0.785 0.065 0.845\n",
      " 0.95  0.08  0.905 0.825 0.075 0.07  0.065 0.845 0.08  0.13  0.29  0.77\n",
      " 0.765 0.105 0.145 0.915 0.95  0.06  0.89  0.26  0.055 0.78  0.16  0.115\n",
      " 0.97  0.26  0.83  0.965 0.79  0.29  0.02  0.935 0.95  0.09  0.92  0.72\n",
      " 0.91  0.065 0.035 0.195 0.695 0.18  0.105 0.215 0.665 0.065 0.985 0.335\n",
      " 0.075 0.175 0.085 0.275 0.145 0.22  0.84  0.895 0.9   0.07  0.02  0.04\n",
      " 0.7   0.065 0.23  0.835 0.125 0.835 0.865 0.1   0.165 0.68  0.78  0.23\n",
      " 0.165 0.96  0.86  0.105 0.165 0.2   0.065 0.85  0.88  0.07  0.205 0.08\n",
      " 0.165 0.09  0.19  0.09  0.915 0.115 0.77  0.04  0.855 0.955 0.98  0.655\n",
      " 0.115 0.945 0.965 0.915 0.13  0.06  0.07  0.88  0.955 0.875 0.185 0.905\n",
      " 0.11  0.985 0.23  0.1   0.035 0.055 0.96  0.22  0.84  0.81  0.07  0.07\n",
      " 0.065 0.08  0.09  0.11  0.865 0.915 0.825 0.915 0.77  0.78  0.795 0.84\n",
      " 0.93  0.96  0.1   0.17  0.705 0.955 0.92  0.785 0.94  0.095 0.07  0.18\n",
      " 0.18  0.95  0.855 0.825 0.085 0.09  0.325 0.79  0.29  0.85  0.945 0.97\n",
      " 0.105 0.05  0.935 0.88  0.065 0.95  0.165 0.93  0.17  0.7   0.93  0.95\n",
      " 0.185 0.105 0.2   0.98  0.08  0.855 0.015 0.175 0.14  0.84  0.745 0.11\n",
      " 0.79  0.095 0.13  0.055 0.785 0.175 0.93  0.79 ]\n",
      "[0.89  0.91  0.945 0.85  0.16  0.065 0.76  0.055 0.18  0.72  0.06  0.795\n",
      " 0.975 0.09  0.945 0.835 0.135 0.04  0.05  0.835 0.04  0.18  0.23  0.71\n",
      " 0.795 0.08  0.135 0.95  0.945 0.08  0.855 0.245 0.055 0.825 0.155 0.095\n",
      " 0.945 0.225 0.81  0.93  0.845 0.22  0.04  0.945 0.965 0.045 0.905 0.785\n",
      " 0.855 0.07  0.035 0.125 0.715 0.205 0.065 0.23  0.76  0.045 0.93  0.325\n",
      " 0.055 0.175 0.1   0.3   0.24  0.175 0.89  0.88  0.94  0.05  0.04  0.1\n",
      " 0.685 0.08  0.175 0.795 0.105 0.725 0.865 0.09  0.165 0.735 0.755 0.235\n",
      " 0.11  0.935 0.86  0.125 0.225 0.28  0.07  0.795 0.88  0.07  0.24  0.065\n",
      " 0.18  0.085 0.265 0.055 0.05  0.735 0.69  0.045 0.145 0.905 0.075 0.025\n",
      " 0.84  0.115 0.045 0.16  0.185 0.925 0.825 0.865 0.02  0.93  0.205 0.79\n",
      " 0.055 0.78  0.11  0.805 0.13  0.84  0.085 0.08  0.965 0.045 0.135 0.045\n",
      " 0.88  0.03  0.115 0.95  0.815 0.11  0.075 0.15  0.965 0.74  0.09  0.045\n",
      " 0.95  0.04  0.925 0.76  0.18  0.89  0.905 0.71  0.94  0.105 0.07  0.185\n",
      " 0.14  0.925 0.86  0.88  0.1   0.045 0.23  0.835 0.26  0.84  0.925 0.945\n",
      " 0.055 0.02  0.92  0.84  0.075 0.9   0.12  0.92  0.13  0.745 0.96  0.965\n",
      " 0.18  0.075 0.24  0.97  0.09  0.865 0.04  0.115 0.145 0.79  0.775 0.1\n",
      " 0.765 0.185 0.125 0.03  0.765 0.225 0.895 0.7  ]\n",
      "[0.86  0.865 0.96  0.83  0.165 0.09  0.75  0.03  0.17  0.665 0.035 0.81\n",
      " 0.97  0.11  0.94  0.8   0.065 0.06  0.09  0.845 0.02  0.125 0.25  0.765\n",
      " 0.81  0.085 0.105 0.94  0.975 0.06  0.86  0.225 0.03  0.78  0.175 0.04\n",
      " 0.98  0.255 0.845 0.905 0.91  0.255 0.05  0.94  0.96  0.09  0.915 0.73\n",
      " 0.87  0.04  0.005 0.17  0.77  0.295 0.115 0.15  0.67  0.025 0.945 0.275\n",
      " 0.05  0.095 0.105 0.275 0.2   0.205 0.92  0.915 0.92  0.085 0.015 0.07\n",
      " 0.72  0.04  0.185 0.775 0.105 0.745 0.86  0.065 0.14  0.75  0.775 0.28\n",
      " 0.135 0.965 0.84  0.12  0.16  0.225 0.06  0.805 0.9   0.03  0.24  0.09\n",
      " 0.205 0.095 0.265 0.05  0.12  0.855 0.725 0.09  0.2   0.89  0.055 0.055\n",
      " 0.91  0.08  0.055 0.245 0.185 0.965 0.76  0.92  0.005 0.925 0.195 0.845\n",
      " 0.105 0.795 0.095 0.845 0.08  0.89  0.09  0.115 0.945 0.025 0.11  0.02\n",
      " 0.89  0.015 0.125 0.955 0.805 0.135 0.03  0.16  0.96  0.795 0.08  0.05\n",
      " 0.915 0.095 0.91  0.79  0.13  0.92  0.915 0.12  0.82  0.015 0.905 0.96\n",
      " 0.955 0.75  0.16  0.975 0.91  0.885 0.115 0.04  0.04  0.95  0.975 0.9\n",
      " 0.1   0.92  0.095 0.955 0.15  0.03  0.055 0.075 0.95  0.145 0.88  0.815\n",
      " 0.095 0.075 0.09  0.06  0.105 0.15  0.935 0.94  0.84  0.895 0.785 0.87\n",
      " 0.805 0.775 0.95  0.935 0.085 0.165 0.715 0.905]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f00c3dbec18>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3Xd4FWX2wPHvITQhNAFZBKQtuoQivahIUREUAREEVAQVsbF2d9cfCgi4a8XK6looCoqCK6LioiIBGyooIEWKFMEKSAtFSHJ+f7yTMLm5JQn35t7A+TzPfTLlnZkzcydz7rwz846oKsYYY0w4xeIdgDHGmMRnycIYY0xEliyMMcZEZMnCGGNMRJYsjDHGRGTJwhhjTESWLExUiEgdEVERKR7vWApCRIaIyCd5LPusiNzrdXcQkTVhyk4WkXEFjGm0iEwtyLSxdjTrZYomSxYJTkQuE5HFIpImIj+LyHsicla84zqeqer1qjrW6/5YVU+Ld0wi0klEMr39ZK+IrBGRq+Id19HykniGt15Zn6cLOQZLjECR/BV4vBCR24F/ANcDc4FDQDegF5CnX8G+eRVX1fSoB2kSyU+qWlNEBOgOzBaRz1Q15JlPEfG5qh7VDyTb/4+enVkkKBGpAIwBblLV/6rqPlU9rKpvq+pdXpkcv3i8X5dbff2bROTvIrIc2Cci94jIzIDlPCEiT3rdV4nIau+X6QYRuS5MfEki8oiIbBeRDcCFgfGLyIve2dCPIjJORJLCzOv/ROR7b9lLRKSWN+4MEflKRHZ7f8/wTZfqzfcz7xfn2yJSWUSmicger3wdX3kVkZu9ddsuIg+LSND/ARH5i4h8ICK/e7/SL/WNy97uQbZ5cxH52luP14DSvnGVROQdEdkmIju97pq+8XVFZIE37QdAlVDbPxx15gC/A019839CRLZ422aJiHTwjRstIq+LyEve8leKSKu8rJc3/loRWe9tr9kicrJvnIrIjSKyzpt+rIjUF5HPvVheF5GS+V1Pbx97yduem739u5g3boiIfCoij4nI78Bob/jV3j6+U0Tmikhtb7h4ZX/z9rXlItJYRIYBlwN/y9rH8hvnMUNV7ZOAH9wZRDpQPEyZycA4X38nYKuvfxOwFKgFnADUBvYD5b3xScDPQDuv/0KgPiBAR69sixDLvh74zpv3icB8QLPiBWYB/wHKAicBXwLXhZjXXcC3wGnesk8HKnvz3QkMwp0FD/T6K3vTpQLrvZgrAKuAtcC5XvmXgEm+5agX54nAKV7Zod64IcAnXndZYAtwlTefFsB2oFHgdvdvc6AksBm4DSgB9AUO+8pWBi4BygDlgBnALF98nwPjgVLA2cBeYKpv/HLgshDb0B9HMaAnkAk095W5wouhOHAH8AtQ2hs3GjgIXODtF/8CFuVxvbp426eFF/tTwMKA7T4bKA80Av4A5gH1fN/b4BDrlf29BBn3EvCWty3reN/nNb7p0oG/eut7AtAbt7809IbdA3zmlT8fWAJUxO2DDYHqwf7PjtdP3AOwT4gvxv2a+SVCmRw7McGTxdUB03wCXOl1nwd8H2b+s4BbQoz7CLje19/VOygUB6p5B4QTfOMHAvNDzGsN0CvI8EHAlwHDPgeGeN2pwAjfuEeB93z9FwFLff0KdPP13wjM87qzD0pAf+DjgOX+BxgVuN3JeZA+G/gJEN90n4U60ADNgJ1e9ynewa2sb/wr+JJFhH2hEy457PK2fQZwa4RpdgKne92jgQ9941KAA3lZL+BF4CHfuGRcMqnj2+5n+sYvAf4e8L09HiLGId522eX7tMMltD+AFF/Z64BU33Q/BMzrPbxk4vUXw/0gqo1LeGu9eRcL9392vH6sGipx7QCqyNHfXbQloP8V3IEb4DKvHwAR6S4ii7yqhF24X5mhqkJODpj3Zl93bdwv0J9FZJc3r//gzjCCqQV8H2IZmwOGbQZq+Pp/9XUfCNKfHDB9YMwnk1ttoG1W7F78lwN/ChG/P94f1TvC+JYBgIiUEZH/eFUme4CFQEWveu5kXOLYF2zaPPpJVSvifsE/iTsAZhORO7wqmN3eOlUg5/f7i697P1Da2//CrhcB35OqpuH236P5nvwWqWpF32eRF3fWGY8/Jv8yA/f92sATvu/0d9xZRA1V/Qh4GpgA/Coiz4lI+TAxHXcsWSSuz3HVAr3DlNmHq9LIEuxgFtis8Aygk1dXfjFeshCRUsAbwCNANe+gMwf3zxTMz7iDfJZTfN1bcL/6qvj+wcuraqMQ89qCq0oK9BPuH9zvFODHEPPJi8CYfwoRz4KAA1Syqt4QYd4/AzVExL/N/NvlDlxVW1tVLY/7xQ5uG/8MVBKRsiGmzTNV/QP4O9BERHqDu8XXG3YpUMn7fncT+vv1i7ReOb4nbx0qc3TfUyTbcWcv/v0jcN8I3Pe34KpC/d/rCar6GYCqPqmqLXFVZafiqkeDzee4ZMkiQanqbmAkMEFEenu/Skt4v/4f8ootBS4QkRNF5E/ArXmY7zZc9c0kYKOqrvZGlcTVN28D0kWkO65qKZTXgZtFpKaIVMLdtZW1jJ+B94FHRaS8iBTzLmh2DDGvF4CxItLAu9DYVEQq45LVqeJuHy4uIv1x1SPvRFrPMO4Sd6G5FnAL8FqQMu94yx3kbfMSItJaRBpGmPfnuCqTm714+wBtfOPL4X5F7xKRE4FRWSNUdTOwGLhPREqKuz36ooKupKoewlXvjPQtOx33/RYXkZG4M5C8iLRerwBXiUgz70fHP4EvVHVTQeOPRFUzcPvg/SJSzrtQfTsQ7rmUZ4G7RaQRZF8g7+d1txaRtiJSAvcj7CCuKg/cWVC9GK1KkWHJIoGp6njcP8A9uH/yLcBw3LUEgJeBZbhrE+8T/MAXzCu4i8DZVVCquhe4GfcPuBNXRTU7zDyex93Ouwz4GvhvwPgrcQlolTe/mUD1EPMa7y33fWAPrg78BFXdAfTA/SLfAfwN6KGq2/O4nsG8haszXwq86y0rB29bdAUG4H41/wI8iEumIXkH6D64+vKduGsf/u3yOO5C63ZgEfC/gFlcBrTFVY+Mwl3AzSbuDqXLI69itonAKSJyEe67eg9XL78ZdzAMrKYp0Hqp6jzgXtyZ6c+4s8QB+YizoP6KO7BvwF2LewW3zkGp6pu473G6Vw24AneLMbjE+Txu/Tbj9rdHvHEvAile9dUsjlOSsxrSmGOXiCjQQFXXH+V8XgLWq+qY6ERmTOKzMwtj8sG74HsasDHesRhTmCxZGJM/v+Bu33wj3oEYU5isGsoYY0xEdmZhjDEmomOmIcEqVaponTp18j3dvn37KFu2bOSCcWZxRldRiLMoxAgWZ7QVdpxLlizZrqpVIxaM9yPk0fq0bNlSC2L+/PkFmq6wWZzRVRTiLAoxqlqc0VbYcQKL1Zr7MMYYEw2WLIwxxkRkycIYY0xEliyMMcZEZMnCGGNMRDFLFiIy0XtF4YoQ40VEnhT3KsblItLCN26wuFcwrhORwbGK0RhjTN7E8sxiMu7VoKF0Bxp4n2HAMwC+ppvb4ppBHuU1gW2MMSZOYvZQnqouFJE6YYr0Al7y7vNdJCIVRaQ67vWQH6jq7wDiXlzfDXg1VrHeeissXRqruUfHrl3NqFgx3lFEZnFGT1GIESzOaCtInM2aweOPxyaeLPF8grsGOdvT3+oNCzU8FxEZhjsroVq1aqSmpuY7iLS0NLZu3cquXeHe6hh/GRkZ7Nq1K95hRGRxRk9RiBEszmgrSJxbt6aRmnpULe9HFM9kEex1jhpmeO6Bqs8BzwG0atVKO3XqlO8gUlNTmTmzZr6nK2ypqakUZP0Km8UZPUUhRrA4o61gcVYEYnsci+fdUFvJ+T7kmri3koUabowxJk7imSxmA1d6d0W1A3are3fzXKCr957kSrjXW86NY5zGGHPci1k1lIi8irtYXUVEtuLucCoBoKrPAnOAC4D1wH7gKm/c7yIyFvjKm9WYrIvdxhhj4iOWd0MNjDBegZtCjJtImBevG2OMKVz2BLcxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiIYposRKSbiKwRkfUi8o8g42uLyDwRWS4iqSJS0zfuIRFZKSKrReRJEZFYxmqMMSa0mCULEUkCJgDdgRRgoIikBBR7BHhJVZsCY4B/edOeAZwJNAUaA62BjrGK1RhjTHixPLNoA6xX1Q2qegiYDvQKKJMCzPO65/vGK1AaKAmUAkoAv8YwVmOMMWGIqsZmxiJ9gW6qOtTrHwS0VdXhvjKvAF+o6hMi0gd4A6iiqjtE5BFgKCDA06o6IsgyhgHDAKpVq9Zy+vTp+Y4zLS2N5OTk/K9gIbM4o6soxFkUYgSLM9oKO87OnTsvUdVWEQuqakw+QD/gBV//IOCpgDInA/8FvgGeALYCFYA/A+8Cyd7nc+DscMtr2bKlFsT8+fMLNF1hszijqyjEWRRiVLU4o62w4wQWax6O6cWjnKT8tgK1fP01gZ/8BVT1J6APgIgkA5eo6m7vjGGRqqZ5494D2gELYxivMcaYEGJ5zeIroIGI1BWRksAAYLa/gIhUEZGsGO4GJnrdPwAdRaS4iJTAXdxeHcNYjTHGhBGzZKGq6cBwYC7uQP+6qq4UkTEi0tMr1glYIyJrgWrA/d7wmcD3wLfAMmCZqr4dq1iNMcaEF8tqKFR1DjAnYNhIX/dMXGIInC4DuC6WsRljjMk7e4LbGGNMRJYsjDHGRGTJwhhjTESWLIwxxkRkycIYY0xEliyMMcZEZMnCGGNMRJYsjDHGRGTJwhhjTESWLIwxxkRkycIYY0xEliyMMcZEZMnCGGNMRJYsjDHGRGTJwhhjTESWLIwxxkRkycIYY0xEliyMMcZEZMnCGGNMRDFNFiLSTUTWiMh6EflHkPG1RWSeiCwXkVQRqekbd4qIvC8iq0VklYjUiWWsxhhjQotZshCRJGAC0B1IAQaKSEpAsUeAl1S1KTAG+Jdv3EvAw6raEGgD/BarWI0xxoQXyzOLNsB6Vd2gqoeA6UCvgDIpwDyve37WeC+pFFfVDwBUNU1V98cwVmOMMWGIqsZmxiJ9gW6qOtTrHwS0VdXhvjKvAF+o6hMi0gd4A6gCdACGAoeAusCHwD9UNSNgGcOAYQDVqlVrOX369HzHmZaWRnJycgHWsHBZnNFVFOIsCjGCxRlthR1n586dl6hqq4gFVTUmH6Af8IKvfxDwVECZk4H/At8ATwBbgQpAX2A3UA8ojksi14RbXsuWLbUg5s+fX6DpCpvFGV1FIc6iEKOqxRlthR0nsFjzcEyPZTXUVqCWr78m8JO/gKr+pKp9VLU5MMIbttub9ht1VVjpwCygRQxjNcYYE0Ysk8VXQAMRqSsiJYEBwGx/ARGpIiJZMdwNTPRNW0lEqnr9XYBVMYzVGGNMGDFLFt4ZwXBgLrAaeF1VV4rIGBHp6RXrBKwRkbVANeB+b9oM4E5gnoh8CwjwfKxiNcYYE17xWM5cVecAcwKGjfR1zwRmhpj2A6BpLOMzxhiTN/YEtzHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJKGSyEJGqIpISZHgj3+tOjTHGHAfCnVk8BQRLCjWBJ2ITjjHGmEQULlk0UdUFgQNVdS72ulNjjDmuhEsWJQo4LpuIdBORNSKyXkT+EWR8bRGZJyLLRSRVRGoGjC8vIj+KyNN5WZ4xxpjYCJcs1onIBYEDRaQ7sCHSjEUkCZgAdAdSgIFBroE8Arykqk2BMcC/AsaPBXKd3RhjjClcxcOMuw14R0QuBZZ4w1oB7YEeeZh3G2C9qm4AEJHpQC9gla9MirccgPnArKwRItISqAb8z1uuMcecw4cPs3XrVg4ePJhrXIUKFVi9enUcosofizO6YhVn6dKlqVmzJiVK5KliKJeQyUJV14pIE+AyoLE3eAFwnarm3rNzqwFs8fVvBdoGlFkGXIK7YH4xUE5EKgM7gUeBQcA5oRYgIsOAYQDVqlUjNTU1D2HllJaWVqDpCpvFGV2JEmdycjLVqlWjRo0aiEiOcRkZGSQlJcUpsryzOKMrFnGqKrt372bZsmWkpaUVaB7hzixQ1T+ASQWaM0iQYRrQfyfwtIgMARYCPwLpwI3AHFXdEvgPFBDfc8BzAK1atdJOnTrlO8jU1FQKMl1hszijK1HiXL16NTVr1syVKAD27t1LuXLl4hBV/lic0RWrOMuVK0daWhqtWhWsoiZkshCRveQ8uCuwHVdd9HdV3RFh3luBWr7+msBP/gKq+hPQx1teMnCJqu4WkfZABxG5EUgGSopImqrmukhuTFEX7geRMdFytPtZyAvcqlpOVcv7PhVw1w5WAs/mYd5fAQ1EpK6IlAQGALP9BUSkiohkxXA3MNFb9uWqeoqq1sGdfbxkicKY2Bs9ejSPPPJI2DKzZs1i1apVYcsURKdOnVi8eHHU55tl8+bNvPLKKzGbf0FccMEF7Nq1q0DTxup7CCVfzX2o6k5VfQyon4ey6cBwYC6wGnhdVVeKyBgR6ekV6wSsEZG1uIvZ9+cnHmNM4Svsg1S0/PDDDyGTRXp6eiFH48yZM4eKFSsWaNqEThYAIlKCCNc6sqjqHFU9VVXrq+r93rCRqjrb656pqg28MkO9aySB85isqsPzG6cxJm/uv/9+TjvtNM4991zWrFmTPfz555+ndevWnH766VxyySXs37+fzz77jNmzZ3PXXXfRrFkzvv/+e5YvX067du1o2rQpF198MTt37gTgySefJCUlhaZNmzJgwIBcyz1w4AADBgygadOm9O/fnwMHDmSPe//992nfvj0tWrSgX79+QS/Kfv/993Tr1o2WLVvSoUMHvvvuOwCGDBnCzTffzBlnnEG9evWYOXMmAKNGjeLjjz+mWbNmPPbYY0yePJl+/fpx0UUX0bVrVwAefvhhWrduTdOmTRk1ahQAmzZtomHDhlx77bU0atSIrl27ZscabBtlxXDDDTfQuXNn6tWrx4IFC7j66qtp2LAhQ4YMyV6HOnXqsH37dgCmTp1KmzZtOPPMM7nuuuvIyMgA3E0QI0aM4PTTT6ddu3b8+uuvQb+HpUuXBv0eokZVg35w1xICP9cA7wMjQ00Xr0/Lli21IObPn1+g6QqbxRldiRLnqlWrsrtvuUW1Y8cjn7POOpyjvyCfW24Jv/zFixdr48aNdd++fbp7926tX7++Pvzww6qqun379uxyI0aM0CeffFJVVQcPHqwzZszIHteoUSNNTU1VVdV7771Xb/EWWr16dT148KCqqu7cuTPXsh999FG96qqrVFV12bJlmpSUpF999ZVu27ZNO3TooGlpaaqq+sADD+h9992Xa/ouXbro2rVrVVV10aJF2rlz5+z4+vbtqxkZGbpy5UqtX7++qqq+++67euGFF2ZPP2nSJK1Ro4bu2LFDVVXnzp2r1157rWZmZmpGRoZeeOGFumDBAt24caMmJSXpN998o6qq/fr105dffjniNurfv79mZmbqrFmztFy5crp8+XLNyMjQFi1aZM+rdu3aum3bNl21apX26NFDDx06pHv27NEbbrhBp0yZoqqqgM6ePVtVVe+66y4dO3Zs0O+hSZMmQb8HP//+lgVYrHk4xoY7Q7goMK8AO4AnVPXd6KYsY0w8fPzxx1x88cWUKVMGgJ49e2aPW7FiBffccw+7du0iLS2N888/P9f0u3fvZvfu3XTs2BGAwYMH069fPwCaNm3K5ZdfTu/evendu3euaRcuXMjNN9+cXbZpU9eK0KJFi1i1ahVnnnkmAIcOHaJ9+/Y5pk1LS+Ozzz7LXhbAH38cqZjo3bs3xYoVIyUlhV9//TXk+p933nmceOKJgDubef/992nevHn2MtatW8cpp5xC3bp1adasGQAtW7Zk06ZNEbfRRRddhIjQpEkTqlWrRpMmTQBo1KgRmzZtyp4fwLx581iyZAmtW7cmMzOTP/74g5NOOgmAkiVL0qNHj+xlf/DBB7nWY/fu3ezatSvo9xAt4Z6zuCrUOBFprapfRTUSY45zjz+es3/v3gOFcqtnqLtkhgwZwqxZszj99NOZPHlyvp9Leffdd1m4cCGzZ89m7NixrFy5kuLFcx5ygi1bVTnvvPN49dVXQ847MzOTihUrsnTp0qDjS5UqlWN+oZQtWzZHubvvvpvrrrsuR5lNmzblmF9SUlJ2NVS4bZQ1TbFixXJMX6xYsVzXSFSVwYMH869//SvXrbMlSpTI3k5JSUlxu76S52sWIpLiXZxeBzwTw5iMMYXk7LPP5s033+TAgQPs3buXt99+O3vc3r17qV69OocPH2batGnZw8uVK8fevXsB97RxxYoV+fjjjwF4+eWX6dixI5mZmWzZsoXOnTvz0EMPZf/yDlx21nxXrFjB8uXLAWjXrh2ffvop69evB2D//v2sXbs2x7Tly5enbt26zJgxA3AH22XLloVd1+Tk5Oy4gzn//POZOHFidpw//vgjv/32W9h5htpG+XXOOecwc+bM7OX9/vvvbN68Oew0gd9DpUqVcn0P0RT2QrWI1AYGep90oDbQSlU3RTUKY0xctGjRgv79+9OsWTNq165Nhw4dsseNHTuWtm3bUrt2bZo0aZJ9YBowYADXXnstTz75JDNnzuTZZ5/ljjvuYP/+/dSrV49JkyaRkZHBFVdcwe7du1FVbrvttlx3/dxwww1cddVVNG3alGbNmtGmTRsAqlatyuTJkxk4cGB21dK4ceM49dRTc0w/bdo0brjhBsaNG8fhw4cZMGAAp59+esh1bdy4McWLF+f0009nyJAhVKpUKcf4rl27snr16uwqr+TkZKZOnRr2aepQ2yi/UlJSGDduHF27diU9PZ1SpUoxYcIEateuHXKawO9hypQpXH/99Tm+h2iSUKdoIvIZUAGYDkxX1XUislFV60Y1gihp1aqVFuQe7UR5kjcSizO6EiXO1atX07Bhw6DjjvcnjqPN4gy+v4nIElWN+Fh3uGqobUA53PMPWS9BCl35Z4wx5pgV7gnuXkAT4GvgPhHZCFQSkTaFFZwxxpjEEKkhwd24JjgmishJQH/gcRGppaq1wk1rjDHm2JHnu6FU9TdVfUpVzwDOimFMxhhjEky+m/sAUNXw93QZY4w5phQoWRhjjIls7ty5IR8cLGosWRhjsh3LTZRff/312Y0KDh06NOg6TJ48meHDC9ZuaWBz4x999BFz584N++xHURLu5UcPARtU9dmA4bcBf1LVv8c6OGNM4pk1axY9evQgJSUl3qEU2AsvvBD1ec6ZMydHf5cuXejSpUvUlxMv4c4seuC9sjTAE8CFsQnHGFPYimIT5atXr85+4htc+01ZDRGOGTOG1q1b07hxY4YNGxa0bSj/WcykSZM49dRT6dixI59++ml2mbfffpu2bdvSvHlzzj333OwGCdPS0rjqqqto0qQJTZs25Y033gByNjc+fvx4GjduTOPGjXnca/QrXFPnRUG4W2dVVTODDMwUew+kMdF3663gq98+ISMDwjQ1kSfNmuVuodBnyZIlTJ8+nW+++Yb09HRatGhBy5YtAejTpw/XXnstAPfccw8vvvgif/3rX+nZsyc9evSgb9++APTq1YsJEybQsWNHRo4cyX333cfjjz/OAw88wMaNGylVqlTQt8E988wzlClThuXLl7N8+XJatGgBwPbt2xk3bhwffvghZcuW5cEHH2T8+PGMHDkye9qGDRty6NAhNmzYQL169Xjttde49NJLARg+fHh22UGDBvHOO+9w0UWBjWg7P//8M6NGjWLJkiVUqFCBzp07Z7c6e9ZZZ7Fo0SJEhBdeeIGHHnqIRx99lLFjx1KhQgW+/fZbgFzvjViyZAmTJk3iiy++QFVp27YtHTt2pFKlSqxbt45XX32V559/nksvvZQ33niDK664IsKXmBjCnVnsF5EGgQO9YUUnHRpjQvI3UV6+fPlcTZR36NCBJk2aMG3aNFauXJlr+mBNlC9cuBA40kT51KlTc7U2C66J8qwDZagmyps1a8aUKVOCNqp36aWX8vrrrwPw2muv0b9/fwDmz59P27ZtadKkCR999FHQuLN88cUXdOrUiapVq1KyZMnseQBs3bqV888/nyZNmvDwww9nz+fDDz/kpptuyi4X2MbUJ598wsUXX0zZsmVJTk6mT58+2Q38hWrqvCgId2YxEnhPRMYBS7xhrXDvyr411oEZc9wJOAM4UEhtGRXFJsoB+vfvT79+/ejTpw8iQoMGDTh48CA33ngjixcvplatWowePZqDBw+GnU+o9f/rX//K7bffTs+ePUlNTWX06NHZ8YWrXAnXJHqops6LgnDNfbwH9AY6A5O9T2fgElWdE2o6Y0zRUVSbKAeoX78+SUlJjB07NvuMICsxVKlShbS0tOy7n0Jp27Ytqamp7Nixg8OHD2c3eQ7urKlGjRoATJkyJXt4165defrpp7P7A6uhzj77bGbNmsX+/fvZt28fb775Zo7WfIuqSM19rAAGi0iy69V9+Zm5iHTDXRBPAl5Q1QcCxtfGNSdSFfgduEJVt4pIM9w7M8oDGcD9qvpafpZtjImsKDdRDu7s4q677mLjxo0AVKxYkWuvvZYmTZpQp04dWrduHXb9q1evzujRo2nfvj3Vq1enRYsW2e++Hj16NP369aNGjRq0a9cuexn33HMPN910E40bNyYpKYlRo0bRp0+fHNt0yJAh2eszdOhQmjdvXqSqnIIK985V4EbgB9zrVHcAm4Eb8/K+VlyC+B6oB5QElgEpAWVmAIO97i7Ay173qUADr/tk4GegYrjl2Tu4E4PFmT/B3omcZc+ePYUYScFZnNEVyziP5h3cIauhROQe3O2znVS1sqpWxlVDdffGRdIGWK+qG1T1EO69GL0CyqQA87x8zVVrAAAgAElEQVTu+VnjVXWtqq7zun8CfuNIM+nGGGMKWbhqqEHA6aqafXVIVTeIyKW4s4RxEeZdA9ji698KtA0oswy4BFdVdTFQTkQqq+qOrAJek+glcWcpOYjIMGAYQLVq1fJ9AQ7cPdMFma6wWZzRlShxVqhQIeTb1TIyMgr85rXCZHFGVyzjPHjwYIH3+0jXLHLdRqCqB0Qk1/MXQQS7XSDwNoE7gadFZAiwEPgR9/pWNwOR6sDLuKqqYM98PIf34GCrVq20IG8+S5Q3pkVicUZXosS5evXqkHc82ZvdosvihNKlS2c/R5Jf4ZLFVhE5R1Xn+QeKSBfcNYRItgL+d17UBH7yF/CqmPp4803G3Wm12+svD7wL3KOqi/KwPGOKJI1wK6Yx0aBhbunNi3DJ4mbgLRH5BPechQKtgTPJfe0hmK+ABiJSF3fGMAC4zF9ARKoAv3tnDXfj7oxCREoCbwIvqeoMjDlGlS5dmh07dlC5cmVLGCZmVJUdO3ZQunTpAs8jZLJQ1ZUi0hh3gG+Eq1ZaCFwXrHoqyPTpIjIcmIu7M2qiN88xuKvvs4FOwL9ERL15Zz0WeSlwNlDZq6ICGKKqx0Zbv8Z4atasydatW9m2bVuucQcPHjyqf+7CYnFGV6ziLF26NDVr1izw9Hm5ZjHRP0xEkkTkclWdFmIy//RzgDkBw0b6umcCuZ6aUdWpwNRI8zemqCtRogR169YNOi41NbXA9cuFyeKMrkSNM9yts+VF5G4ReVpEzhNnOLAB98vfGGPMcSLcmcXLwE7gc+Ba4G+4W1h7WXWQMcYcX8Ili3qq2gRARF4AtgOnqGri36hsjDEmqsI1UX44q0NVM4CNliiMMeb4FO7M4nQR2eN1C3CC1y+4RgXLxzw6Y4wxCSHcrbNH+YouY4wxx4pw1VDGGGMMYMnCGGNMHliyMMYYE5ElC2OMMRFZsjDGGBORJQtjjDERWbIwxhgTkSULY4wxEVmyMMYYE5ElC2OMMRFZsjDGGBORJQtjjDERWbIwxhgTUUyThYh0E5E1IrJeRP4RZHxtEZknIstFJFVEavrGDRaRdd5ncCzjNMYYE17MkoWIJAETgO5ACjBQRFICij0CvKSqTYExwL+8aU8ERgFtgTbAKBGpFKtYjTHGhBfLM4s2wHpV3aCqh4DpQK+AMinAPK97vm/8+cAHqvq7qu4EPgC6xTBWY4wxYYR7U97RqgFs8fVvxZ0p+C0DLgGeAC4GyolI5RDT1ghcgIgMA4YBVKtWjdTU1HwHmZaWVqDpCpvFGV1FIc6iECNYnNGWqHHGMllIkGEa0H8n8LSIDAEWAj8C6XmcFlV9DngOoFWrVtqpU6d8B5mamkpBpitsFmd0FYU4i0KMYHFGW6LGGctksRWo5euvCfzkL6CqPwF9AEQkGbhEVXeLyFagU8C0qTGM1RhjTBixvGbxFdBAROqKSElgADDbX0BEqohIVgx3AxO97rlAVxGp5F3Y7uoNM8YYEwcxSxaqmg4Mxx3kVwOvq+pKERkjIj29Yp2ANSKyFqgG3O9N+zswFpdwvgLGeMOMMcbEQSyroVDVOcCcgGEjfd0zgZkhpp3IkTMNY4wxcWRPcBtjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjInIkoUxxpiILFkYY4yJyJKFMcaYiCxZGGOMiciShTHGmIgsWRhjjIkopslCRLqJyBoRWS8i/wgy/hQRmS8i34jIchG5wBteQkSmiMi3IrJaRO6OZZzGGGPCi1myEJEkYALQHUgBBopISkCxe4DXVbU5MAD4tze8H1BKVZsALYHrRKROrGKNuu3bYeBAWLo03pEYY0xUFI/hvNsA61V1A4CITAd6Aat8ZRQo73VXAH7yDS8rIsWBE4BDwJ4Yxho9hw5B376wYIHrfuONwo9BFTIyoHgsv15jzPFEVDU2MxbpC3RT1aFe/yCgraoO95WpDrwPVALKAueq6hIRKQG8DJwDlAFuU9XngixjGDAMoFq1ai2nT5+e7zjT0tJITk7O93RBqXLq+PGc/M477D31VMp+/z2fz5jB4UqVjnrWeY1T0tNpNHo05dasYcWYMext2PCol50fUd2eMVQU4iwKMYLFGW2FHWfnzp2XqGqriAVVNSYfXFXSC77+QcBTAWVuB+7wutvjzjqKAWcC04ASwEnAGqBeuOW1bNlSC2L+/PkFmi6op55SBdW771ZdudJ1P/JIVGadpzjT01X793fLPekk1VKlVKdNi8ry8yqq2zOGikKcRSFGVYsz2go7TmCx5uGYHssL3FuBWr7+mhypZspyDfA6gKp+DpQGqgCXAf9T1cOq+hvwKRA588XTBx/ArbdCr14wbhykpED79vDCC65aKNZU4frr4bXX4KGHYMUKaNsWLr8cRoyAzMzYx2CMOWbFMll8BTQQkboiUhJ3AXt2QJkfcFVNiEhDXLLY5g3vIk5ZoB3wXQxjPTpr18Kll7oE8fLLUMzbrNdcA999B59/Htvlq8Jdd7nENGKE665a1SWwoUPhn/+EPn0gLS22cRhjjlkxSxaqmg4MB+YCq3F3Pa0UkTEi0tMrdgdwrYgsA14FhninRROAZGAFLulMUtXlsYr1qOzcCRdd5C4mz54N5codGde/PyQnw4svxjaGcePg0Ufhr3+FsWOPDC9ZEp57Dp54At5+G848EzZtyv/8Dx+GN99062eMOS7F9HYZVZ0DzAkYNtLXvQp3fSJwujTcNY/Elp4OAwbAxo0wbx7UqZNzfHKySxjTp8Pjj+dMJNHyxBMwciQMHuyWIZJzvAjcfDP85S/u7Kd1a3fgP+usyPNet86drUyeDL/9BklJsHy5O4MyxhxX7Anuo3HnnfD++/DMM9ChQ/Ay11wD+/a5awnRNmmSu07Sp487qBcL83V27QpffAGVKkGXLjBxYvByBw7AtGnQqROceqo7Y2nf3sWfnAy3314412CMMQnFkkVBPf+8+1V/220uIYTSrh00bBj9qqiZM931iK5d4ZVX8vZMxWmnuYTRqZOL+fbb3dkRwLffujOQGjXgiitgyxZ3reOHH2DWLHdWMmoUzJ0L770X3XUxxiQ8SxYFsWAB3HgjdOvm7jwKR8Qd1BctgpUro7P8//0PLrvMJaL//hdKlcr7tJUqwZw5LjE89hicd567a6ppU/jPf+D8812V2rp1cPfdcPLJR6a96SZ3tnH77e46hjHmuGHJIr82boRLLoE//9ldi8jLL/pBg6BEieicXXz8sat2atQI3n0XypbN/zyKF3dnRf/5D3z2mbtL6rHH4Mcf4dVXXTVVsCqtkiVh/HhYswYmTDj6ddm/H3755ejnY4yJOUsW+bFnj7vzKTPT3RlUoULepqtaFXr2dLfVHjpU8OV//TX06AGnnOKqgypWLPi8AIYNc4lixQp37aNKlcjTXHCBq/q67z7XBlZBHTwInTtD7dpw//1Ht12MMTFnySKv0tNd1c9338GMGdCgQf6mHzrUHVwLePtpqV9/dVVEFSu65ydOOqlA88mlRIncd1CFI+LOQvbuddcwCkIVbrgBvvzS3c57zz3QqpXrN8YkJEsWeaHq6vjffddVv5xzTv7ncd55UKuWu2spvw4fJmXsWPjjD3f3Va1akaeJpZQUd7B/9ll3VpJfEya423FHjoSPPnIX0H//3d11ddtt9vCgMQnIkkVejB/vbo+96y647rqCzSMpCYYMcQf7H37I37QjR1Jh5Ur3gN1ppxVs+dE2erSrhrvttvzdSpua6qq8LrroyJlJr17u4v9117lnRRo3dtVs+ZWR4e72+uab/E9rjAnLkkUkb7zhkkTfvvDAA0c3r6uvdn8nT877NHPnwgMP8NOFF7oHABNF5couYXz4IbzzTt6m2bwZ+vVzVXhTp+a8iF6hAvz73+4CfunS7k6zK6+MfF1k0yaXRPv2dddc2rVzd3e9/35B1yzxxKJdr337oj9Pc0yzZBHOokXumYN27eCll8I/9JYXdeq4KqyJE/N2APj5Z3cnVaNGrB8+PHL5wnbDDe4ZkjvuiHyBev9+uPhiV+6tt6B8+eDlzjrLvTTq3nvdnVkNG7rnSLLOXvbscdd9hg93t/HWrevOSL74wt0lNm2am+bii92dXkVRZiZ89RW1p0xxie+EE9wPloyMo5+3Kjz4oNv+0bijLZY2bHAPvq5dG+9IDMSuifLC/kS9ifLvv1etWlW1Xj3V334r0LyDevVV14T4+++HL5eertqli+oJJ6iuXJm4zSu/955bn0cfVdUQ2zMzU/Wyy1RFVN95J+/zXr5ctW1bN/8uXVQ7dFAtXtz1lymjeuGFqk88obpqlVtGll9+Uf3zn1UrVlRdujTorBNue+7Y4faNQYPcfgeaKeLWv3dvt84XXKC6e3fBl3HwoOqVV7p5Va2qWrKk6tdfH3XoUd+W6elufypTxsVaoYLqnDlHPduE+M4PHVLNyAhbJFGbKI/7QT5an6gmix07VE87TbVSJdXvvivQfEM6cED1xBPdeyfCGTvWfT0vvhg6zkTRvbv7h/7tt+BxPvKIW5f778//vNPTXUKoUkW1ZUv3rpD5892BL5xNm1Rr1nTv9Vi7NtfouG/PjAzVJUtUx41TPeMM1WLF3DaqXFn18stVp07VT2bNOlL+3/9WTUpSTUlRXb8+/8v79Ve3HFAdM0Z12zbVGjVUGzRQ3bPnqFYlqtty+XLVNm1cnBdeqPrJJ6rNmrkfGg88kPNHQTzjzI+MDNWPPlK95hr3f3LKKaqffhqyuCWLopIsDh5U7djR/epasKBA84zo5pvd/LdvDz5+wQJ38Ljssux/jrgf3MJZvdr94r/uutxxzp3r1qVv36P6Ry9wXFWrun/OH37IMSrP23PvXtU771Q96yzVAQNU//Y395Krt95S/eYb9x2GWq9Dh1TXrXNnX08/rXrrraoXXaTasKF7MZWrFFJt1Up15EjVRYtccgwV40cfuR8aJ57ouvNq2TK3DU44QXXGjCPDU1Pdd3PllXmfVxBR2TcPHlS99163H1WpovrKK0e26759R17qNWCA649XnHmVmen2jzvvdEkZVJOT3ZljvXou8T/wQNCzDEsWRSFZZGaqXnGF2yyxfMPcsmVuGY8/nntc1i++P/85xy++hE4Wqqq33KJarJh++cILR4atX+/Ozpo0cQfdePj6a9Xy5d2Z4q+/Zg/O0/Z85x13kBVRbdfO/ZOXLHnkIJ/1KVPGzf+889zB4LzzjhwQ/OVOOMFti969Ve+4Q3XKFFdlFkLQGNevd8mmeHF3thHJrFmqZcu6fWrx4tzjR492sU2ZEnle+YkzPz791K0TuP+/bdtyl8nMdAdXEdXmzd2ZY0Hj3L5ddefOHIk5ajZtUv3nP1UbNXLrU7y4+4EwffqRJLdrl2q/fm589+65qrkTNVnEtInyImf0aHeXzrhx7gG8WGna1D2E9uKL7vmNrIfiVN3ttdu2uYvrsWjSPFZGjoSXX+bPEya4u7727YPevd24WbNci7Xx0Ly5ez6ma1f3UOP8+ZGffP/lF7jlFnj9ddesyiefwBlnuHGZma659i1b3C3QgX+/+w6qVYM2bWDgQNcsTP367u+f/pS/ByCDqV/f7RsDB7r2yVascLcblyiRs5yqa7fs7rvdvvbWW1C9eu753XOPe9blxhvdjRynnnp08eXH3r3wf//nLrTXrOnaLOvePXhZEfj736FJE/e/2aqVa0yzY8c8L+tP777r1vfTT48ML1vWXewP/JQr5/6WKeOaxyle3G3jrG7/p0QJ92zQG2+4u/nAPWz673+7u/8CW0aoUMG14tyli7uNvFkzdzPH2WfnfxsWprxklKLwOeozi0mTXKa/+urCqS559lm3vC+/PDLs0UfdsKeeCh1nInv6aRf/f/+resklrorjgw/iHZXz3nuqJUqonnmm6r59wbdnRobqc8+5C+MlS7rrRn/8UeihZgn7naenuyqOrIv/O3YcGXfgwJEL2QMGqO7fH35BW7a4qq3mzSNfC8pvnKG8996Rs7bhw/N33WTNGtW//MX9ap8wIfT/a0aGu7515ZVHLpafdpq7ZjN+vDuruv121aFDVS+9VLVbN3ddp0kT1dq13VlxqVK5zxBDfRo2dNflNmzI+7p88427blSsmNvf0tMT9swi7gf5aH2OKlnMm+d2vHPOcfXMhWHXLlctcd11rv+LL1wMF18cdOcvEsni8GFNq1PHrZfvDqmE8frr7p/y/PM1NfButNWrVc8+28XdsWP0b2wogDx955Mnu8RWv767K+yXX1Tbt9fsC9l5/eEze7ab5pZbohfngQNuO773nqsy+9vfXPVLixZuWX/5S9gLvWHt2qXao4ebz9ChOZPc5s1u3evVc+PLlVO99lpd8vTTBf8hmJmpeviwW6e9e1011rZtqj//7JLt1q0Fn/eePe76JKiec45+8sYbBZtPAVmyyKPPp051dyg0auR2gMJ05ZVuR/7xR9W6dd2vmd9/D1q0SCQLVV368MNut7r88sK/oJ0XL76oCvprx47u1/nBg6r33ecOuJUqufEJEneev/PPPnN3fZUrp1qrVu4L2Xl1yy3uu3vrrfzH+csv7gB9+eXu13n16prrl3fJku5XdNeu7hf4gQP5j9EvI0N1xAg37zPOcInz3HPd2UrWGdfLL2dfK0jo/6HMTNUXXlAtXVr/qFRJ9cMPC23RlizyKPWDD9zpfAEumB21hQvdV3DKKe5U97PPQhZN6B3dZ/78+aorVhTeGVpBjB/vtvsllxy5sDpwYNiLzfGQr+9882ZXjVSzprsltyAOHnTzOPFE92s5L/bs0Y2DB7uL6CKqdeqoduqketVVLnm89JLqxx+7X94Rni8osNdfP1LNVKeOq17auDFXsSLxP/Ttt5qWVT13772qaWkxX2RCJAugG7AGWA/8I8j4U4D5wDfAcuAC37imwOfASuBboHS4ZUX9obzCkJmpeuqp7mt48MGwRYvEjq5FJ86NWXX6tWtH5YGvWMj3tsw6Uzoaa9e6Wzw7dHDVLqEcOuSuUZ10ktuOffu6awnxsnGjeyYjTEIqKvvmgjlzVAcP1uzbba+6yt3mHKNkm9dkEbPmPkQkCZgAdAdSgIEikhJQ7B7gdVVtDgwA/u1NWxyYClyvqo2ATsCx92o2EXcny9/+5po1MIVm05Ah7o2HK1eGvgOnqElKyt9bE4Np0OBIG13jxuUer+ruEktJcU2uNGzIkgkTXLP9hXknVaA6ddwdSEfbJE8CyDzhBNd+3IIF7m6qGTPcq5Dr13eNb65fH5e4Yrll2wDrVXWDqh4CpgO9AsookNVIUAXgJ6+7K7BcVZcBqOoOVY1CwzgJqHt311bPMbCTFyki7lbFgrxp8Fg3aJBrxHHsWNdKcJbUVNdWVf/+rrHHd9+F+fPZmxL4G9BExdlnu3bkfvnF3dLfoIH7Tho0cG2oPf887N5daOGIOwuJwYxF+gLdVHWo1z8IaKuqw31lqgPvA5WAssC5qrpERG4FWgInAVWB6aqa62XXIjIMGAZQrVq1ltOnT893nGlpaSTH6xmAfLA4o6soxBnPGJMOHKDlsGEkHTzI6hEjqDV9OpW/+IKDVauy6eqr+eW889yZTJzjzI9jIc5S27ZR7YMPqDZ3LmV/+IGMkiXZceaZ/NKtG7+3aVOg5XXu3HmJqraKWDAvdVUF+QD9gBd8/YOApwLK3A7c4XW3B1bhznbuBDYCVYAyuGsX54RbXpG8ZpEPFmd0FYU44x7j118feWK9YkXVhx4K+sxG3OPMo2MqzsxM94zWTTe5GxLaty/w8kiAJ7i3Av5XutXkSDVTlmtwF8FR1c9FpLSXILYCC1R1O4CIzAFaAPNiGK8xxq95c9fk+7ffuieNK1WKd0Qmiwi0bu0+48fDT4GH1uiLZUX5V0ADEakrIiVxF7ADX0D9A3AOgIg0BEoD24C5QFMRKeNd7O6IO+swxhSmvn3hvvssUSSykiXdBf4Yi9mZhaqmi8hw3IE/CZioqitFZAzutGc2cAfwvIjchrvYPcQ7LdopIuNxCUeBOar6bqxiNcYYE15MGxJU1TnAnIBhI33dq4AzQ0w7FXf7rDHGmDiz+zWNMcZEZMnCGGNMRJYsjDHGRGTJwhhjTESWLIwxxkRkycIYY0xEMWsbqrCJyDZgcwEmrQJsj3I4sWBxRldRiLMoxAgWZ7QVdpy1VbVqpELHTLIoKBFZrHlpRCvOLM7oKgpxFoUYweKMtkSN06qhjDHGRGTJwhhjTESWLOC5eAeQRxZndBWFOItCjGBxRltCxnncX7MwxhgTmZ1ZGGOMiciShTHGmIiO22QhIt1EZI2IrBeRf8Q7niwiUktE5ovIahFZKSK3eMNPFJEPRGSd9zch3kYjIkki8o2IvOP11xWRL7w4X/NefBXvGCuKyEwR+c7bru0TcXuKyG3ed75CRF4VkdKJsD1FZKKI/CYiK3zDgm4/cZ70/q+Wi0iLOMf5sPe9LxeRN0Wkom/c3V6ca0Tk/HjG6Rt3p4ioiFTx+uO2PQMdl8lCRJKACUB3IAUYKCIp8Y0qWzruveQNgXbATV5s/wDmqWoD3OtlEyXB3QKs9vU/CDzmxbkT9+rceHsC+J+q/gU4HRdvQm1PEakB3Ay0UtXGuBeGDSAxtudkvNcf+4Taft2BBt5nGPBMIcUIweP8AGisqk2BtcDdAN7/1ACgkTfNv73jQrziRERqAefh3iCaJZ7bM4fjMlkAbYD1qrpBVQ8B04FecY4JAFX9WVW/9rr34g5sNXDxTfGKTQF6xyfCI0SkJnAh8ILXL0AXYKZXJO5xikh54GzgRQBVPaSqu0jA7Yl7GdkJ3quEywA/kwDbU1UXAr8HDA61/XoBL6mzCKgoItXjFaeqvq+q6V7vIqCmL87pqvqHqm4E1uOOC3GJ0/MY8Dfc20GzxG17Bjpek0UNYIuvf6s3LKGISB2gOfAFUE1VfwaXUICT4hdZtsdxO3em118Z2OX750yE7VoP9173SV512QsiUpYE256q+iPwCO5X5c/AbmAJibc9s4Tafon8v3U18J7XnVBxikhP4EdVXRYwKmHiPF6ThQQZllD3EItIMvAGcKuq7ol3PIFEpAfwm6ou8Q8OUjTe27U40AJ4RlWbA/tInCq8bF6dfy+gLnAyUBZXBREo3tszkkTcBxCREbgq3mlZg4IUi0ucIlIGGAGMDDY6yLC4xHm8JoutQC1ff03gpzjFkouIlMAlimmq+l9v8K9Zp5/e39/iFZ/nTKCniGzCVeN1wZ1pVPSqUSAxtutWYKuqfuH1z8Qlj0TbnucCG1V1m6oeBv4LnEHibc8sobZfwv1vichgoAdwuR55sCyR4qyP+5GwzPt/qgl8LSJ/IoHiPF6TxVdAA+9Ok5K4C12z4xwTkF3v/yKwWlXH+0bNBgZ73YOBtwo7Nj9VvVtVa6pqHdz2+0hVLwfmA329YokQ5y/AFhE5zRt0DrCKBNueuOqndiJSxtsHsuJMqO3pE2r7zQau9O7iaQfszqquigcR6Qb8Heipqvt9o2YDA0SklIjUxV1A/jIeMarqt6p6kqrW8f6ftgItvH03cbanqh6XH+AC3N0R3wMj4h2PL66zcKeZy4Gl3ucC3PWAecA67++J8Y7VF3Mn4B2vux7un249MAMolQDxNQMWe9t0FlApEbcncB/wHbACeBkolQjbE3gVdx3lMO5Adk2o7YerNpng/V99i7u7K55xrsfV+Wf9Lz3rKz/Ci3MN0D2ecQaM3wRUiff2DPxYcx/GGGMiOl6roYwxxuSDJQtjjDERWbIwxhgTkSULY4wxEVmyKKLENeB3i+8efGOMiRlLFgnGa3HyZV9/cRHZJl6rrj63A2l6pCmIUPPrKTFuVVdEOgWJryDzqSgiN0Yo81lBlykivROowcijIiJz/C2o+oaPFpE74xFTYYnW/paoy0tUliwSzz6gsYic4PWfB/zoLyAixYBfVPXFSDNT1dmq+kD0w4yJikDQZJHVIqiqnnEU8++Na2W4yFPVC9Q1iHhU8npmamewxpJFYnoP15orwEDcQzwAiEgb4BPgdhH5LOvJZBG5XUQmet1NxL0ToYyIDBGRp73hk0XkGXHvy9ggIh29tvVXi8hk3zIGisi33jweDBaguPeBfCcinwB9fMPLevP8ymu4L1drviKSLCLzRORrbzlZZR4A6ovIUnHvIejkxfoK7oEkRCTNN6vy4t5RsEpEnvWSaI4yItLXW+8zgJ7Aw97864tIMxFZJEfedZD1ToabvXkuF5HpQeKvIyIfe/F/7c072Da60pvHsqyzRRGp7a37cu/vKb7v5knvO90gIn294dVFZKEX8woR6eAN3yRH3nkwQtw7GT4ETvMtv76I/E9Elnjx/sW3rPEiMh94UETaeMv9JmCfGiIiM0TkbeD9IOt3hYh86cX2n6yELiJpInK/t96LRKRaiH1gkvf9LxeRS7zhz4jIYnHv9bjPVz7U/hY09oBldRKRVDnyTpNpIiLeuHO8ab/19ttSEZZ3oojM8mJeJCJNg333x6R4PQ1on+AfIA1oimvDqDTuqdNOHHlCujxQ3Os+H3jD6y4GLAQuxj2tfKY3fAjwtNc9GdeOk+AardsDNPGmXYJ70vlkXNMTVXGN8H0E9A6IsTTuqdgG3rxe98X3T+AKr7si7in5sgHTFwfKe91VcE/ZClAHWOEr1wl3plXXv3184w7innBOwr23oK+/jNfdF5jsW/++vnHLgY5e9xjgca/7J7wnpYGKQb6jMkBpr7sBsDhImUa4J4OznsTNesL5bWCw1301MMsX2wzvu0jBNaEPcAdeCwPeepbzujd5264lLpGWwe0b64E7vTLzgAZed1tckyxZy3oHSLdinvMAAAU3SURBVAqyT53LkX1qCO4J41xPtwMNvXUp4fX/G7jS61bgIq/7IeCeINM/mLW9vf5KAdspCUjF/S+E29+Cxh6wrE64Vnxretv3c1xLCVnzPdUr9xJwa4TlPQWM8rq7AEvjfcworI+dWiYgVV0urnnygcCcgNHlgOfFvSxHcM0uoKqZIjIEdwD8j6p+GmL2b6uqisi3wK+qmvWLfSXuYF0bSFXVbd7wabj3QczyzeMvuEbv1nllpuJezALQFdfAYFa9eWngFHK+IEmAf4rI2bjmzWsAuX59er5U976BUOM2eDG8ijsAzAxRNgcRqYBLBAu8QVNwB2tw23CaiMwi53pnKQE8LSLNgAzg1CBlugAzVXU7gKpmvb+gPUd+qb6MO5hmmaWqmcAq36/xr4CJ4hqXnKWqSwOW0wF4U712j0Rktvc3GdcQ4QzvRzS45kOyzFDVDK+7AjBFRBrgDvQlfOU+8MXudw4uUX3lzf8EjjQmeAiXjMD9CDkvyPTn4toUA0BVd3qdl4rIMNwPiuq4xFmM0PtbuNj9vlTVrd70S3H7+l5vvmu9MlOAm3BJKtTyzgIu8WL+SEQqi0gFVd0dYrnHDEsWiWs27v0GnfASgmccMF9VnxXXANp837gGuDOTk8PM9w/vb6avO6u/OK4Z57wI1U6MAJeo6pow016OO3NpqaqHxbW0WTpE2X35iEGDDA8133AuxCXInsC9ItJIc95IcBvwK+6te8VwZziBJEh8wfjL+L8PAfeiHC+pXgi8LCIP/3975/NSRRwE8M94CzIzo1tU2KVTBdU1/QM6PChC5GHitToVFHQILYo8hBcJBKEoqENEhw4ZZhr2AyLTMrxJEHgppE6Ch+kws7o+9+0+heBR87ntfr9vZ/a783a+35nlO6p6L+caCQ1YLYxDVeSmx7UPs6mST1JeVemXRoC7qno5o21ZfeqNOdOs98y68XF7vgAcVdVFsdBo8vyqjWWe7mnSY5volLX9d0Kefdfa958ichb1yzDQm8z8UzRjxXzAwgTAykx5AHvJtSQx703wHjguIjs9Bt0BjFf0mQP2iUirH3ek2p4D51Ix4cMZMpqwWhjLItKOrWbAZnqNG9D1mNjOwQ3AaSyXA7Z99gE/X0r1X7m+zwQXkxwAUAbG/Te7VXUMK+y0Hdiaof+CrwLKWMikklFsltwCFuv2829YnVF3pnTORET2YGM1hO1GXFmDeQIoicgWEWkETvj9/QbmReSUX0dE5GAVMU2sfkRxJk+fFKPASRHZ5dff4brWyghwNjkQyxdtw5zTL19ZJfU88uxtM7onzAF7RWS/H5cxW8+TN4E9N0SkDfihdVhv5m8QzqJOUdXvqjqQ0dQP3BCRSdY+v9vAoC+pe4CbyR95g3IXsDrFY8A08FFVn1b0WcKW5c88Afgt1dyHhQJmxArS92WIeQAcEZEP2B9vzq/7E5gUS+T216DuWywp/gWYB574+UtYGOQltrtnwkPgoic0W7GttftFZAbL1/RiL/77HqabwupfV351NAh0icg7LAS1bvatqrPAdcwBTQPJdvPngW6XWcZqmOfRBnwSkSks/LHGJtRK8D7CcluPgdep5k6gx+XPUr108C1WbaqmOtSq+hW4Aoz4vbzAwka1cg1o9mc9DbSrVYmbcl2HgUmXlWdvG9Y9dQ9LQDcWqvuMra7vFMi7itnuDGZ7XfwnxK6zQRAEQSGxsgiCIAgKCWcRBEEQFBLOIgiCICgknEUQBEFQSDiLIAiCoJBwFkEQBEEh4SyCIAiCQv4ARcODjbWSjMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##CORREGIDO\n",
    "\n",
    "\n",
    "roc_auc_forest_train = []\n",
    "roc_auc_forest_test = []\n",
    "\n",
    "max_features_list = np.arange(1,150,5)\n",
    "\n",
    "for max_features in max_features_list:\n",
    "        \n",
    "    lista_aucs_train = []\n",
    "    lista_aucs_test = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_dev):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_dev_np[train_index], X_dev_np[test_index]\n",
    "        y_train, y_test = y_dev_np[train_index], y_dev_np[test_index]\n",
    "\n",
    "        forest = RandomForestClassifier(n_estimators=200, max_features=max_features, n_jobs=-1)\n",
    "        forest.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_test = forest.predict_proba(X_test)\n",
    "        y_pred_train = forest.predict_proba(X_train)\n",
    "        \n",
    "        y_pred_test=y_pred_test[:,1]\n",
    "        y_pred_train=y_pred_train[:,1]\n",
    "        \n",
    "        #print(y_pred_train)\n",
    "            \n",
    "        roc_auc_test=sklearn.metrics.roc_auc_score(y_test, y_pred_test, sample_weight=None)\n",
    "        roc_auc_train=sklearn.metrics.roc_auc_score(y_train, y_pred_train, sample_weight=None)\n",
    "        \n",
    "        roc_auc_test=sklearn.metrics.roc_auc_score(y_test, forest.predict_proba(X_test)[:,1], sample_weight=None)\n",
    "        roc_auc_train=sklearn.metrics.roc_auc_score(y_train, forest.predict_proba(X_train)[:,1], sample_weight=None)\n",
    "        \n",
    "        lista_aucs_train.append(roc_auc_train)\n",
    "        lista_aucs_test.append(roc_auc_test)\n",
    "\n",
    "\n",
    "    roc_auc_forest_train.append( np.mean(lista_aucs_train) )\n",
    "    roc_auc_forest_test.append( np.mean(lista_aucs_test) )\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_features_list, np.array(roc_auc_forest_train), color = 'blue', label = 'datos de entrenamiento')\n",
    "plt.plot(max_features_list, np.array(roc_auc_forest_test), color = 'red', label = 'datos de validación')\n",
    "plt.xlabel('Máximo de atributos a considerar en cada nodo')\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.title('Curva de complejidad: Random Forest')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86  0.865 0.96  0.83  0.165 0.09  0.75  0.03  0.17  0.665 0.035 0.81\n",
      " 0.97  0.11  0.94  0.8   0.065 0.06  0.09  0.845 0.02  0.125 0.25  0.765\n",
      " 0.81  0.085 0.105 0.94  0.975 0.06  0.86  0.225 0.03  0.78  0.175 0.04\n",
      " 0.98  0.255 0.845 0.905 0.91  0.255 0.05  0.94  0.96  0.09  0.915 0.73\n",
      " 0.87  0.04  0.005 0.17  0.77  0.295 0.115 0.15  0.67  0.025 0.945 0.275\n",
      " 0.05  0.095 0.105 0.275 0.2   0.205 0.92  0.915 0.92  0.085 0.015 0.07\n",
      " 0.72  0.04  0.185 0.775 0.105 0.745 0.86  0.065 0.14  0.75  0.775 0.28\n",
      " 0.135 0.965 0.84  0.12  0.16  0.225 0.06  0.805 0.9   0.03  0.24  0.09\n",
      " 0.205 0.095 0.265 0.05  0.12  0.855 0.725 0.09  0.2   0.89  0.055 0.055\n",
      " 0.91  0.08  0.055 0.245 0.185 0.965 0.76  0.92  0.005 0.925 0.195 0.845\n",
      " 0.105 0.795 0.095 0.845 0.08  0.89  0.09  0.115 0.945 0.025 0.11  0.02\n",
      " 0.89  0.015 0.125 0.955 0.805 0.135 0.03  0.16  0.96  0.795 0.08  0.05\n",
      " 0.915 0.095 0.91  0.79  0.13  0.92  0.915 0.12  0.82  0.015 0.905 0.96\n",
      " 0.955 0.75  0.16  0.975 0.91  0.885 0.115 0.04  0.04  0.95  0.975 0.9\n",
      " 0.1   0.92  0.095 0.955 0.15  0.03  0.055 0.075 0.95  0.145 0.88  0.815\n",
      " 0.095 0.075 0.09  0.06  0.105 0.15  0.935 0.94  0.84  0.895 0.785 0.87\n",
      " 0.805 0.775 0.95  0.935 0.085 0.165 0.715 0.905]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_train=sklearn.metrics.roc_auc_score(y_train, forest.predict_proba(X_train)[:,1], sample_weight=None)\n",
    "print(forest.predict_proba(X_train)[:,1])\n",
    "roc_auc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.0187, 0.0374, 0.0467, 0.0654, 0.0935, 0.1495,\n",
       "       0.1589, 0.1776, 0.1869, 0.2243, 0.2804, 0.2991, 0.3178, 0.3271,\n",
       "       0.3458, 0.4019, 0.4206, 0.4673, 0.4766, 0.5327, 0.5981, 0.6542,\n",
       "       0.6729, 0.6822, 0.7009, 0.8131, 0.8598, 0.8692, 0.9159, 0.9533,\n",
       "       0.9813, 1.    ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_train, y_pred_train, pos_label=1)\n",
    "plt.plot(fpr,tpr)\n",
    "tpr\n",
    "fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "\n",
    "Para el caso del RandomForest, la complejidad del modelo viene dada por la cantidad de atributos disponibles en cada nodo de cada árbol. Mientras se permitan utilizar más atributos, aumentarán las posibles decisiones que tome cada árbol, y de ese modo la cantidad de parámetros libres.\n",
    "\n",
    "En primer lugar, podemos notar que la métrica ROC AUC tiene un valor óptimo sobre los datos de entrenamiento para cualquier valor de la complejidad, nuevamente con valores menores sobre los datos de validación. Este error sobre validación disminuye ligeramente al aumentar la cantidad de atributos disponibles, pero presenta cierta dispersión. \n",
    "\n",
    "Creemos que el constante valor perfecto de la ROC AUC sobre los datos de entrenamiento se debe a el bajo sesgo del algortimo, pues se utilizan árboles que pueden crecer profundo (lo cual resulta en un bajo sesgo para cada árbol). El promediado sobre el conjunto de árboles resulta en una reducción de la varianza. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_parameters = [{'n_estimators':[200], 'max_features':range(1, 100, 5), 'max_depth':range(1,11)}]\n",
    "%time GridSearch(RandomForestClassifier(random_state=1234, n_jobs = -1), tuned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb09a825588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJQtjjDExWbIwxhgTkyULY4wxMf0/qWT91CoM6UEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb09a490080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sizes, train_scores_forest, valid_scores_forest = learning_curve(\n",
    "    RandomForestClassifier(n_estimators=200, max_features=1, max_depth=5, n_jobs=-1), X_dev_np, y_dev_np, train_sizes=range(5, 199, 5), cv=5, \n",
    "    scoring = 'roc_auc', random_state=1234, n_jobs=-1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, 1-np.mean(train_scores_forest, axis = 1), color = 'blue', label = 'Error de entrenamiento')\n",
    "plt.plot(train_sizes, 1-np.mean(valid_scores_forest, axis = 1), color = 'red', label = 'Error de validación')\n",
    "#plt.ylim(0, 0.6)\n",
    "plt.xlabel('Datos de entrenamiento')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Curva de aprendizaje: Random Forest')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#print(train_scores_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "Vemos aquí que el error sobre los datos de entrenamiento es prácticamente nulo. Esto indica que el sesgo (al menos considerando sólo los datos de entrenamiento) _parece_ ser muy bajo. Por otro lado, el error de validación disminuye notablemente al aumentar la cantidad de datos de entrenamiento de 5 a 100. Su disminución se frena gradualmente y, pasado ese rango, parece estabilizarse. \n",
    "Observando la distancia entre curvas, vemos que la varianza disminuye de igual manera, y su disminución también se hace menos pronunciada aumentando la cantidad de datos. Al pasar los 125 datos, parece haberse estabilizado. \n",
    "\n",
    "Teniendo en cuenta lo anterior, podemos indicar que no resultaría útil conseguir más datos para mejorar la performance del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'n_estimators': 168, 'max_depth': 1, 'learning_rate': 0.15454545454545454}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.877 (+/-0.091) for {'n_estimators': 90, 'max_depth': 1, 'learning_rate': 0.96363636363636362}\n",
      "0.785 (+/-0.134) for {'n_estimators': 188, 'max_depth': 10, 'learning_rate': 0.84545454545454535}\n",
      "0.882 (+/-0.083) for {'n_estimators': 122, 'max_depth': 3, 'learning_rate': 0.65454545454545454}\n",
      "0.864 (+/-0.141) for {'n_estimators': 56, 'max_depth': 4, 'learning_rate': 0.97272727272727266}\n",
      "0.801 (+/-0.114) for {'n_estimators': 199, 'max_depth': 5, 'learning_rate': 0.47272727272727266}\n",
      "0.878 (+/-0.099) for {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.24545454545454545}\n",
      "0.885 (+/-0.064) for {'n_estimators': 67, 'max_depth': 2, 'learning_rate': 0.3545454545454545}\n",
      "0.802 (+/-0.123) for {'n_estimators': 47, 'max_depth': 8, 'learning_rate': 0.28181818181818186}\n",
      "0.820 (+/-0.114) for {'n_estimators': 97, 'max_depth': 8, 'learning_rate': 0.13636363636363635}\n",
      "0.787 (+/-0.153) for {'n_estimators': 156, 'max_depth': 8, 'learning_rate': 0.79999999999999993}\n",
      "0.795 (+/-0.147) for {'n_estimators': 194, 'max_depth': 10, 'learning_rate': 0.72727272727272718}\n",
      "0.785 (+/-0.134) for {'n_estimators': 52, 'max_depth': 10, 'learning_rate': 0.84545454545454535}\n",
      "0.786 (+/-0.085) for {'n_estimators': 89, 'max_depth': 5, 'learning_rate': 0.28181818181818186}\n",
      "0.890 (+/-0.108) for {'n_estimators': 101, 'max_depth': 1, 'learning_rate': 0.13636363636363635}\n",
      "0.814 (+/-0.117) for {'n_estimators': 68, 'max_depth': 10, 'learning_rate': 0.42727272727272725}\n",
      "0.632 (+/-0.119) for {'n_estimators': 1, 'max_depth': 1, 'learning_rate': 0.67272727272727273}\n",
      "0.798 (+/-0.147) for {'n_estimators': 77, 'max_depth': 9, 'learning_rate': 0.66363636363636358}\n",
      "0.899 (+/-0.093) for {'n_estimators': 129, 'max_depth': 1, 'learning_rate': 0.46363636363636362}\n",
      "0.814 (+/-0.117) for {'n_estimators': 148, 'max_depth': 8, 'learning_rate': 0.42727272727272725}\n",
      "0.802 (+/-0.112) for {'n_estimators': 190, 'max_depth': 10, 'learning_rate': 0.51818181818181819}\n",
      "0.805 (+/-0.123) for {'n_estimators': 108, 'max_depth': 9, 'learning_rate': 0.6272727272727272}\n",
      "0.794 (+/-0.108) for {'n_estimators': 47, 'max_depth': 9, 'learning_rate': 0.31818181818181818}\n",
      "0.881 (+/-0.105) for {'n_estimators': 184, 'max_depth': 1, 'learning_rate': 0.86363636363636354}\n",
      "0.893 (+/-0.087) for {'n_estimators': 109, 'max_depth': 3, 'learning_rate': 0.57272727272727275}\n",
      "0.785 (+/-0.116) for {'n_estimators': 174, 'max_depth': 8, 'learning_rate': 0.70909090909090899}\n",
      "0.879 (+/-0.123) for {'n_estimators': 110, 'max_depth': 1, 'learning_rate': 0.79999999999999993}\n",
      "0.793 (+/-0.130) for {'n_estimators': 20, 'max_depth': 8, 'learning_rate': 0.79090909090909089}\n",
      "0.888 (+/-0.103) for {'n_estimators': 125, 'max_depth': 1, 'learning_rate': 0.45454545454545459}\n",
      "0.790 (+/-0.096) for {'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.32727272727272727}\n",
      "0.826 (+/-0.099) for {'n_estimators': 104, 'max_depth': 5, 'learning_rate': 0.44545454545454544}\n",
      "0.736 (+/-0.120) for {'n_estimators': 11, 'max_depth': 10, 'learning_rate': 0.20000000000000001}\n",
      "0.782 (+/-0.134) for {'n_estimators': 82, 'max_depth': 8, 'learning_rate': 0.87272727272727268}\n",
      "0.825 (+/-0.112) for {'n_estimators': 132, 'max_depth': 7, 'learning_rate': 0.19090909090909092}\n",
      "0.900 (+/-0.090) for {'n_estimators': 168, 'max_depth': 1, 'learning_rate': 0.15454545454545454}\n",
      "0.886 (+/-0.106) for {'n_estimators': 95, 'max_depth': 3, 'learning_rate': 0.44545454545454544}\n",
      "0.842 (+/-0.153) for {'n_estimators': 88, 'max_depth': 4, 'learning_rate': 0.32727272727272727}\n",
      "0.809 (+/-0.124) for {'n_estimators': 103, 'max_depth': 8, 'learning_rate': 0.77272727272727271}\n",
      "0.875 (+/-0.087) for {'n_estimators': 134, 'max_depth': 2, 'learning_rate': 0.97272727272727266}\n",
      "0.803 (+/-0.101) for {'n_estimators': 78, 'max_depth': 7, 'learning_rate': 0.30909090909090908}\n",
      "0.896 (+/-0.098) for {'n_estimators': 167, 'max_depth': 2, 'learning_rate': 0.72727272727272718}\n",
      "0.822 (+/-0.099) for {'n_estimators': 110, 'max_depth': 9, 'learning_rate': 0.1090909090909091}\n",
      "0.883 (+/-0.109) for {'n_estimators': 133, 'max_depth': 1, 'learning_rate': 0.76363636363636356}\n",
      "0.804 (+/-0.152) for {'n_estimators': 177, 'max_depth': 8, 'learning_rate': 0.75454545454545452}\n",
      "0.821 (+/-0.099) for {'n_estimators': 135, 'max_depth': 7, 'learning_rate': 0.14545454545454545}\n",
      "0.797 (+/-0.113) for {'n_estimators': 197, 'max_depth': 6, 'learning_rate': 0.40909090909090906}\n",
      "0.769 (+/-0.167) for {'n_estimators': 79, 'max_depth': 10, 'learning_rate': 0.96363636363636362}\n",
      "0.792 (+/-0.123) for {'n_estimators': 18, 'max_depth': 8, 'learning_rate': 0.67272727272727273}\n",
      "0.824 (+/-0.079) for {'n_estimators': 129, 'max_depth': 6, 'learning_rate': 0.5}\n",
      "0.795 (+/-0.117) for {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.40909090909090906}\n",
      "0.807 (+/-0.123) for {'n_estimators': 36, 'max_depth': 8, 'learning_rate': 0.5}\n",
      "\n",
      "CPU times: user 35.4 s, sys: 0 ns, total: 35.4 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "# Extra\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tuned_parameters = {'n_estimators':range(1,200), 'learning_rate':np.linspace(0.1,1,100) ,'max_depth':range(1,11)}\n",
    "#tuned_parameters = {'n_estimators':range(1,100)}\n",
    "n_iter = 50\n",
    "\n",
    "%time RandomSearch(GradientBoostingClassifier(random_state=1234), tuned_parameters, n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discusión\n",
    "\n",
    "Al realizar la GridSearch sobre los hiperparámetros ```max_features``` y ```max_depth``` del algoritmo RandomForest con 200 árboles, observamos que los puntajes de ROC AUC obtenidos fueron de aproximadamente 0.86-0.87 en general, con poca dispersión ente ellos. \n",
    "\n",
    "Por otro lado, al utilizar la RandomSearch sobre el algoritmo GradientBoost obtenemos resultados de la métrica muy variables entre sí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUCs para diferentes modelos:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5915159764080626"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=51, p=1,\n",
       "           weights='distance')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7109987685527253"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage='auto',\n",
       "              solver='lsqr', store_covariance=False, tol=0.0001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6975176615464386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6822217901354592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=21, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6830967658305788"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.001539926526059492, class_weight=None, dual=True,\n",
       "     fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
       "     max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7272344286732776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.15454545454545454, loss='deviance',\n",
       "              max_depth=1, max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=168,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6597316741201633"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Competencia\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_eval_np = np.array(X_eval)\n",
    "y_eval_np = np.array(y_eval).ravel()\n",
    "\n",
    "models = [\n",
    "    DecisionTreeClassifier(max_depth= 2, criterion= 'entropy'),\n",
    "    KNeighborsClassifier(p=1, n_neighbors= 51, weights= 'distance'),\n",
    "    LinearDiscriminantAnalysis(shrinkage= 'auto', solver= 'lsqr'),\n",
    "    GaussianNB(priors = None),\n",
    "    RandomForestClassifier(max_features=21, n_estimators= 200),\n",
    "    LinearSVC(C=0.001539926526059492),\n",
    "    GradientBoostingClassifier(n_estimators= 168, learning_rate= 0.15454545454545454, max_depth=1)\n",
    "]\n",
    "\n",
    "print(\"ROC AUCs para diferentes modelos:\")\n",
    "for model in models:\n",
    "\n",
    "    model.fit(X_dev_np, y_dev_np)\n",
    "    y_pred = model.predict(X_eval_np)\n",
    "    r = sklearn.metrics.roc_auc_score(y_eval_np, y_pred, sample_weight=None)\n",
    "    #print(\"ROC AUC de {0}:\".format(str(model)))\n",
    "    display(model)\n",
    "    display(r)\n",
    "    print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.5478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.5669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.5492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.6299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.2931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.8071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.7429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4499 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "index        \n",
       "501    0.5478\n",
       "502    0.5669\n",
       "503    0.5492\n",
       "504    0.6299\n",
       "505    0.2931\n",
       "...       ...\n",
       "4995   0.3520\n",
       "4996   0.3325\n",
       "4997   0.8071\n",
       "4998   0.7429\n",
       "4999   0.2186\n",
       "\n",
       "[4499 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = KNeighborsClassifier(p=1, n_neighbors= 51, weights= 'distance')\n",
    "best_model.fit(X_dev_np, y_dev_np)\n",
    "all_probas = best_model.predict_proba(np.array(X_competencia))\n",
    "\n",
    "true_proba = list()\n",
    "for x in all_probas:\n",
    "    true_proba.append(x[1])\n",
    "    \n",
    "final_res = pd.DataFrame(index=X_competencia.index)\n",
    "final_res[\"output\"] = np.around(true_proba, decimals=4)\n",
    "\n",
    "display(final_res)\n",
    "final_res.to_csv(\"y_competencia.csv\")\n",
    "\n",
    "auc_roc_held_out = sklearn.metrics.roc_auc_score(np.array(y_eval).ravel(), best_model.predict(np.array(X_eval)), sample_weight=None)\n",
    "f = open(\"auc_roc_competencia.txt\", \"w\")\n",
    "f.write(str(auc_roc_held_out - 0.02)) #usamos el auc roc del held out, menos 2 puntos porcentuales para ajustar sobreestimacion\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competencias\n",
    "\n",
    "La entrega del trabajo estará acompañada de una competencia en la cual deberán poner a prueba su mejor modelo y sobre todo, su capacidad para estimar sus resultados. \n",
    "\n",
    "Su tarea será estimar la performance (AUC ROC) que tendrá su mejor modelo en datos de evaluación (X_competencia). \n",
    "\n",
    "Para ello, deberán predecir las probabilidades de las distintas instancias con su modelo, enviarnos dichas probabilidades junto a una estimación con 4 decimales de cuál será el AUC ROC resultante y calcularemos el resultado real. El grupo que consiga acercarse más al valor real, será el grupo ganador.  \n",
    "\n",
    "Recomendamos no perder de vista esta competencia en el momento de separar los datos en los primeros puntos. \n",
    "\n",
    "Para esto, junto con la entrega del informe, deberán enviar un archivo en formato csv con las columnas “index” y “output” (ver ejemplo de archivo en: [y_competencia_ejemplo.csv](https://github.com/pbrusco/aa-notebooks/blob/master/TP1/y_competencia_ejemplo.csv)) y un valor esperado de AUC ROC. \n",
    "\n",
    "\n",
    "## Entrega\n",
    "- Contarán con un esqueleto en formato Jupyter Notebook en donde tendrán que completar las celdas faltantes (ya sea con explicaciones y gráficos o código). \n",
    "- El notebook final deberá ser entregado en formatos .html e .ipynb. Es necesario que los resultados puedan reproducirse al ejecutar todas las celdas en orden (Kernel - Restart and Run All) utilizando las bibliotecas requeridas en el archivo: requirements.txt del repositorio. \n",
    "- Tienen tiempo hasta las 23:59hs del día miércoles 17/10/2018. La entrega se debe realizar a través del campus virtual y debe contener el informe.\n",
    "- El trabajo deberá elaborarse en grupos de 3 personas.\n",
    "- Se podrán pedir pruebas de integridad y autoría; es decir, verificar que la salida solicitada es fruto del modelo presentado y que el modelo fue construido según lo requerido en este enunciado.\n",
    "- La evaluación será grupal y se basará en la calidad del informe (presentación, claridad, prolijidad); la originalidad, practicidad y coherencia técnica de la solución; la corrección y solidez de las pruebas realizadas.\n",
    "- En el primer parcial se incluirá una pregunta sobre la solución entregada. Esa pregunta no influirá en la nota del parcial, pero sí en la nota individual del TP1.\n",
    "- La participación en la competencia es obligatoria. De todas maneras, el resultado no incidirán en la nota de la materia.\n",
    "- Los ejercicios extra son opcionales para aprobar el TP, pero son obligatorios para promocionar la materia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
